<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Concurrency | Wendi's Blog]]></title>
  <link href="http://blog.ifyouseewendy.com/blog/categories/concurrency/atom.xml" rel="self"/>
  <link href="http://blog.ifyouseewendy.com/"/>
  <updated>2020-07-27T22:46:48-04:00</updated>
  <id>http://blog.ifyouseewendy.com/</id>
  <author>
    <name><![CDATA[wendi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ruby Concurrency Article Collection]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-article-collection/"/>
    <updated>2016-02-16T20:04:50+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-article-collection</id>
    <content type="html"><![CDATA[<p>This an article collection about concurrency in Ruby, which benefits me a lot and to be continued.</p>

<blockquote>
  <p><a href="http://www.jstorimer.com/blogs/workingwithcode/7766063-threads-not-just-for-optimizations">Threads, Not Just for Optimisations - Jesse Storimer</a></p>
</blockquote>

<p>Threads can help us organize our programs.</p>

<p>When a signal is delivered to a multithreaded process that has established a signal handler, the kernel arbitrarily selects one thread in the process to which to deliver the signal and invokes the handler in that thread. So Ruby uses a dedicated thread to handle incoming Unix signals. This has nothing to do with speeding things up, it’s just good programming practice.</p>

<p>When you spawn a new Unix process using fork, you really should either wait for it to finish using Process.wait, or detach from it using Process.detach. The reason is that when the process exits, it leaves behind some information about its exit status. This status info can’t be cleaned up until it’s been consumed by the parent process using Process.wait. When you use something like Process.spawn or backticks, Process.wait is called internally to cleanup the aforementioned status info. So Process.detach is just a thin wrapper around Process.wait, using a background thread to wait for the return value of Process.wait, while the main thread continues execution concurrently. Again, this has nothing to do with speed, but allows the proper housekeeping to be done without burdening the program with extra state.</p>

<blockquote>
  <p><a href="http://concur.rspace.googlecode.com/hg/talk/concur.html#title-slide">Concurrency is not Parallelism (it’s better) - Rob Pike</a></p>
</blockquote>

<p>Go provides</p>

<ul>
  <li>concurrent execution (coroutines. They’re a bit like threads, but they’re much cheaper. Goroutines are multiplexed onto OS threads as required. When a goroutine blocks, that thread blocks but no other goroutine blocks.)</li>
  <li>synchronization and messaging (channels)</li>
  <li>multi-way concurrent control (select)</li>
</ul>

<p>Concurrency vs. Paralelism</p>

<ul>
  <li>Concurrency is about dealing with lots of things at once.</li>
  <li>Parallelism is about doing lots of things at once.</li>
  <li>Not the same, but related.</li>
  <li>One is about structure (design), one is about execution.</li>
  <li>Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.</li>
</ul>

<p>Concurrency plus communication</p>

<ul>
  <li>Concurrency is a way to structure a program by breaking it into pieces that can be executed independently.</li>
  <li>Communication is the means to coordinate the independent executions.</li>
  <li>This is the Go model and (like Erlang and others) it’s based on CSP (Communicating Sequential Processes)</li>
</ul>

<blockquote>
  <p><a href="https://blog.engineyard.com/2011/ruby-concurrency-and-you">Ruby, Concurrency, and You - Engine Yard</a></p>
</blockquote>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCAC-ruby_support.png" alt="RCAC-ruby_support.png" /></p>

<blockquote>
  <p><a href="https://github.com/jruby/jruby/wiki/Concurrency-in-jruby">Concurrency in JRuby</a></p>
</blockquote>

<p>In general, the safest path to writing concurrent code in JRuby is the same as on any other platform:</p>

<ul>
  <li>Don’t do it, if you can avoid it.</li>
  <li>If you must do it, don’t share data across threads.</li>
  <li>If you must share data across threads, don’t share mutable data.</li>
  <li>If you must share mutable data across threads, synchronize access to that data.</li>
</ul>

<p>Thread Safety refers to the ability to perform operations against a shared structure across multiple threads and know there will be no resulting errors or data integrity issues.</p>

<p>Volatility refers to the visibility of changes across threads on multi-core systems that may have thread or core-specific views of system memory.</p>

<p>Atomicity refers to the ability to perform a write to memory based on some view of that memory and to know the write happens before the view is invalid.</p>

<blockquote>
  <ul>
    <li><a href="http://www.jstorimer.com/blogs/workingwithcode/8085491-nobody-understands-the-gil%0A">Nobody understands the GIL - Part 1 - Jesse Storimer</a></li>
    <li><a href="http://www.jstorimer.com/blogs/workingwithcode/8100871-nobody-understands-the-gil-part-2-implementation">Nobody understands the GIL - Part 2: Implementation - Jesse Storimer</a></li>
    <li><a href="http://www.rubyinside.com/does-the-gil-make-your-ruby-code-thread-safe-6051.html">Does the GIL Make Your Ruby Code Thread-Safe? - Jesse Storimer</a></li>
  </ul>
</blockquote>

<p>It’s possible for all of the Ruby implementations to provide thread-safe data structures, but that requires extra overhead that would make single-threaded code slower.</p>

<p>For the MRI core team, the GIL protects the internal state of the system. With a GIL, they don’t require any locks or synchronization around the internal data structures. If two threads can’t be mutating the internals at the same time, then no race conditions can occur. For you, the developer, this will severely limit the parallelism you get from running your Ruby code on MRI.</p>

<p>All that the GIL guarantees is that MRI’s native C implementations of Ruby methods will be executed atomically (but even this has caveats). This behaviour can sometimes help us as Ruby developers, but the GIL is really there for the protection of MRI internals, not as a dependable API for Ruby developers. So the GIL doesn’t ‘solve’ thread-safety issues.</p>

<p>Don’t communicate by sharing state; share state by communicating.</p>

<blockquote>
  <p><a href="https://www.igvita.com/2008/11/13/concurrency-is-a-myth-in-ruby/">Parallelism is a Myth in Ruby - Ilya Grigorik</a></p>
</blockquote>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCAC-ruby_gil.png" alt="RCAC-ruby_gil.png" /></p>

<blockquote>
  <p><a href="https://github.com/jdantonio/Everything-You-Know-About-the-GIL-is-Wrong-RubyConf-2015">Everything You Know About GIL is Wrong - Jerry D’Antonio</a></p>
</blockquote>

<p>Summary</p>

<ul>
  <li>Concurrency is not parallelism</li>
  <li>The GIL protects Ruby’s internal state when the operating system context switches
    <ul>
      <li>The GIL does not provide thread safety guarantees to user code</li>
      <li>But it imposes an implicit memory model</li>
    </ul>
  </li>
  <li>The GIL prevents true parallelism in Ruby</li>
  <li>But Ruby is pretty good at multiplexing threads performing blocking I/O</li>
</ul>

<p>Concurrency vs. Parallelism</p>

<p>Non-concurrent programs gain no benefit from running on multiple processors. Concurrent programs get parallelism for free when the runtime supports it.</p>

<ul>
  <li>Parallelism requires two processor cores. No matter the language/runtime, a processor core can only execute one instruction at a time.</li>
  <li>Concurrency can happen when there is only one core. Concurrency is about design, improved performance is a side effect</li>
</ul>

<p>Ruby is selfish</p>

<ul>
  <li>Ruby is an interpreted language
    <ul>
      <li>Ruby is compiled to bytecode within the interpreter</li>
      <li>Ruby is free to optimize and reorder your code</li>
    </ul>
  </li>
  <li>Every Ruby operation is implemented in C</li>
  <li>The Ruby runtime is just another program; it is under the control of the compiler and the operating system
    <ul>
      <li>The C compiler is free to optimize and reorder instructions during compilation</li>
      <li>An operating system context switch can occur at any point in the running C code</li>
    </ul>
  </li>
  <li>The GIL protects Ruby, not your code</li>
</ul>

<p>Ruby is thread safe, your code isn’t.</p>

<ul>
  <li>Every individual read and write to memory is guaranteed to be thread-safe in Ruby
    <ul>
      <li>The GIL prevents interleaved access to memory used by the runtime</li>
      <li>The GIL prevents interleaved access to individual variables</li>
      <li>Ruby itself will never become corrupt</li>
    </ul>
  </li>
  <li>Ruby makes no guarantees about your code</li>
</ul>

<p><a href="https://www.wikiwand.com/en/Memory_model_(programming)">Memory model</a></p>

<ul>
  <li>“In computing, a memory model describes the interactions of threads through memory and their shared use of the data.” Wikipedia</li>
  <li>Defines visibility, volatility, atomicity, and synchronization barriers
    <ul>
      <li>Java’s current memory model was adopted in 2004 as part of Java</li>
      <li>The C and C++ memory models were adopted in 2011 with C11 and C++11</li>
      <li><a href="https://golang.org/ref/mem">The Go Memory Model</a></li>
    </ul>
  </li>
  <li>Ruby does NOT have a documented memory model. The GIL provides an implied memory model but no guarantees</li>
</ul>

<p>I/O</p>

<p>Ruby programs which perform significant I/O generally benefit from concurrency.</p>

<ul>
  <li>I/O in Ruby programs is blocking</li>
  <li>I/O within Ruby is asynchronous</li>
</ul>

<p>You can’t spell GIL without I/O. The GIL exists to maintain the internal consistency of the Ruby runtime. I/O operations are slow, which is why asynchronous I/O was invented. While I/O is in progress the Ruby thread is blocked so it cannot change the internal state, so Ruby allows other threads to do useful work. All Ruby I/O calls unlock the GIL, as do backtick and <code>system</code> calls. When Ruby thread is waiting on I/O it does not block other threads.</p>

<blockquote>
  <p><a href="http://merbist.com/2011/02/22/concurrency-in-ruby-explained/">Ruby concurrency explained - Matt Aimonetti</a></p>
</blockquote>

<p>The thing to keep in mind is that the concurrency models are often defined by the programming language you use. The advantage of the Java threaded approach is that the memory is shared between the threads so you are saving in memory (and startup time), each thread can easily talk to each other via the shared memory. The advantage of PHP is that you don’t have to worry about locks, deadlocks, threadsafe code and all that mess hidden behind threads.</p>

<p>Others programming languages like Erlang and Scala use a third approach: the actor model. The actor model is somewhat a bit of a mix of both solutions, the difference is that actors are a like threads which don’t share the same memory context. Communication between actors is done via exchanged messages ensuring that each actor handles its own state and therefore avoiding corrupt data (two threads can modify the same data at the same time, but an actor can’t receive two messages at the exact same time).</p>

<p>Actors/Fibers</p>

<p>Ruby 1.9, developers now have access to a new type of “lightweight” threads called Fibers. Fibers are not actors and Ruby doesn’t have a native Actor model implementation but some people wrote some actor libs on top of fibers. A fiber is like a simplified thread which isn’t scheduled by the VM but by the programmer. Fibers are like blocks which can be paused and resumed from the outside of from within themselves.</p>

<p>How do fibers help with concurrency? The answer is that they are part of a bigger solution. Ruby 1.9 gave us fibers which allow for a more granular control over the concurrency scheduling, combined with non-blocking IO, high concurrency can be achieved. Fiber allow developers to manually control the scheduling of “concurrent” code but also to have the code within the fiber to auto schedule itself.  Well, the only problem is that if you are doing any type of blocking IO in a fiber, the entire thread is blocked and the other fibers aren’t running. So avoid blocking IOs.</p>

<p>Non blocking IOs/Reactor pattern</p>

<p>The reactor pattern is quite simple to understand really. The heavy work of making blocking IO calls is delegated to an external service (reactor) which can receive concurrent requests. The service handler (reactor) is given callback methods to trigger asynchronously based on the type of response received.</p>

<p>When a request comes in and your code makes a DB query, you are blocking any other requests from being processed. To avoid that, we could wrap our request in a fiber, trigger an async DB call and pause the fiber so another request can get processed as we are waiting for the DB. Once the DB query comes back, it wakes up the fiber it was trigger from, which then sends the response back to the client. Technically, the server can still only send one response at a time, but now fibers can run in parallel and don’t block the main tread by doing blocking IOs (since it’s done by the reactor).</p>

<p>This is the approach used by Twisted, EventMachine and Node.js. Ruby developers can use EventMachine or an EventMachine based webserver like Thin as well as EM clients/drivers to make non blocking async calls.</p>

<blockquote>
  <p><a href="https://www.quora.com/Node-js/What-is-a-good-comparison-of-the-reactor-pattern-vs-actor-model">Node.js: What is a good comparison of the reactor pattern vs actor model? - Sean Byrnes</a></p>
</blockquote>

<p>The reactor model follows a purely event driven system where the entire system can be implemented as a single-threaded process with a series of event generators and event handlers. In most implementations there is a “event loop” that continues to run which takes all of the generated events, sends them to all registered event handles and then starts over again.</p>

<p>An actor model is a more abstract method of breaking up execution into different processes that interact with each other. While it is possible to do this similarly to the reactor model, I see this mostly as a series of processes running in different threads and exchanging information through messages or protocols.</p>

<blockquote>
  <p><a href="http://www.toptal.com/ruby/ruby-concurrency-and-parallelism-a-practical-primer?utm_source=rubyweekly&amp;utm_medium=email">Ruby Concurrency and Parallelism: A Practical Tutorial</a></p>
</blockquote>

<ul>
  <li>Ruby concurrency is when two tasks can start, run, and complete in overlapping time periods. It doesn’t necessarily mean, though, that they’ll ever both be running at the same instant (e.g., multiple threads on a single-core machine).</li>
  <li>Parallelism is when two tasks literally run at the same time.</li>
</ul>

<blockquote>
  <p><a href="http://oldmoe.blogspot.jp/2008/08/ruby-fibers-vs-ruby-threads.html">Ruby Fibers Vs Ruby Threads</a></p>
</blockquote>

<p>Fibers are much faster to create than threads, they eat much less memory too.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby Concurrency In Practice]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-in-practice/"/>
    <updated>2016-02-16T19:58:54+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-in-practice</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#guidance">Guidance</a>    <ul>
      <li><a href="#safest-path-to-concurrency">Safest path to concurrency</a></li>
      <li><a href="#writing-thread-safe-code">Writing Thread-safe Code</a></li>
    </ul>
  </li>
  <li><a href="#into-the-wild">Into the Wild</a></li>
</ul>

<h2 id="guidance">Guidance</h2>

<h3 id="safest-path-to-concurrency">Safest path to concurrency</h3>

<blockquote>
  <p>from <a href="https://github.com/jruby/jruby/wiki/Concurrency-in-jruby">JRuby wiki</a></p>
</blockquote>

<ol>
  <li>Don’t do it.</li>
  <li>If you must do it, don’t share data across threads.</li>
  <li>If you must share data across threads, don’t share mutable data.</li>
  <li>If you must share mutable data across threads, synchronize access to that data.</li>
</ol>

<p>Do not communicate by sharing data; instead, share data by communicating</p>

<h3 id="writing-thread-safe-code">Writing Thread-safe Code</h3>

<p><strong>Avoid mutating globals</strong></p>

<ul>
  <li>Constants</li>
  <li>The AST</li>
  <li>Class variables/methods</li>
</ul>

<p><strong>Create more objects, rather than sharing one</strong></p>

<ul>
  <li>Thread-locals</li>
  <li>Connection pools</li>
</ul>

<p><strong>Avoid lazy loading</strong></p>

<ul>
  <li>No autoload</li>
</ul>

<p><strong>Prefer data structures over mutexes</strong></p>

<p>Mutexes are notoriously hard to use correctly. For better or worse, you have a lot of things to decide when using a mutex.</p>

<ul>
  <li>How coarse or fine should this mutex be?</li>
  <li>Which lines of code need to be in the critical section?</li>
  <li>Is a deadlock possible here?</li>
  <li>Do I need a per-instance mutex? Or a global one?</li>
</ul>

<p>By leaning on a data structure, you remove the burden of correct synchronization from your code and depend on the semantics of the data structure to keep things consistent.</p>

<p><strong>Wrap your threads in an abstraction</strong></p>

<ul>
  <li>Actor model</li>
  <li>Reactor Pattern, event-driven I/O</li>
</ul>

<h2 id="into-the-wild">Into the Wild</h2>

<p><strong>Primitives</strong></p>

<ul>
  <li><a href="http://ruby-doc.org/core-2.2.2/Thread.html">Thread</a></li>
  <li><a href="http://ruby-doc.org/core-2.2.2/Mutex.html">Mutex</a></li>
  <li><a href="http://ruby-doc.org/core-2.2.2/ConditionVariable.html">ConditionVariable</a></li>
</ul>

<p><strong>Thread-safe Data Structure</strong></p>

<ul>
  <li><a href="https://github.com/hamstergem/hamster">hamster</a> - Efficient, Immutable, Thread-Safe Collection classes for Ruby</li>
  <li><a href="https://github.com/ruby-concurrency/thread_safe">thread_safe</a> - Thread-safe collections for Ruby</li>
  <li><a href="https://github.com/ruby-concurrency/atomic">atomic</a> - Atomic references for Ruby (merged with concurrent-ruby)</li>
  <li><a href="https://github.com/mperham/connection_pool">connection_pool</a> - Generic connection pooling for Ruby</li>
</ul>

<p><strong>Abstraction / Framework</strong></p>

<p><a href="https://github.com/celluloid/celluloid">celluloid</a></p>

<p>Actor-based concurrent object framework for Ruby.</p>

<ul>
  <li><a href="https://github.com/celluloid/reel/">Reel</a> - An “evented” web server based on Celluloid::IO</li>
  <li><a href="https://github.com/kenichi/angelo">angelo</a> - Sinatra-like DSL for Reel that supports WebSockets and SSE</li>
</ul>

<p><a href="https://github.com/eventmachine/eventmachine">eventmachine</a></p>

<p>EventMachine is an event-driven I/O and lightweight concurrency library for Ruby. It provides event-driven I/O using the Reactor pattern.</p>

<ul>
  <li><a href="http://code.macournoyer.com/thin/">Thin</a>, <a href="https://github.com/postrank-labs/goliath/">Goliath</a> - Scalable event-driven servers. Examples:</li>
  <li><a href="https://github.com/igrigorik/em-http-request">em-http-request</a> - Asynchronous HTTP Client (EventMachine + Ruby)</li>
  <li><a href="https://github.com/igrigorik/em-synchrony">em-synchrony</a> - Fiber aware EventMachine clients and convenience classes</li>
</ul>

<p><a href="https://github.com/puma/puma">puma</a></p>

<p>A ruby web server built for concurrency</p>

<p><a href="https://github.com/ruby-concurrency/concurrent-ruby">concurrent-ruby</a></p>

<p>Modern concurrency tools including agents, futures, promises, thread pools, supervisors, and more. Inspired by Erlang, Clojure, Scala, Go, Java, JavaScript, and classic concurrency patterns.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ruby Concurrency In Theory]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-in-theory/"/>
    <updated>2016-02-16T19:35:19+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/02/16/ruby-concurrency-in-theory</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#what-is-concurrency">What is concurrency?</a>    <ul>
      <li><a href="#concurrency-vs-paralelism">Concurrency vs. Paralelism</a></li>
      <li><a href="#concurrency-plus-communication">Concurrency plus communication</a></li>
    </ul>
  </li>
  <li><a href="#what-does-ruby-support">What does Ruby support?</a>    <ul>
      <li><a href="#gil">GIL</a></li>
      <li><a href="#ruby-support">Ruby Support</a></li>
      <li><a href="#fiber">Fiber</a></li>
    </ul>
  </li>
  <li><a href="#how-to-enhance-concurrency-by-ruby">How to enhance concurrency by Ruby?</a>    <ul>
      <li><a href="#basics">Basics</a></li>
      <li><a href="#concurrency-model-software-transactional-memory">Concurrency Model: Software Transactional Memory</a></li>
      <li><a href="#concurrency-model-actor-model">Concurrency Model: Actor Model</a></li>
    </ul>
  </li>
</ul>

<h2 id="what-is-concurrency">What is concurrency?</h2>

<h3 id="concurrency-vs-paralelism">Concurrency vs. Paralelism</h3>

<ul>
  <li>Concurrency is about dealing with lots of things at once.</li>
  <li>Parallelism is about doing lots of things at once.</li>
  <li>Not the same, but related.</li>
  <li>One is about structure (design), one is about execution.</li>
  <li>Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.</li>
</ul>

<h3 id="concurrency-plus-communication">Concurrency plus communication</h3>

<ul>
  <li>Concurrency is a way to structure a program by breaking it into pieces that can be executed independently.</li>
  <li>Communication is the means to coordinate the independent executions.</li>
  <li>This is the Go model and (like Erlang and others) it’s based on CSP (Communicating Sequential Processes)</li>
</ul>

<p><em>Reference</em></p>

<ul>
  <li><a href="http://concur.rspace.googlecode.com/hg/talk/concur.html#title-slide">Concurrency is not Parallelism (it’s better) - Rob Pike</a></li>
</ul>

<h2 id="what-does-ruby-support">What does Ruby support?</h2>

<h3 id="gil">GIL</h3>

<p>A global interpreter lock (GIL) is a mutual-exclusion lock held by a programming language interpreter thread to avoid sharing code that is not thread-safe with other threads. In implementations with a GIL, there is always one GIL for each interpreter process.</p>

<p>Global interpreter lock (GIL) is a mechanism used in computer language interpreters to synchronize the execution of threads so that only one native thread can execute at a time. An interpreter that uses GIL always allows exactly one thread to execute at a time, even if run on a multi-core processor.</p>

<p><strong>Benefits</strong></p>

<ul>
  <li>increased speed of single-threaded programs (no necessity to acquire or release locks on all data structures separately)</li>
  <li>easy integration of C libraries that usually are not thread-safe</li>
  <li>ease of implementation</li>
</ul>

<p><strong>Drawbacks</strong></p>

<p>Limits the amount of parallelism reachable through concurrency of a single interpreter process with multiple threads. Hence a significant slowdown for CPU-bound thread.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCIT-native_threads.png" alt="RCIT-native_threads.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCIT-threads_with_GIL.png" alt="RCIT-threads_with_GIL.png" /></p>

<p><em>Reference</em></p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Global_interpreter_lock">Global Interpreter Lock - Wikipedia</a></li>
</ul>

<h3 id="ruby-support">Ruby Support</h3>

<p>Ruby 1.8, uses only a single native thread and runs all Ruby threads within that one native thread. A single OS thread is allocated for the Ruby interpreter, a GIL lock is instantiated, and Ruby threads (‘Green Threads’), are spooled up by our program. This means that threads can never run in parallel, even on multicore CPUs.</p>

<p>Ruby 1.9, allocates a native thread for each Ruby thread. But because some of the C libraries used in this implementation are not themselves thread-safe. Ruby never allows more than one of its native threads to run at the same time. Now the GIL is the bottleneck, and Ruby will never take advantage of multiple cores!</p>

<p>Ruby 1.9, also provides Fiber.</p>

<p>Ruby concurrency without parallelism can still be very useful, though, for tasks that are IO-heavy (e.g., network I/O, disk I/O).  Ruby can release the lock on the GIL on that thread while it blocks on I/O. There is a reason threads were, after all, invented and used even before multi-core servers were common.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCIT-ruby_support.png" alt="RCIT-ruby_support.png" /></p>

<p><em>Reference</em></p>

<ul>
  <li><a href="https://blog.engineyard.com/2011/ruby-concurrency-and-you">Ruby, Concurrency, and You - Engine Yard</a></li>
</ul>

<h3 id="fiber">Fiber</h3>

<p>Fibers are primitives for implementing light weight cooperative concurrency in Ruby (think lightweight threads, minus the thread scheduler and less overhead). Basically they are a means of creating code blocks that can be paused and resumed, much like threads. A fiber is a unit of execution that must be manually scheduled by the application. Fibers run in the context of the threads that schedule them. Each thread can schedule multiple fibers.</p>

<p>As opposed to other stackless light weight concurrency models, each fiber comes with a small 4KB stack. This enables the fiber to be paused from deeply nested function calls within the fiber block.</p>

<p>Normal usage: start an async operation, yield the fiber, and then make the callback resume the fiber once the operation is complete.</p>

<p><strong>Compered to Thread</strong></p>

<p>Fibers are never preempted, the scheduling must be done by the programmer and not the VM.</p>

<p><strong>Why Fiber?</strong></p>

<p>In general, fibers do not provide advantages over a well-designed multithreaded application. However, using fibers can make it easier to port applications that were designed to schedule their own threads. The availability of Fibers allows Actor-style programming, without having to worry about overhead.</p>

<p><strong>Why Fiber is called a semi-coroutine?</strong></p>

<p>Coroutines (cooperative multitasking) are computer program components that generalize subroutines for nonpreemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.</p>

<p>Asymmetric Coroutines can only transfer control back to their caller, where Coroutines are free to transfer control to any other Coroutine, as long as they have a handle to it.</p>

<p>We may infer that Ruby encapsulate a Fiber::Core which supports coroutine, and only expose Fiber as a semi-coroutine data structure.</p>

<p><strong>What’s the performance of Fiber?</strong></p>

<p>Fibers are much faster to create than threads, they eat much less memory too.</p>

<p><em>Reference</em></p>

<ul>
  <li><a href="http://ruby-doc.org/core-2.2.2/Fiber.html">Fiber - Ruby Doc</a></li>
  <li><a href="http://www.infoq.com/news/2007/08/ruby-1-9-fibers">Ruby 1.9 adds Fibers for lightweight concurrency - Werner Schuster</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Coroutine#Implementations_for_Ruby">Coroutine - Wikipedia</a></li>
  <li><a href="http://oldmoe.blogspot.jp/2008/08/ruby-fibers-vs-ruby-threads.html">Ruby Fibers Vs Ruby Threads - oldmoe</a></li>
</ul>

<h2 id="how-to-enhance-concurrency-by-ruby">How to enhance concurrency by Ruby?</h2>

<h3 id="basics">Basics</h3>

<p><strong>How to provide more concurrency?</strong></p>

<ul>
  <li>Multi processing (parallelism), like Resque, Unicorn. Simply to fork a running process to multiply its processing power.</li>
  <li>Multi threading, like Sidekiq, Puma and Thin. Lighter than processes, requiring less overhead. At some point, you may find it necessary to use a thread pool.</li>
  <li>Background processing</li>
  <li>Rely on other concurrency models (event, actor, message-passing)</li>
</ul>

<p><strong>Multi-processing vs. Multi-threading</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCIT-multi_processing_vs_multi_threading.png" alt="RCIT-multi_processing_vs_multi_threading.png" /></p>

<p><strong>Thread Pooling</strong></p>

<p>A key configuration parameter for a thread pool is typically the number of threads in the pool. These threads can either be instantiated all at once (i.e., when the pool is created) or lazily (i.e., as needed until the maximum number of threads in the pool has been created).</p>

<p><code>Queue</code> and <code>SizedQueue</code> are thread-safe data structures in Ruby, maybe the only two.</p>

<p><a href="https://gist.github.com/ifyouseewendy/a8fc663ae575843f9e8f">demo snippet</a></p>

<p><em>Reference</em></p>

<ul>
  <li><a href="https://www.igvita.com/2010/08/18/multi-core-threads-message-passing/">Multi-core, Threads &amp; Message Passing - Ilya Grigorik</a></li>
  <li><a href="http://adam.herokuapp.com/past/2009/8/13/threads_suck/">Threads Suck -  Adam Wiggins</a></li>
  <li><a href="https://www.usenix.org/legacy/events/hotos03/tech/full_papers/vonbehren/vonbehren_html/index.html">Why Events Are A Bad Idea - Rob von Behren, Jeremy Condit and Eric Brewer</a></li>
</ul>

<h3 id="concurrency-model-software-transactional-memory">Concurrency Model: Software Transactional Memory</h3>

<p>Software transactional memory (STM) is a concurrency control mechanism analogous to database transactions for controlling access to shared memory in concurrent computing. It is an alternative to lock-based synchronization. STM is a strategy implemented in software, rather than as a hardware component.</p>

<ul>
  <li>A thread completes modifications to shared memory without regard for what other threads might be doing, recording every read and write that it is performing in a log.</li>
  <li>Instead of placing the onus on the writer to make sure it does not adversely affect other operations in progress, it is placed on the reader, who after completing an entire transaction verifies that other threads have not concurrently made changes to memory that it accessed in the past.</li>
  <li>This final operation, in which the changes of a transaction are validated and, if validation is successful, made permanent, is called a commit. A transaction may also abort at any time, causing all of its prior changes to be rolled back or undone. If a transaction cannot be committed due to conflicting changes, it is typically aborted and re-executed from the beginning until it succeeds.</li>
</ul>

<p>Clojure has STM support built into the core language.</p>

<p><em>Reference</em></p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Software_transactional_memory">Software transactional memory - Wikipedia</a></li>
</ul>

<h3 id="concurrency-model-actor-model">Concurrency Model: Actor Model</h3>

<p>The actor model has its theoretical roots in concurrency modelling and message passing concepts.</p>

<p>The basic operation of an Actor is easy to understand: like a thread, it runs concurrently with other Actors. However, unlike threads it is not pre-emptable. Instead, each Actor has a mailbox and can call a routine named “receive” to check its mailbox for new messages. The “receive” routine takes a filter, and if no messages in an Actor’s mailbox matches the filter, the Actor sleeps until it receives new messages, at which time it’s rescheduled for execution.</p>

<p>Well, that’s a bit of a naive description. In reality the important part about Actors is that they cannot mutate shared state simultaneously. That means there are no race conditions or deadlocks because there are no mutexes, conditions, and semaphores, only messages and mailboxes.</p>

<p>Actors are an approach to concurrency which has proven remarkably successful in languages like Erlang and Scala. They emphasize message passing as the only means of exchanging state, as opposed to threaded approaches like mutexes, conditions, and semaphores which hopefully guard access and mutation of any shared state, emphasis on the hopefully. Using messaging eliminates several problems in multithreaded programming, including many types of race conditions and deadlocks which result from hope dying in the cold light of reality.</p>

<p><strong>Message Passing</strong></p>

<p>The fundamental idea of the actor model is to use actors as concurrent primitives that can act upon receiving messages in different ways:</p>

<ul>
  <li>Send a finite number of messages to other actors.</li>
  <li>Spawn a finite number of new actors.</li>
  <li>Change its own internal behavior, taking effect when the next incoming message is handled.</li>
</ul>

<p>For communication, the actor model uses asynchronous message passing. In particular, it does not use any intermediate entities such as channels. Instead, each actor possesses a mailbox and can be addressed. These addresses are not to be confused with identities, and each actor can have no, one or multiple addresses. When an actor sends a message, it must know the address of the recipient. In addition, actors are allowed to send messages to themselves, which they will receive and handle later in a future step.</p>

<p>Messages are sent asynchronously and can take arbitrarily long to eventually arrive in the mailbox of the receiver. Also, the actor models makes no guarantees on the ordering of messages. Queuing and dequeuing of messages in a mailbox are atomic operations, so there cannot be a race condition.</p>

<p>There is no shared state and the interaction between actors is purely based on asynchronous messages.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/RCIT-actor_message_passing.png" alt="RCIT-actor_message_passing.png" /></p>

<p><strong>Implementation</strong></p>

<ul>
  <li>Thread-based Actors - the actor is internally backed by a dedicated thread. This obviously limits scalability and requires the thread to suspend and block when waiting for new messages.</li>
  <li>Event-driven Actors - which does not directly couple actors to threads. Instead, a thread pool can be used for a number of actors. This approach uses a continuation closure to encapsulate the actor and its state. Conceptually, this implementation is very similar to an event loop backed by a threadpool.</li>
</ul>

<p><strong>Reactor Pattern</strong></p>

<p>The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers.</p>

<p>The reactor pattern completely separates application specific code from the reactor implementation, which means that application components can be divided into modular, reusable parts. Also, due to the synchronous calling of request handlers, the reactor pattern allows for simple coarse-grain concurrency while not adding the complexity of multiple threads to the system.</p>

<p><em>Reference</em></p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Actor_model">Actor Model - Wikipedia</a></li>
  <li><a href="http://revactor.github.io/philosophy/">Philosophy - Revactor</a></li>
  <li><a href="http://on-ruby.blogspot.jp/2008/01/ruby-concurrency-with-actors.html">Ruby Concurrency with Actors - Pat Eyler</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Reactor_pattern">Reactor Pattern - Wikipedia</a></li>
  <li><a href="http://berb.github.io/diploma-thesis/original/054_actors.html#02">Actor-based Concurrency - Benjamin Erb</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Working With Ruby Threads]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/02/16/review-working-with-ruby-threads/"/>
    <updated>2016-02-16T13:07:05+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/02/16/review-working-with-ruby-threads</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Working With Ruby Threads</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.jstorimer.com/">Jesse Storimer</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://www.jstorimer.com/products/working-with-ruby-threads">www.jstorimer.com/products/working-with-ruby-threads</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#concurrent--parallel">Concurrent != Parallel</a></li>
  <li><a href="#the-gil-and-mri">The GIL and MRI</a></li>
  <li><a href="#thread-execution">Thread Execution</a></li>
  <li><a href="#mutual-exclusion">Mutual Exclusion</a></li>
  <li><a href="#create-a-pending-order-for-100">Create a pending order for $100</a></li>
  <li><a href="#ask-5-threads-to-check-the-status-and-collect">Ask 5 threads to check the status, and collect</a></li>
  <li><a href="#with-this-line-its-guaranteed-that-this-value-is">With this line, it’s guaranteed that this value is</a>    <ul>
      <li><a href="#signaling-threads-with-condition-variables">Signaling Threads with Condition Variables</a></li>
      <li><a href="#thread-safe-data-structures">Thread-safe Data Structures</a></li>
      <li><a href="#writing-thread-safe-code">Writing Thread-safe Code</a></li>
      <li><a href="#wrap-your-threads-in-an-abstraction">Wrap Your Threads in an Abstraction</a></li>
    </ul>
  </li>
  <li><a href="#these-behave-like-regular-method-calls">these behave like regular method calls</a></li>
  <li><a href="#this-will-fire-the-next-method-without">this will fire the <code>next</code> method without</a></li>
  <li><a href="#celluloid-kicks-off-that-method-asynchronously-and-returns-you-a-celluloidfuture-object">Celluloid kicks off that method asynchronously and returns you a Celluloid::Future object.</a></li>
  <li><a href="#calling-value-on-that-future-object-will-block-until-the-value-has-been-computed">Calling #value on that future object will block until the value has been computed.</a>    <ul>
      <li><a href="#into-the-wild">Into The Wild</a></li>
      <li><a href="#closing">Closing</a></li>
    </ul>
  </li>
</ul>

<h2 id="concurrent--parallel">Concurrent != Parallel</h2>

<ul>
  <li>Making it execute in parallel is out of your hands. That responsibility is left to the underlying thread scheduler.</li>
  <li>Making it concurrent, you enable it to be parallelized when the underlying system allows it.</li>
</ul>

<p>Example</p>

<ol>
  <li>You could complete Project A today, then complete Project B tomorrow. (Serial)</li>
  <li>You could work on Project A for a few hours this morning, then switch to Project B for a few hours this afternoon, and then do the same thing tomorrow. Both projects will be finished at the end of the second day. (Concurrent)</li>
  <li>Your agency could hire another programmer. He could work on Project B and you could work on Project A. Both projects will be finished at the end of the first day. (Concurrent &amp;&amp; Parallel)</li>
</ol>

<h2 id="the-gil-and-mri">The GIL and MRI</h2>

<p><strong>MRI allows concurrent execution of Ruby code, but prevents parallel execution of Ruby code.</strong></p>

<p>The GIL prevents parallel execution of Ruby code, but it doesn’t prevent concurrent execution of Ruby code. Remember that concurrent code execution is possible even on a single core CPU by giving each thread a turn with the resources.</p>

<p>MRI doesn’t let a thread hog the GIL when it hits blocking IO. This is a no-brainer optimization for MRI. When a thread is blocked waiting for IO, it won’t be executing any Ruby code. Hence, when a thread is blocking on IO, it releases the GIL so another thread can execute Ruby code.</p>

<p>Example</p>

<p><code>ruby
require 'open-uri'
3.times.map do
  Thread.new do
    open('http://zombo.com')
  end
end.each(&amp;:value)
</code></p>

<p>Thread A gets the GIL. It starts executing Ruby code. It gets down to Ruby’s Socket APIs and attempts to open a connection to zombo.com. At this point, while Thread A is waiting for its response, it releases the GIL. Now Thread B acquires the GIL and goes through the same steps.</p>

<p>Meanwhile, Thread A is still waiting for its response. Remember that the threads can execute in parallel, so long as they’re not executing Ruby code. So it’s quite possible for Thread A and Thread B to both have initiated their connections, and both be waiting for a response.</p>

<p>Under the hood, each thread is using a ppoll(2) system call to be notified when their connection attempt succeeds or fails. When the ppoll(2) call returns, the socket will have some data ready for consumption. At this point, the threads will need to execute Ruby code to process the data. So now the whole process starts over again.</p>

<p><strong>Why GIL Exists?</strong></p>

<p>MRI core developers have been calling the GIL a feature for some time now, rather than a bug. There are three reasons that the GIL exists:</p>

<ul>
  <li>To protect MRI internals from race conditions. The same issues that can happen in your Ruby code can happen in MRI’s C code. When it’s running in a multithreaded context, it will need to protect critical parts of the internals with some kind of synchronization mechanism.</li>
  <li>To facilitate the C extension API</li>
  <li>To reduce the likelihood of race conditions in your Ruby code. It’s important to note that the GIL only reduces entropy here; it can’t rule it out all together. It’s a bit like wearing fully body armour to walk down the street: it really helps if you get attacked, but most of the time it’s just confining.</li>
</ul>

<p><strong>MRI with blocking IO encourages a context switch while waiting for the thread to print to stdout</strong></p>

<p>```ruby
@counter = 0</p>

<p>5.times.map do
  Thread.new do
    temp = @counter
    temp = temp + 1
    @counter = temp
  end
end.each(&amp;:join)</p>

<p>puts @counter
```</p>

<p>With no synchronization, even with a GIL, it’s possible that a context switch happens between incrementing temp and assigning it back to counter. If this is the case, it’s possible that two threads assign the same value to counter. In the end the result of this little snippet could be less than 5.</p>

<p>It’s rare to get an incorrect answer using MRI with this snippet, but almost guaranteed if you use JRuby or Rubinius. If you insert a puts in the middle of the block passed to Thread.new, then it’s very likely that MRI will produce an incorrect result. Its behaviour with blocking IO encourages a context switch while waiting for the thread to print to stdout.</p>

<p><strong>Compare to JRuby, and Rubinius</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/WWRT-multi_thread_prime_number_generation.png" alt="WWRT-multi_thread_prime_number_generation.png" /></p>

<p>GIL makes MRI run faster in single-threaded way, as no need to accquire or release locks for data structures. But also makes MRI run slower in multi-threaded way, as disabling on parellelism.</p>

<p>JRuby and Rubinius do indeed protect their internals from race conditions. But rather than wrapping a lock around the execution of all Ruby code, they protect their internal data structures with many fine-grained locks. Rubinius, for instance, replaced their GIL with about 50 fine-grained locks.</p>

<h2 id="thread-execution">Thread Execution</h2>

<p><strong>Threads in Ruby</strong></p>

<p>There’s always at least one: the main thread. The main thread has one special property that’s different from other threads. When the main thread exits, all other threads are immediately terminated and the Ruby process exits.</p>

<p>The most important concept to grasp is that threads have a shared address space. A race condition involves two threads racing to perform an operation on some shared state.</p>

<p><code>Thread#join</code></p>

<p>When one thread raises an unhandled exception, it terminates the thread where the exception was raised, but doesn’t affect other threads. Similarly, a thread that crashes from an unhandled exception won’t be noticed until another thread attempts to join it.</p>

<p><code>Thread#status</code></p>

<ul>
  <li>run: Threads currently running have this status.</li>
  <li>sleep: Threads currently sleeping, blocked waiting for a mutex, or waiting on IO, have this status.</li>
  <li>false: Threads that finished executing their block of code, or were successfully killed, have this status.</li>
  <li>nil: Threads that raised an unhandled exception have this status.</li>
  <li>aborting: Threads that are currently running, yet dying, have this status.</li>
</ul>

<p><code>Thread.stop</code></p>

<p>This method puts the current thread to sleep and tells the thread scheduler to schedule some other thread. It will remain in this sleeping state until its alternate, Thread#wakeup is invoked.</p>

<p><code>Thread.pass</code></p>

<p>It asks the thread scheduler to schedule some other thread. Since the current thread doesn’t sleep, it can’t guarantee that the thread scheduler will take the hint.</p>

<p>Avoid <code>Thread#raise</code> and <code>Thread#kill</code></p>

<p>It doesn’t properly respect ensure blocks, which can lead to nasty problems in your code.</p>

<p><strong>How Many Threads Are Too Many?</strong></p>

<p>It depends, there will be a sweet spot between utilizing available resources and context switching overhead.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/WWRT-io_bound.png" alt="WWRT-io_bound.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/WWRT-cpu_bound.png" alt="WWRT-cpu_bound.png" /></p>

<p>CPU-bound code is inherently bound by the rate at which the CPU can execute instructions. Creating more threads isn’t necessarily faster. On the other hand, introducing more threads improved performance in these two examples by anywhere between 100% and 600%. Finding that sweet spot is certainly worth it.</p>

<p><strong>Thread safety</strong></p>

<p>When your code isn’t thread-safe, the worst that can happen is that your underlying data becomes incorrect, yet your program continues as if it were correct.</p>

<p>The computer is unaware of thread-safety issues. The onus is on you to notice these problems and deal with them. This is one of the hardest problems when it comes to thread safety. There are no exceptions raised or alarm bells rung when the underlying data is no longer correct. Even worse, sometimes it takes a heavy load to expose a race condition like this.</p>

<p>Any concurrent modifications to the same object are not thread-safe.</p>

<h2 id="mutual-exclusion">Mutual Exclusion</h2>

<p><strong>Demo Snippet</strong></p>

<p>```ruby
# This class represents an ecommerce order
class Order
  attr_accessor :amount, :status</p>

<p>def initialize(amount, status)
    @amount, @status = amount, status
  end</p>

<p>def pending?
    status == ‘pending’
  end</p>

<p>def collect_payment
    puts “Collecting payment…”
    self.status = ‘paid’
  end
end</p>

<h1 id="create-a-pending-order-for-100">Create a pending order for $100</h1>
<p>order = Order.new(100.00, ‘pending’)
mutex = Mutex.new</p>

<h1 id="ask-5-threads-to-check-the-status-and-collect">Ask 5 threads to check the status, and collect</h1>
<p># payment if it’s ‘pending’
5.times.map do
  Thread.new do
    mutex.synchronize do
      if order.pending?
        order.collect_payment
      end
    end
  end
end.each(&amp;:join)
```</p>

<p>The block of code inside of a <code>Mutex#synchronize</code> call is often called a critical section, pointing to the fact that this code accesses a shared resource and must be handled correctly.</p>

<p><strong>Memory Visibility (Volatility)</strong></p>

<p>```ruby
# With this line, it’s possible that another thread
# updated the status already and this value is stale
status = order.status</p>

<h1 id="with-this-line-its-guaranteed-that-this-value-is">With this line, it’s guaranteed that this value is</h1>
<p># consistent with any changes in other threads
status = mutex.synchronize { order.status }
```</p>

<p>The reason for this is due to low-level details. The kernel can cache in, for instance, L2 cache before it’s visible in main memory. It’s possible that after the status has been set to ‘paid,’ by one thread, another thread could still see the Order#status as ‘pending’ by reading the value from main memory before the change has propagated there.</p>

<p>The solution to this is something called a memory barrier. Mutexes are implemented with memory barriers, such that when a mutex is locked, a memory barrier provides the proper memory visibility semantics.</p>

<p>Scenarios around memory visibility are difficult to understand and reason about. That’s one reason other programming languages have defined something called a memory model, a well-defined specification describing how and when changes to memory are visible in other threads.</p>

<p>Ruby has no such specification yet, so situations like this are tricky to reason about and may even yield different results with different runtimes. That being said, <strong>mutexes carry an implicit memory barrier</strong>. So, if one thread holds a mutex to write a value, other threads can lock the same mutex to read it and they will see the correct, most recent value.</p>

<p><strong>Performance</strong></p>

<p>Mutexes inhibit parallelism. Restrict the critical section to be as small as possible, while still preserving the safety of your data.</p>

<p><strong>The dreaded deadlock</strong></p>

<p>The <code>try_lock</code> method attempts to acquire the mutex, just like the lock method. But unlike lock, try_lock will not wait if the mutex isn’t available. If another thread already owns the mutex, try_lock will return false. If it successfully acquires the mutex, try_lock will return true.</p>

<p>The downside to this approach is that another kind of issue can arise: <strong>livelocking</strong>. A livelock is similar to a deadlock in that the system is not progressing, but rather than threads stuck sleeping, they would be stuck in some loop with each other with none progressing.</p>

<p>A better solution is to define a mutex hierarchy. In other words, <strong>any time that two threads both need to acquire multiple mutexes, make sure they do it in the same order</strong>.</p>

<h2 id="signaling-threads-with-condition-variables">Signaling Threads with Condition Variables</h2>

<p>Condition variables provide an inter-thread control flow mechanism. A classic usage pattern is Producer-Consumer.</p>

<p><strong>Demo Snippet</strong></p>

<p>```ruby
require ‘thread’
require ‘net/http’</p>

<p>mutex    = Mutex.new
condvar  = ConditionVariable.new
results  = Array.new</p>

<p>Thread.new do
  10.times do
    response = Net::HTTP.get_response(‘dynamic.xkcd.com’, ‘/random/comic/’)
    random_comic_url = response[‘Location’]</p>

<pre><code>mutex.synchronize do
  results &lt;&lt; random_comic_url
  puts 'Produced result'
  condvar.signal # Signal the ConditionVariable
end   end end
</code></pre>

<p>comics_received = 0</p>

<p>until comics_received &gt;= 10
  mutex.synchronize do
    while results.empty?
      condvar.wait(mutex)
    end</p>

<pre><code>url = results.shift
puts "You should check out #{url}"   end
</code></pre>

<p>comics_received += 1
end
```</p>

<ol>
  <li><code>ConditionVariable#signal</code> will wake up exactly one thread that’s waiting on this ConditionVariable.</li>
  <li><code>ConditionVariable#broadcast</code> will wake up all threads currently waiting on this ConditionVariable.</li>
</ol>

<h2 id="thread-safe-data-structures">Thread-safe Data Structures</h2>

<p><strong>Implementing a thread-safe, blocking queue</strong></p>

<p>```ruby
require ‘thread’</p>

<p>class BlockingQueue
  attr_reader :queue, :mutex, :cv</p>

<p>def initialize
    @queue = Array.new
    @mutex = Mutex.new
    @cv    = ConditionVariable.new
  end</p>

<p>def push(ele)
    @mutex.synchronize do
      @queue.push ele
      @cv.signal
    end
  end</p>

<p>def pop
    @mutex.synchronize do
      while @queue.empty?
        @cv.wait(@mutex)
      end</p>

<pre><code>  @queue.pop
end   end end
</code></pre>

<p>bq = BlockingQueue.new</p>

<p>bq.push ‘a’
bq.push ‘b’</p>

<p>loop do
  puts bq.pop
end
```</p>

<p><strong>Queue, from the standard lib</strong></p>

<p>This is the only thread-safe data structure that ships with Ruby. Queue is very useful because of its blocking behaviour. Typically, you would use a Queue to distribute workloads to multiple threads, with one thread pushing to the queue, and multiple threads popping.</p>

<p><strong>Array and Hash</strong></p>

<p>Ruby doesn’t ship with any thread-safe Array or Hash implementations. Thread-safety concerns would add overhead to their implementation, which would hurt performance for single-threaded use cases.</p>

<p>You might be thinking: “With all of the great concurrency support available to Java on the JVM, surely the JRuby Array and Hash are thread-safe?” They’re not. For the exact reason mentioned above, using a thread-safe data structure in a single-threaded context would reduce performance.</p>

<p><strong>Immutable data structures</strong></p>

<p>When you need to share objects between threads, share immutable objects. It’s very easy to pass out immutable objects to share, but if you need to have multiple threads modifying an immutable object you still need some form of synchronization.</p>

<p>Immutability is a nice guarantee to have, it’s the simplest path to thread safety when sharing objects.</p>

<p>```ruby
require ‘hamster/queue’
require ‘atomic’</p>

<p>@queue_wrapper = Atomic.new(Hamster::Queue.new)</p>

<p>30.times do
  @queue_wrapper.update { |queue|
    queue.enqueue(rand(100))
  }
end</p>

<p>consumers = []</p>

<p>3.times do
  consumers « Thread.new do
    10.times do
      number = nil
      @queue_wrapper.update { |queue|
        number = queue.head
        queue.dequeue
      }</p>

<pre><code>  puts "The cubed root of #{number} is #{Math.cbrt(number)}"
end   end end
</code></pre>

<p>consumers.each(&amp;:join)
```</p>

<h2 id="writing-thread-safe-code">Writing Thread-safe Code</h2>

<p>Any guideline has exceptions, but it’s good to know when you’re breaking one, and why.</p>

<p>Idiomatic Ruby code is most often thread-safe Ruby code.</p>

<p><strong>Avoid mutating globals</strong></p>

<p>Any time there is only one shared instance (aka. singleton), it’s a global.</p>

<p>There are other things that fit this definition in Ruby:</p>

<ul>
  <li>Constants</li>
  <li>The AST</li>
  <li>Class variables/methods</li>
</ul>

<p>A slightly more nefarious example is the AST. Ruby, being such a dynamic language, allows you to change this at runtime. I don’t imagine this would be a common problem, but I saw it come up as an issue with the kaminari rubygem. Some part of the code was defining a method dynamically, then calling alias_method with that method, then removing it.</p>

<p>Again, this has to be a rare example, but it’s good to keep in mind that modifying the AST at runtime is almost always a bad idea, especially when multiple threads are involved. When I say ‘runtime’, I mean during the course of the lifecycle of the application. In other words, it’s expected that the AST will be modified at startup time, most Ruby libraries depend on this behaviour in some way. However, in the case of a Rails application, once it’s been initialized, changes to the AST shouldn’t happen at runtime, just as it’s rare to require new files in the midst of a controller action.</p>

<p><strong>Create more objects, rather than sharing one</strong></p>

<ul>
  <li>Thread-locals</li>
  <li>Connection pools</li>
</ul>

<p>A thread-local lets you define a variable that is global to the scope of the current thread. In other words, it’s a global variable that is locally scoped on a per-thread basis.</p>

<p><code>ruby
# Instead of
$redis = Redis.new
# use
Thread.current[:redis] = Redis.new
</code></p>

<p>It’s perfectly acceptable to tell users of your API that they should create one object for each thread, rather than trying to write difficult, thread-safe code that will increase your maintainenace costs.</p>

<p>This N:N connection mapping is fine for small numbers of threads, but gets out of hand when the number of threads starts to increase. For connections, a pool is often a better abstraction.</p>

<p>Resource pool still ensures that your threads aren’t sharing a single connection, but doesn’t require each thread to have its own. Implementing a connection pool is a good exercise in thread-safe programming, you’ll probably need to make use of both thread-locals and mutexes to do it safely.</p>

<p><strong>Avoid lazy loading</strong></p>

<p>A common idiom in Ruby on Rails applications is to lazily load constants at runtime, using something similar to Ruby’s <code>autoload</code>. But <code>autoload</code> in MRI is not thread-safe. It is thread-safe in recent versions of JRuby, but the best practice is simply to eager load files before spawning worker threads. This is done implicitly in Rails 4+, and can be enabled in Rails 3.x using the ` config.threadsafe!` configuration setting.</p>

<p><strong>Prefer data structures over mutexes</strong></p>

<p>Mutexes are notoriously hard to use correctly. For better or worse, you have a lot of things to decide when using a mutex.</p>

<ul>
  <li>How coarse or fine should this mutex be?</li>
  <li>Which lines of code need to be in the critical section?</li>
  <li>Is a deadlock possible here?</li>
  <li>Do I need a per-instance mutex? Or a global one?</li>
</ul>

<p>By leaning on a data structure, you remove the burden of correct synchronization from your code and depend on the semantics of the data structure to keep things consistent.</p>

<p>This only works if you choose not to share objects between threads directly. Rather than letting threads access shared objects and implementing the necessary synchronization, you pass shared objects through data structures.</p>

<p><strong>Finding bugs</strong></p>

<p>Like most bugs, if you can reproduce the issue, you can almost certainly track it down and fix it. However, some thread-safety issues may appear in production under heavy load, but can’t be reproduced locally. In this case, there’s no better solution than grokking the code.</p>

<p>Look at the code and assume that 2 threads will be accessing it simulatneously. Step through the possible scenarios. It can be helpful to jot these things down somewhere.</p>

<p><strong>Thread-safety on Rails</strong></p>

<ul>
  <li>Gem dependencies</li>
  <li>The request is the boundary. Don’t share objects between requests.</li>
</ul>

<p>A good example of this is something like a <code>User.current</code> reference.</p>

<p>If you really need a global reference, follow the guidelines from the last chapter. Try using a thread-local, or else a thread-aware object that will preserve data correctness.</p>

<p>The same heuristic is applicable to a background job processor. Each job will be handled by a separate thread. A thread may process multiple jobs in its lifetime, but a job will only be processed by a single thread in its lifecycle.</p>

<p>Again, the path to thread safety is clear: create the necessary objects that you need in the body of the job, rather than sharing any global state.</p>

<h2 id="wrap-your-threads-in-an-abstraction">Wrap Your Threads in an Abstraction</h2>

<p><strong>Single level of abstraction</strong></p>

<p><code>ruby
module Enumerable
  def concurrent_each
    threads = []
    each do |element|
      threads &lt;&lt; Thread.new { end yield element }
      threads.each(&amp;:join)
    end
  end
end
</code></p>

<p>This is a simple wrapper around Enumerable#each that will spawn a thread for each element being iterated over. It wouldn’t be wise to use this code in production yet because it has no upper bound on the number of threads it will spawn.</p>

<p><strong>Actor model</strong></p>

<p>At a high level, an Actor is a long-lived ‘entity’ that communicates by sending messages.</p>

<p>In the Actor model, each Actor has an ‘address’. If you know the address of an Actor, you can send it a message. These messages go to the Actor’s mailbox, where they’re processed asynchronously when the Actor gets around to it.</p>

<p>What sets Celluloid apart is that it takes this conceptual idea of the Actor model and marries it to Ruby’s object model.</p>

<p>```ruby
require ‘celluloid/autostart’
require ‘net/http’</p>

<p>class XKCDFetcher
  include Celluloid</p>

<p>def next
    response = Net::HTTP.get_response(‘dynamic.xkcd.com’, ‘/random/comic/’)
    random_comic_url = response[‘Location’]</p>

<pre><code>random_comic_url   end end ```
</code></pre>

<p>Including the Celluloid module into any Ruby class will turn instances of that class into full-fledged Celluloid actors. When you create a new actor, you immediately know its ‘address’. So long as you hold a reference to that object, you can send it messages. In Celluloid, sending messages to an actor equates to calling methods on an object.</p>

<p>```ruby
# this spawns a new thread containing a Celluloid actor
fetcher = XKCDFetcher.new</p>

<h1 id="these-behave-like-regular-method-calls">these behave like regular method calls</h1>
<p>fetcher.object_id
fetcher.inspect</p>

<h1 id="this-will-fire-the-next-method-without">this will fire the <code>next</code> method without</h1>
<p># waiting for its result
fetcher.async.next
fetcher.async.next</p>

<h1 id="celluloid-kicks-off-that-method-asynchronously-and-returns-you-a-celluloidfuture-object">Celluloid kicks off that method asynchronously and returns you a Celluloid::Future object.</h1>
<p>futures = []
10.times do
  futures « fetcher.future.next
end</p>

<h1 id="calling-value-on-that-future-object-will-block-until-the-value-has-been-computed">Calling #value on that future object will block until the value has been computed.</h1>
<p>futures.each do |future|
  puts “You should check out #{future.value}”
end
```</p>

<h2 id="into-the-wild">Into The Wild</h2>

<p><strong>How Sidekiq Uses Celluloid</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/WWRT-how_sidekiq_uses_celluloid.png" alt="WWRT-how_sidekiq_uses_celluloid.png" /></p>

<p>The most obvious difference I see between the Sidekiq codebase and a more traditional Ruby codebase is the lack of dependence upon return values.</p>

<p><strong>Puma’s Thread Pool Implementation</strong></p>

<p>At Puma’s multi-threaded core is a thread pool implementation. Once initialized, the pool is responsible for receiving work and feeding it to an available worker thread. The ThreadPool also has an auto-trimming feature, whereby the number of active threads is kept to a minimum, but more threads can be spawned during times of high load. Afterwards, the thread pool would be trimmed down to the minimum again.</p>

<h2 id="closing">Closing</h2>

<p>The safest path to concurrency: (from JRuby wiki)</p>

<ol>
  <li>Don’t do it.</li>
  <li>If you must do it, don’t share data across threads.</li>
  <li>If you must share data across threads, don’t share mutable data.</li>
  <li>If you must share mutable data across threads, synchronize access to that data.</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Concurrency - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces/"/>
    <updated>2015-12-26T11:33:57+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#concurrency">Concurrency</a>    <ul>
      <li><a href="#chapter-26---introduction">Chapter 26 - Introduction</a></li>
      <li><a href="#chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</a></li>
      <li><a href="#chapter-28---locks">Chapter 28 - Locks</a></li>
      <li><a href="#chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</a></li>
      <li><a href="#chapter-30---condition-variables">Chapter 30 - Condition Variables</a></li>
      <li><a href="#chapter-31---semaphores">Chapter 31 - Semaphores</a></li>
      <li><a href="#chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</a></li>
      <li><a href="#chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</a></li>
    </ul>
  </li>
</ul>

<h1 id="concurrency">Concurrency</h1>

<h2 id="chapter-26---introduction">Chapter 26 - Introduction</h2>

<p><strong>Background</strong></p>

<p>With time sharing, we can take a single physical CPU and turn it into multiple virtual CPUs, thus enabling the illusion of multiple programs running at the same time, through time sharing.</p>

<p>With paging (base and bounds, segmentation), we can create the illusion of a large, private virtual memory for each process; this abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk).</p>

<p>But the abstraction of running program we use along is the process, and it’s a classic view of a single point of execution within a program. Now we introduce a new abstraction, thread. And  a <strong>multi-threaded</strong> program has more than one point of execution.</p>

<p>Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus ca access the same data.</p>

<p><strong>Thread vs. Process</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_vs_process.png" alt="os-thread_vs_process.png" /></p>

<p>Address space</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_address_space.png" alt="os-thread_address_space.png" /></p>

<p><strong>Advantage</strong></p>

<p>Efficiency, as they share the same address space.</p>

<ul>
  <li>Save storage</li>
  <li>Easy context switching (no need to change page)</li>
</ul>

<p><strong>Issues</strong></p>

<ul>
  <li><strong>Sharing data</strong>, that of accessing shared variables and the need to support atomicity for critical sections.</li>
  <li><strong>Waiting for another</strong>, sleeping and waking interaction, where one thread must wait for another to complete some action before it continues.</li>
</ul>

<p><strong>Shared Data</strong></p>

<p>The heart of the problem is <strong>uncontrolled scheduling</strong>.</p>

<p>It is a wonderful and hard problem, and should make your mind hurt (a bit). If it doesn’t, then you don’t understand! Keep working until your head hurts; you then know you’re headed in the right directinn.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_sharing_data.png" alt="os-thread_sharing_data.png" /></p>

<p><strong>Key Concurrency Terms</strong> (from Edsger Dijkstra)</p>

<p>A <strong>critical section</strong> is a piece of code that accesses a shared resource, usually a variable or data structure.</p>

<p>A <strong>race condition</strong> arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps un- desirable) outcome. The results depend on the timing execution of the code.</p>

<p>An <strong>indeterminate</strong> program consists of one or more races onditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems.</p>

<p>To avoid these problems, threads should use some kind of <strong>mutual exclusion primitives</strong>; doing so guarantees that only a single thread ever enters a critical section, thus avoiding racoes, and resulting in deterministic program outputs.</p>

<p><strong>Atomic</strong></p>

<p>Atomic operations are one of the most powerful underlying techniques in building computer systems.</p>

<p>The idea behind making a series of actions <strong>atomic</strong> is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a <strong>transaction</strong>.</p>

<p>In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution.</p>

<p><strong>The Wish For Atomicity</strong></p>

<p>Hardware guarantees the instructions is atomic, and provide a general set we call <strong>synchronisation primitives</strong> to ensure atomicity.</p>

<p>Hardware guarantees that the instructions execute atomically. It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state.</p>

<p>But, would we really want the hardware to support an “atomic update of B-tree” instruction?</p>

<p>No. Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call <strong>synchronization primitives</strong>. By using these hardware synchronization primitives, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution.</p>

<p><strong>Why in OS Class?</strong></p>

<p>“History” is the one-word answer; the OS was the first concurrent program, and many techniques were created for use within the OS. Later, with multi-threaded processes, application programmers also had to consider such things.</p>

<p>OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.</p>

<h2 id="chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</h2>

<p><strong>Guidelines</strong></p>

<p>There are a number of small but important things to remember when you use the POSIX thread library.</p>

<ul>
  <li><strong>Keep it simple</strong>. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interactions lead to bugs.</li>
  <li>Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum.</li>
  <li><strong>Each thread has its own stack</strong>. If you have a locally-allocated variable inside of some function a thread is exe- cuting, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the values must be in the heap or otherwise some locale that is globally accessible.</li>
  <li><strong>Be careful with how you pass arguments to, and return values from, threads.</strong> In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong.</li>
  <li><strong>Check your return codes.</strong> Of course, in any C and UNIX program- ming you do, you should be checking each and every return code, and it’s true here as well.</li>
  <li><strong>Always use condition variables to signal between threads.</strong> While it is often tempting to use a simple flag, don’t do it.</li>
  <li><strong>Initialize locks and condition variables.</strong> Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways.</li>
  <li><strong>Use the manual pages.</strong> On Linux, in particular, the pthread man pages (man -k pthread) are highly informative and discuss much of the nuances pre- sented here, often in even more detail.</li>
</ul>

<p><strong>Thread Creation</strong></p>

<p><code>c
#include &lt;pthread.h&gt;
int pthread_create(pthread_t * thread,
                     const pthread_attr_t *  attr,
                     void * (*start_routine)(void*),
                     void *  arg);
</code></p>

<ul>
  <li><code>thread</code>, is a pointer to a structure of type pthread t; we’ll use this structure to interact with this thread</li>
  <li><code>attr</code>, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread.</li>
  <li>The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (<code>start routine</code>), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer).</li>
  <li><code>arg</code>, is exactly the argument to be passed to the function where the thread begins execution.</li>
</ul>

<p><strong><em>Why do we need these void pointers?</em></strong></p>

<p>Having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result.</p>

<p><strong>Thread Completion</strong></p>

<p><code>c
int pthread_join(pthread_t thread, void **value_ptr);
</code></p>

<ul>
  <li><code>thread</code> is used to specify which thread to wait for</li>
  <li><code>value_ptr</code> is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo.png" alt="os-thread_waiting_demo.png" /></p>

<p>Note that one has to be extremely careful with how values are returned from a thread. In particular, never return a pointer which refers to something allocated on the thread’s call stack.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo_wrong.png" alt="os-thread_waiting_demo_wrong.png" /></p>

<p>However, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results.</p>

<p>Not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely. Such long-lived programs thus may not need to join.</p>

<p><strong>Locks</strong></p>

<p>Providing mutual exclusion to a critical section via locks.</p>

<p><code>c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
</code></p>

<p>When you have a region of code you realize is a critical section, and thus needs to be pro- tected by locks in order to operate as desired.</p>

<p>```c
pthread_mutex_t lock;</p>

<p>Pthread_mutex_init(&amp;lock);</p>

<p>Pthread_mutex_lock(&amp;lock);
x = x + 1; // or whatever your critical section is
Pthread_mutex_unlock(&amp;olock);</p>

<p>// Always check for failure
void Pthread_mutex_init(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_init(&amp;lock, NULL); // dynamic initialisation, or PTHREAD_MUTEX_INITIALIZER
    assert(rc == 0); // always check success!
}
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_lock(mutex);
    assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_unlock(mutex);
    assert(rc == 0);
}
```</p>

<p><strong>Condition Variables</strong></p>

<p>Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue.</p>

<p>To use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of the above routines, this lock should be held.</p>

<p><code>c
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
</code></p>

<p>pthread_cond_wait(), puts the calling thread to sleep, ad thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about.</p>

<p><code>c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t  cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&amp;lock);
while (ready == 0)
    Pthread_cond_wait(&amp;cond, &amp;lock);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>After initialization of the relevant lock and condition, a thread checks to see if the variable ready has yet been set to something other than zero. If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.</p>

<p><code>c
Pthread_mutex_lock(&amp;lock);
ready = 1;
Pthread_cond_signal(&amp;cond);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>Notice 1</p>

<p>When signaling (as well as when modifying the global variable ready), we always make sure to have the lock held. This ensures that we don’t accidentally introduce a race condition into our code.</p>

<p>Notice 2</p>

<p>Notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep.</p>

<p>Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However, before returning after being woken, the pthread_cond_wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.</p>

<p>Notice 3</p>

<p>The waiting thread re-checks the condition in a while loop, instead of a simple if statement. Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not.</p>

<p>Notice 4</p>

<p>Don’t ever use these ad hoc synchronisations.</p>

<p>```c
// waitingnwhile (ready == 0)
    ; // spin</p>

<p>// signaling
ready = 1;
```</p>

<p>First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone.</p>

<p><strong>Others</strong></p>

<p>On the link line, you must also explicitly link with the pthreads library, by adding the -pthread flag.</p>

<p><code>sh
prompt&gt; gcc -o main main.c -Wall -pthread
</code></p>

<h2 id="chapter-28---locks">Chapter 28 - Locks</h2>

<p><strong>The Basic Idea</strong></p>

<p>Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction.</p>

<p>This lock variable (or just “lock” for short) holds the state of the lock at any instant in time. It is either available (or unlocked or free) and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_demo.png" alt="os-lock_demo.png" /></p>

<p>In general, we view thre
ads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code.</p>

<p>The name that the <strong>POSIX</strong> library uses for a lock is a <strong>mutex</strong>, as it is used to provide <strong>mutual exclusion</strong> between threads.</p>

<p><strong>Building A Lock</strong></p>

<p>Some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux).</p>

<p><strong>Evaluating Locks</strong></p>

<ul>
  <li>The first is whether the lock does its basic task, which is to provide <strong>mutual exclusion</strong>. Basically, does the lock work, preventing multiple threads from entering a critical section?</li>
  <li>The second is <strong>fairness</strong>. Does each thread contending for the lock get a fair shot at acquiring it once it is free?</li>
  <li>The final criterion is <strong>performance</strong>, specifically the time overheads added by using the lock.</li>
</ul>

<p><strong>Controlling Interrupts</strong></p>

<p>Turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow.</p>

<p><strong>Plain Solution</strong></p>

<p>Without hardware support, just use a flag.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_plain_solution.png" alt="os-lock_plain_solution.png" /></p>

<p>The core issue is that the testing and setting part can be interrupted by context switch, and both thread enters the critical section.</p>

<p>You should get used to this thinking about concurrent programming. Maybe pretend yourself as a <strong>malicious scheduler</strong> to understand the concurrent execution.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_no_mutal_exclusion.png" alt="os-lock_no_mutal_exclusion.png" /></p>

<p><strong>Test And Set (Atomic Exchange)</strong></p>

<p>Let hardware provides a transaction-like instrument to ensure the sequence of operations is performed <strong>atomically</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_test_and_set.png" alt="os-lock_test_and_set.png" /></p>

<p>The key, of course, is that this sequence of operations is performed atomically. The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_test_and_set.png" alt="os-lock_spin_lock_by_test_and_set.png" /></p>

<p>By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock.</p>

<p><strong>Compare-And-Swap</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_compare_and_swap.png" alt="os-lock_compare_and_swap.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_compare_and_swap.png" alt="os-lock_spin_lock_by_compare_and_swap.png" /></p>

<p>compare-and-swap is a more powerful instruction than test-and-set. We will make some use of this power in the future when we briefly delve into <strong>wait-free synchronisation</strong>.</p>

<p><strong>Load-Linked and Store-Conditional</strong></p>

<p>Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture, for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_load_linked_store_conditional.png" alt="os-lock_load_linked_store_conditional.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_load_linked_store_conditional.png" alt="os-lock_spin_lock_by_load_linked_store_conditional.png" /></p>

<p><strong>Fetch-And-Add</strong></p>

<p>Fetch-and-add atomically increments a value while returning the old value at a particular address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_fetch_and_add.png" alt="os-lock_fetch_and_add.png" /></p>

<p>Fetch-and-add could build a <em>ticket lock</em>, this solution uses a ticket and turn variable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (myturn). The globally shared lock-&gt;turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. It has the advantage of the fairness, ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_ticket_lock_by_fetch_and_add.png" alt="os-lock_ticket_lock_by_fetch_and_add.png" /></p>

<p><strong>Spin Lock</strong></p>

<p>We use a while loop to endlessly check the value of a flag, this technique is known as <strong>spin-waiting</strong>. Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)!</p>

<p><strong>Spin lock</strong> is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a <strong>preemptive scheduler</strong>. (Remember that SJF is non-preemptive, but STCF is preemptive, which means permitting one thread to be interrupted).</p>

<p>Evaluating</p>

<ul>
  <li>√ correctness, the spin lock only allows a single thread to enter the critical section at a time.</li>
  <li>X fairness, spin locks don’t provide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Spin locks are not fair and may lead to starvation.</li>
  <li>X performance, bad in the single CPU case. The problem gets worse with N threads contending for a lock; N − 1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock.</li>
</ul>

<p><strong>Avoid Spinning by Yield</strong></p>

<blockquote>
  <p>“just yield, baby!”</p>
</blockquote>

<p>Hardware support alone cannot solve the problem. We’ll need OS support too! Assume an operating system primitive <strong>yield()</strong> which a thread can call when it wants to give up the CPU and let another thread run. A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller from the <strong>running</strong> state to the <strong>ready</strong> state, and thus promotes another thread to running. Thus, the yielding process essentially <strong>deschedules</strong> itself.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield.png" alt="os-lock_with_test_and_set_and_yield.png" /></p>

<p>This approach eliminates the spinning time, but still costly when context switching. And we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section.</p>

<p><strong>Avoid Spnning by Queues</strong></p>

<p>The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread runs that must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation.</p>

<p>Thus, we must explicitly exert some control over who gets to acquire the lock next after the current holder releases it.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield_and_queue.png" alt="os-lock_with_test_and_set_and_yield_and_queue.png" /></p>

<p>This approach thus doesn’t avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.</p>

<p>With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the first thread would then sleep forever (potentially). This problem is sometimes called the <strong>wakeup/waiting race</strong>.</p>

<p>Solaris solves this problem by adding a third system call: <strong>setpark()</strong>. By calling this routine, a thread can indicate it is about to park. If it then happens t be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.</p>

<p>You might also notice the interesting fact that the flag does not get set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from park(); however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; flag is not set to 0 in-between.</p>

<p><strong>Linux Support</strong></p>

<p>Linux provides something called a <strong>futex</strong> which is similar to the Solaris interface but provides a bit more in-kernel functionality. Specifically, each futex has associated with it a specific physical memory location; associated with each such memory location is an in-kernel queue.</p>

<ul>
  <li><code>futex_wait(address, expected)</code> puts the calling thread to sleep, assouming the value at address is equal to expected. If it is not equal, the call returns immediately.</li>
  <li><code>futex_wake(address)</code> wakes one thread that is wait- ing on the queue.</li>
</ul>

<p>Linux approach has the flavor of an old approach that has been used on and off for years, , and is now referred to as a <strong>two-phase lock</strong>. A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So in the first phase, the lock spins for a while, hoping that it can acquire the lock. However, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.</p>

<h2 id="chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</h2>

<p><strong>Background</strong></p>

<p>Adding locks to a data structure to make it usable by threads makes the structure <strong>thread safe</strong>. There is always a standard method to make a concurrent data structure: add a big lock. But sometimes we need to ensure the scalability.</p>

<p>To evaluate the concurrent data structures, theres are two factors to concern:</p>

<ul>
  <li>Correctness</li>
  <li>Performance. MORE CONCURRENCY ISN’T NECESSARILY FASTER. If the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important. Build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do.</li>
</ul>

<p>Ideally, you’d like to see the threads complete just as quickly on multiple processors as the single thread does on one. Achieving this end is called <strong>perfect scaling</strong>.</p>

<p><strong>Guidelines</strong></p>

<ul>
  <li>Be careful with acquisition and release of locks around control flow changes</li>
  <li>Enabling more concurrency does not necessarily increase performance</li>
  <li>Performance problems should only be remedied once they exist, avoiding premature optimization, is central to any performance-minded developer</li>
  <li>There is no value in making something faster if doing so will not improve the overall performance of the application.</li>
</ul>

<p><strong>Concurrent Counters</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_performance_concurrent_counters.png" alt="os-lock_performance_concurrent_counters.png" /></p>

<p>Traditional Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_traditional_counter.png" alt="os-lock_traditional_counter.png" /></p>

<p>In this manner, it is similar to a data structure built with <strong>monitors</strong>, where locks are acquired and released automatically as you call and return from object methods.</p>

<p>The performance of the synchronized counter scales poorly.</p>

<p>Sloppy Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter.png" alt="os-lock_sloppy_counter.png" /></p>

<p>The sloppy counter works by representing a single logical counter via numerous local physical counters, one per CPU core, as well as a single global counter.
When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock.
How often this local-to-global transfer occurs is determined by a threshold, which we call S here (for sloppiness). The smaller S is, the more the counter behaves like the non-scalable counter above; the bigger S is, the more scalable the counter, but the further off the global value might be from the actual count.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter_scaling.png" alt="os-lock_sloppy_counter_scaling.png" /></p>

<p><strong>Concurrent Linked Lists</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list.png" alt="os-lock_concurrent_link_list.png" /></p>

<p>One small tricky issue arises if malloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert. This kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths.</p>

<p>BE WARY OF LOCKS AND CONTROL FLOW</p>

<p>Many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone. Thus, it is best to structure code to minimize this pattern.</p>

<p>Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list_optimized.png" alt="os-lock_concurrent_link_list_optimized.png" /></p>

<p>Once again we are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency within a list is something called <strong>hand-over-hand locking</strong> (a.k.a. <strong>lock coupling</strong>).</p>

<p>Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing the list, the code first grabs the next node’s lock and then releases the current node’s lock.</p>

<p>It enables a high degree of concurrency in list operations. However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive. Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating.</p>

<p><strong>Concurrent Queues</strong></p>

<p>Look at a slightly more concurrent queue designed by Michael and Scott.</p>

<p>There are two locks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. One trick used by the Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_queue.png" alt="os-lock_concurrent_queue.png" /></p>

<p><strong>Concurrent Hash Table</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_hash_table.png" alt="os-lock_concurrent_hash_table.png" /></p>

<p>This concurrent hash table is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well. The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_scaling_hash_table.png" alt="os-lock_scaling_hash_table.png" /></p>

<p>AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)</p>

<blockquote>
  <p>“Premature optimization is the root of all evil.”</p>
</blockquote>

<p>Many operating systems utilized a single lock when first transitioning to multiprocessors, including Sun OS and Linux. In the latter, this lock even had a name, the <strong>big kernel lock (BKL)</strong>. When multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck. Thus, it was finally time to add the optimization of improved concurrency to these systems. Within Linux, the more straightforward approach was taken: replace one lock with many. Within Sun, a more radical decision was made: build a brand new operating system, known as Solaris, that incorporates concurrency more fundamentally from day one.</p>

<h2 id="chapter-30---condition-variables">Chapter 30 - Condition Variables</h2>

<p><strong>Background</strong></p>

<p>There are many cases where a thread wishes to check whether a condition is true before continuing its execution. For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a <code>join()</code>).</p>

<p>In multi-threaded programs, it is often useful for a thread to wait for some conditio to become true before proceeding. The simple approach, of just spinning until the condition becomes true, is grossly inefficient and wastes CPU cycles, and in some cases, can be incorrect.</p>

<p><strong>Definition and Routines</strong></p>

<p>To wait for a condition to become true, a thread can make use of what is known as a condition variable. A <strong>condition variable</strong> is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition) is not as desired (by <strong>waiting</strong> on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by <strong>signaling</strong> on the condition).</p>

<p>By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the famous (and still important) producer/consumer problem, as well as covering conditions.</p>

<p>A condition variable has two operations associated with it: <strong>wait()</strong> and <strong>signal()</strong>.</p>

<ul>
  <li>The <strong>wait()</strong> call is executed when a thread wishes to put itself to sleep</li>
  <li>The <strong>signal()</strong> call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo.png" alt="os-cv_waiting_demo.png" /></p>

<p><strong><em>Is the state variable <code>done</code> necessary?</em></strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo_2.png" alt="os-cv_waiting_demo_2.png" /></p>

<p>Yes. Imagine the case where the child runs immediately and calls thr exit() immediately; in this case, the child will signal, but there is no thread asleep on the condition. When the parent runs, it will simply call wait and be stuck; no thread will ever wake it. From this example, you should appreciate the importance of the state variable done; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it.</p>

<p><strong><em>Is there a need to hold the lock while singaling?</em></strong></p>

<p>Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables. The generalization of this tip is correct: hold the lock when calling signal or wait, and you will always be in good shape.</p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>The producer/consumer problem, or sometimes as the bounded buffer problem, which was first posed by Dijkstra. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized <strong>semaphore</strong> (which can be used as either a lock or a condition variable).</p>

<p>A bounded buffer is also used when you pipe the output of one program into another, e.g.,</p>

<p><code>sh
// grep process is the producer
// wc process is the consumer
// between them is an in-kernel bounded buffer
grep foo file.txt | wc -l
</code></p>

<p>Basic operations: <code>put()</code> and <code>get()</code></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_put_and_get_v1.png" alt="os-cv_put_and_get_v1.png" /></p>

<p><strong>Plain Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_v1.png" alt="os-cv_producer_and_consumer_v1.png" /></p>

<p><strong>Single CV and If</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if.png" alt="os-cv_producer_and_consumer_single_cv_and_if.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_if_trace.png" /></p>

<p><strong>Single CV and While</strong></p>

<p>Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired. This interpretation of what a signal means is often referred to as <strong>Mesa semantics</strong>, after the first research that built a condition variable in such a manner. Virtually every system ever built employs Mesa semantincs.</p>

<p>Thanks to Mesa semantics, a simple rule to remember with condition variables is to <strong>always use while loops</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while.png" alt="os-cv_producer_and_consumer_single_cv_and_while.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_while_trace.png" /></p>

<p><strong>Two CVs and While</strong></p>

<p>Signaling is clearly needed, but must be more directed. <strong>A consumer should not wake other consumers, only producers</strong>, and vice-versa.</p>

<p>Use two condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Producer threads wait on the condition empty, and signals fill. Conversely, consumer threads wait on fill and signal empty.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_two_cv_and_while.png" alt="os-cv_producer_and_consumer_single_two_cv_and_while.png" /></p>

<p><strong>Final Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_final_solution.png" alt="os-cv_producer_and_consumer_final_solution.png" /></p>

<p><strong>Covering Conditions</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_covering_conditions.png" alt="os-cv_producer_and_consumer_covering_conditions.png" /></p>

<p>Assume there are zero bytes free; thread Ta calls <code>allocate(100)</code>, followed by thread Tb which asks for less memory by calling <code>allocate(10)</code>. Both Ta and Tb thus wait on the condition and go to sleep; there aren’t enough free bytes to satisfy either of these requests. At that point, assume a third thread, Tc, calls <code>free(50)</code>. Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed; Ta should remain waiting, as not enough memory is yet free. Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.</p>

<p>The solution suggested by Lampson and Redell is straightforward: replace the <code>pthread_cond_signal()</code> call in the code above with a call to <code>pthread_cond_broadcast()</code>, which wakes up all waiting threads. Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.</p>

<p>Lampson and Redell call such a condition a <strong>covering condition</strong>, as it covers all the cases where a thread needs to wake up (conservatively); the cost, is that too many threads might be woken.</p>

<p>In general, if you find that your program only works when you change your signals to broadcasts (but you don’t think it should need to), you probably have a bug; fix it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available.</p>

<h2 id="chapter-31---semaphores">Chapter 31 - Semaphores</h2>

<p><strong>Background</strong></p>

<p>As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems. One of the first people to realize this years ago was Edsger Dijkstra. Dijkstra and colleagues invented the semaphore as a single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables.</p>

<p><strong>Definition</strong></p>

<p>A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem <code>wait()</code> and sem <code>post()</code>. The initial value of the semaphore determines its behaviour.</p>

<p>Semaphores are a powerful and flexible primitive for writing concurrent programs. Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility.</p>

<p>In my view, semaphore is an primitive, which can be made by locks and condition variables, also can’t be used as locks and condition variables.</p>

<p>Initialization</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_init.png" alt="os-semaphore_init.png" /></p>

<p>Usage
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_definition.png" alt="os-semaphore_definition.png" /></p>

<ul>
  <li><code>sem_wait()</code> will either return right away (because the value of the semaphore was one or higher when we called <code>sem_wait()</code>), or it will cause the caller to suspend execution waiting for a subsequent post.</li>
  <li><code>sem_post()</code> does not wait for some particular condition to hold like <code>sem_wait()</code> does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.</li>
  <li>The value of the semaphore, when negative, is equal to the number of waiting threads</li>
</ul>

<p><strong>Semaphores As Locks</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_locks.png" alt="os-semaphore_as_locks.png" /></p>

<p>Because locks only have two states (held and not held), this usage is sometimes known as a <strong>binary semaphore</strong>.</p>

<p><strong>Semaphores As Condition Variables</strong></p>

<p>Semaphores are also useful when a thread wants to halt its progress waiting for a
 condition to become true. In this pattern of usage, we often find a thread waiting for something to happen, and a different thread making that something happen and then signaling that it has happened, thus waking the waiting thread.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_cv.png" alt="os-semaphore_as_cv.png" /></p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>Plain Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_plain.png" alt="os-semaphore_producer_and_consumer_plain.png" /></p>

<p>The condition variable (semaphore based) controls the execution order, which can let multiple threads enter the critical section at the same time. It still needs a lock.</p>

<p>Adding Mutual Exclusion</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex.png" alt="os-semaphore_producer_and_consumer_add_mutex.png" /></p>

<p>The consumer holds the mutex and is waiting for the someone to signal full. The producer could si!gnal full but is waiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.</p>

<p>To avoid the deadlock, we can simply move the mutex acquire and release to be just around the critical section. The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex_correctly.png" alt="os-semaphore_producer_and_consumer_add_mutex_correctly.png" /></p>

<p><strong>Reader-Writer Locks</strong></p>

<p>Another classic problem stems from the desire for a more flexible <strong>locking primitive</strong> that admits that different data structure accesses might require different kinds of locking.</p>

<p>Imagine a number of concurrent list operations, including inserts and simple lookups. While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a <strong>reader-writer lock</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_reader_writer_lock.png" alt="os-semaphore_reader_writer_lock.png" /></p>

<p>Once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until all readers are finished; the last one to exit the critical section calls sem <code>post()</code> on “writelock” and thus enables a waiting writer to acquire the lock.</p>

<p>This approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it would be relatively easy for readers to starve writers. It should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives.</p>

<p>SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)</p>

<p>You should never underestimate the notion that the simple and dumb approach can be the best one. Always try the simple and dumb approach first.</p>

<p><strong>The Dining Philosophers</strong></p>

<p>One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem.</p>

<p>There are five “philosophers” sitting around a table. Between each pair of philosophers is a single fork (and thus, five total). The philosophers each have times where they think, and don’t need any forks, and times where they eat. In order to eat, a philosopher needs two forks, both the one on their left and the one on their right. The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers.png" alt="os-semaphore_dinning_philosophers.png" /></p>

<p>Broken Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_deadlock_solution.png" alt="os-semaphore_dinning_philosophers_deadlock_solution.png" /></p>

<p>The problem is deadlock. If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever.</p>

<p>A Solution: Breaking The Dependency</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_solution.png" alt="os-semaphore_dinning_philosophers_solution.png" /></p>

<p><strong>Implement Semaphores</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_implementation.png" alt="os-semaphore_implementation.png" /></p>

<h2 id="chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</h2>

<p><strong>Background</strong></p>

<p>Lu et al has made a study, which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_bugs.png" alt="os-concurrency_bugs.png" /></p>

<p><strong>Non-Deadlock Bugs</strong></p>

<ul>
  <li>Atomicity violation bugs. The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution). Solve by locks.</li>
  <li>Order violation bugs. The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution). Solve by condition variables.</li>
</ul>

<p><strong>Deadlock Bugs</strong></p>

<p>Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_dependency.png" alt="os-concurrency_deadlock_dependency.png" /></p>

<p><strong>Caused by</strong></p>

<p>One reason is that in large code bases, complex dependencies arise between cmponents. The design of locking strategies in large systems must be carefully done to avoid deadlock in the case of <strong>circular dependencies</strong> that may occur naturally in the code.</p>

<p>Another reason is due to the nature of <strong>encapsulation</strong>. As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_by_encapsulation.png" alt="os-concurrency_deadlock_by_encapsulation.png" /></p>

<p><strong>Conditions for Deadlock</strong></p>

<ul>
  <li><strong>Mutual exclusion</strong>: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).</li>
  <li><strong>Hold-and-wait</strong>: Threads hold resources allocated to them (e.g.,locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).</li>
  <li><strong>No preemption (hold)</strong>: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.</li>
  <li><strong>Circular wait (wait)</strong>: There exists a coircular chain of threads such that each thread holds one more resources (e.g., locks) that are being requested by the next thread in the chain.</li>
</ul>

<p><strong>Prevention Based on Four Conditions</strong></p>

<p>Mutual Exclusion</p>

<p>To avoid the need for mutual exclusion at all. Herlihy had the idea that one could design various data structures to be <strong>wait-free</strong>. The idea here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_wait_free.png" alt="os-concurrency_deadlock_wait_free.png" /></p>

<p>However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.</p>

<p>Hold-and-wait</p>

<p>The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_hold_and_wait.png" alt="os-concurrency_deadlock_hold_and_wait.png" /></p>

<p>By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided.</p>

<p>Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.</p>

<p>No Preemption</p>

<p>Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, a <code>trylock()</code> routine will grab the lock (if it is available) or return -1 indicating that the lock is held right now and that you should try again later if you want to grab that lock.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_no_preemption.png" alt="os-concurrency_deadlock_no_preemption.png" /></p>

<p>One new problem does arise, however: <strong>livelock</strong>. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name lovelock. One could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.</p>

<p>Another issues arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement.</p>

<p>Circular Wait</p>

<p>The best solution in practice is to be careful, develop a lock acquisition order, and thus prevent deadlock from occurring in the first place.</p>

<ul>
  <li>The most straightforward way to do that is to provide a <strong>total ordering</strong> on lock acquisition. For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.</li>
  <li>A <strong>partial ordering</strong> can be a useful way to structure lock acquisition so as to avoid deadlock.</li>
</ul>

<p><strong>Avoidance via Scheduling</strong></p>

<p>Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_avoid_via_scheduling.png" alt="os-concurrency_deadlock_avoid_via_scheduling.png" /></p>

<p>Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution</p>

<p><strong>Detect and Recover</strong></p>

<p>One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected.</p>

<p>Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.</p>

<p>DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)</p>

<p>Tom West says famously, “Not everything worth doing is worth doing well”, which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small.</p>

<p><strong>Others</strong></p>

<p>Perhaps the best solution is to develop new concurrent programming models: in systems such as <strong>MapReduce</strong> (from Google), programmers can describe certain types of parallel computations without any locks whatsoever.</p>

<h2 id="chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</h2>

<p><strong>Background</strong></p>

<p>A different style of concurrent programming is often used in both GUI-based applications as well as some types of internet servers. This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js, but its roots are found in C/UNIX systems that we’ll discuss below.</p>

<p>Event-based servers give control of scheduling to the application itself, but do so at some cost in complexity and difficulty of integration with other aspects of modern systems (e.g., paging). Because of these challenges, no single approach has emerged as best; thus, both threads and events are likely to persist as two different approaches to the same concurrency problem for many years to come.</p>

<p>The problem that event-based concurrency addresses is two-fold.</p>

<ul>
  <li>The first is that managing concurrency correctly in multi-threaded applications can be challenging.</li>
  <li>The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs.</li>
</ul>

<p><strong>The Basic Idea: An Event Loop</strong></p>

<p>The approach is quite simple: you simply wait for something (i.e., an “event”) to occur; when it does, you check what type of  event it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.). That’s it!</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_loop.png" alt="os-event_loop.png" /></p>

<p>Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle ext is equivalent to scheduling. This explicit control over scheduling is one of the fundamental advantages of the event- based approach.</p>

<p>But there is a big question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O? Specifically, how can an event server tell if a message has arrived for it?</p>

<p><strong>An Important API: select() (or poll())</strong></p>

<p>In most systems, a basic API is available, via either the <strong>select()</strong> or <strong>poll()</strong> system calls. Either way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed.</p>

<p>What these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_api.png" alt="os-event_select_api.png" /></p>

<p>First, note that it lets you check whether descriptors can be reand from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).</p>

<p>Second, note the timeout argument. One common usage here is to set the timeout to <code>NULL</code>, which causes <code>select()</code> to block indefinitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to <code>select()</code> to return immediately.</p>

<p>Now linux uses <strong>epoll</strong>, FreeBSD (Mac OS) uses <strong>kqueue</strong>, and Windows uses <strong>IOCP</strong>.</p>

<p>BLOCKING VS. NON-BLOCKING INTERFACES</p>

<ul>
  <li>Blocking (or synchronous) interfaces do all of their work before returning to the caller. The usual culprit in blocking calls is I/O of some kind.</li>
  <li>Non-blocking (or asynchronous) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background. Non-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.</li>
</ul>

<p>DON’T BLOCK IN EVENT-BASED SERVERS</p>

<p>Event-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution the caller can ever be made; failing to obey this design tip will result in a blocked event-based server.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_code_demo.png" alt="os-event_select_code_demo.png" /></p>

<p>Advantage</p>

<p>With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Specifically, because only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread because it is decidedly single threaded. Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach.</p>

<p><strong>Issue: Blocking System Calls</strong></p>

<p>For example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request). Both the open() and read() calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service.</p>

<p>With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress. Indeed, this natural <strong>overlap</strong> of I/O and other computation is what makes thread-based programming quite natural and straight-forward.</p>

<p>With an event-based approach, however, there are no other threads to run: just the main event loop. And this implies that if an event handler issues a call that blocks, the entire server will do just that: block until the call completes.</p>

<p>We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed.</p>

<p>Solution: Asynchronous I/O</p>

<p>To overcome this limit, many modern operating systems have intro- duced new ways to issue I/O requests to the disk system, referred to generically as asynchronous I/O. These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed; additional interfaces enable an application to determine whether various I/Os have completed.</p>

<p>The APIs revolve around a basic structure, the struct aiocb or <strong>AIO control block</strong> in common terminology.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_aio_control_block.png" alt="os-event_aio_control_block.png" /></p>

<ul>
  <li>An application can periodically poll the system via a call to aio error() to determine whether said I/O has yet completed.</li>
  <li>Some systems provide an approach based on the interrupt. This method uses UNIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system.</li>
</ul>

<p>In systems without asynchronous I/O, the pure event-based approach cannot be implemented. However, clever researchers have derived methods that work fairly well in their place. For example, Pai et al describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os.</p>

<p>UNIX SIGNALS</p>

<p>A huge and fascinating infrastructure known as <strong>signals</strong> is present in all mod ern UNIX variants. At its simplest, signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a <strong>signal handler</strong>, i.e., some code in the application to handle that signal. When finished, the process just resumes its previous behaviour. A program can be configured to catch that signal. Or when a signal is sent to a process not config- ured to handle that signal, some default behavior is enacted; for SEGV, the process is killed.</p>

<p><strong>Issue: State Management</strong></p>

<p>When an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread. Adya et al. call this work <strong>manual stack management</strong>, and it is fundamental to event-based programming.</p>

<p>Solution: Continuation</p>

<p>Use an old programming language construct known as a <strong>continuation</strong>. Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_state_management.png" alt="os-event_state_management.png" /></p>

<p>Record the socket descriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (fd). When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller.</p>

<p><strong>What Is Still Difficult With Events</strong></p>

<p>Multiple CPUS. When systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared. Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel; when doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed. Thus, on modern multicore systems, simple event handling without locks is no longer possible.</p>

<p>Implicit blocking. It does not integrate well with certain kinds of systems activity, such as paging. For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes. Even though the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.</p>

<p>API changes all the time. That event-based code can be hard to manage over time, as the exact semantics of various routines changes]. For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces. Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.</p>

<p>Async network I/O. Though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there, and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think. For example, while one would simply like to use the select() interface to manage all outstanding I/Os, usually some combination of select() for networking and the AIO calls for disk I/O are required.</p>
]]></content>
  </entry>
  
</feed>
