<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Books | Wendi's Blog]]></title>
  <link href="http://blog.ifyouseewendy.com/blog/categories/books/atom.xml" rel="self"/>
  <link href="http://blog.ifyouseewendy.com/"/>
  <updated>2025-01-27T17:18:43-08:00</updated>
  <id>http://blog.ifyouseewendy.com/</id>
  <author>
    <name><![CDATA[wendi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Notes on DDD Quickly]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2019/05/27/notes-on-ddd/"/>
    <updated>2019-05-27T11:43:15-04:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2019/05/27/notes-on-ddd</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Domain Driven Design Quickly</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td>Floyd Marinescu &amp; Abel Avram</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="https://www.infoq.com/minibooks/domain-driven-design-quickly">InfoQ</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#what-is-ddd" id="markdown-toc-what-is-ddd">What is DDD?</a></li>
  <li><a href="#the-ubiquitous-language" id="markdown-toc-the-ubiquitous-language">The Ubiquitous Language</a></li>
  <li><a href="#model-driven-design" id="markdown-toc-model-driven-design">Model-Driven Design</a>    <ul>
      <li><a href="#building-blocks" id="markdown-toc-building-blocks">building blocks</a></li>
      <li><a href="#layered-architecture" id="markdown-toc-layered-architecture">Layered Architecture</a></li>
      <li><a href="#express-model-with-entities-value-objects-and-services" id="markdown-toc-express-model-with-entities-value-objects-and-services">Express Model with Entities, Value Objects and Services</a></li>
      <li><a href="#struct-model-with-modules" id="markdown-toc-struct-model-with-modules">Struct model with Modules</a></li>
      <li><a href="#manage-a-domain-object" id="markdown-toc-manage-a-domain-object">Manage a domain object</a></li>
    </ul>
  </li>
  <li><a href="#refactoring-toward-deeper-insight" id="markdown-toc-refactoring-toward-deeper-insight">Refactoring Toward Deeper Insight</a></li>
  <li><a href="#preserving-model-integrity" id="markdown-toc-preserving-model-integrity">Preserving Model Integrity</a>    <ul>
      <li><a href="#bounded-context" id="markdown-toc-bounded-context">Bounded Context</a></li>
      <li><a href="#continuous-integration" id="markdown-toc-continuous-integration">Continuous integration</a></li>
      <li><a href="#context-map" id="markdown-toc-context-map">Context map</a></li>
      <li><a href="#shared-kernel" id="markdown-toc-shared-kernel">Shared Kernel</a></li>
      <li><a href="#customer-supplier" id="markdown-toc-customer-supplier">Customer-supplier</a></li>
      <li><a href="#conformist" id="markdown-toc-conformist">Conformist</a></li>
      <li><a href="#anticorruption-layer" id="markdown-toc-anticorruption-layer">Anticorruption layer</a></li>
      <li><a href="#separate-ways" id="markdown-toc-separate-ways">Separate ways</a></li>
      <li><a href="#open-host-service" id="markdown-toc-open-host-service">Open host service</a></li>
      <li><a href="#distillation" id="markdown-toc-distillation">Distillation</a></li>
    </ul>
  </li>
  <li><a href="#advices" id="markdown-toc-advices">Advices</a></li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-mind-map.png" alt="ddd-mind-map" /></p>

<h2 id="what-is-ddd">What is DDD?</h2>

<p>The main idea is to identify the <strong>domain model</strong> and design your system around it. Usually, a software designer will talk with the domain expert to figure out the domain.</p>

<blockquote>
  <p>You cannot create a banking software system unless you have a good understanding of what banking is all about, one must understand the domain of banking.
How can we make the software fit harmoniously with the domain? The best way to do it is to make software a reflection of the domain. Software needs to incorporate the core concepts and elements of the domain, and to precisely realize the relationships between them. Software has to <strong>model</strong> the domain.</p>
</blockquote>

<p>So, what is domain model? It’s an abstraction of the domain. It’s an internal representation of the target domain.</p>

<blockquote>
  <p>A model is an abstraction of the domain. It is not just the knowledge in a domain expert’s head; it is a rigorously organized and selective abstraction of that knowledge. The model is our internal representation of the target domain,</p>
</blockquote>

<h2 id="the-ubiquitous-language">The Ubiquitous Language</h2>

<p>A core <strong>principle</strong> of DDD is to use a language to communicate the domain. Use the model as the backbone of a language. Request that the team use the language consistently in all communications, and also in the code. While sharing knowledge and hammering out the model, the team uses speech, writing and diagrams. Make sure this language appears consistently in all the communication forms used by the team; for this reason, the language is called <strong>the Ubiquitous Language</strong>.</p>

<p>Building a language like that has a clear outcome: the model and the language are strongly interconnected with one another. A change in the language should become a change to the model.</p>

<h2 id="model-driven-design">Model-Driven Design</h2>

<h3 id="building-blocks">building blocks</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-building-blocks.png" alt="ddd-building-blocks" /></p>

<h3 id="layered-architecture">Layered Architecture</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-layered-architecture.png" alt="ddd-layered-architecture" /></p>

<h3 id="express-model-with-entities-value-objects-and-services">Express Model with Entities, Value Objects and Services</h3>

<p><strong>Entity</strong>: implementing entities in software means creating identity.</p>

<p><strong>Value Object</strong>: is an object that is used to describe certain aspects of a domain, being immutable, having no identity, thus can be shared.</p>

<p>It is recommended to select as entities only those objects which conform to the entity definition. And make the rest of the objects Value Objects.</p>

<p><strong>Service</strong>: without an internal state, its purpose is to simply provide functionality for the domain. There are three characters of a Service</p>

<ol>
  <li>The operation performed by the Service refers to a domain concept which does not naturally belong to an Entity or Value Object.</li>
  <li>The operation performed refers to other objects in the domain.</li>
  <li>The operation is stateless.</li>
</ol>

<p>While using Services, is important to keep the domain layer isolated. It is easy to get confused between services which belong to the domain layer, and those belonging to the infrastructure.</p>

<h3 id="struct-model-with-modules">Struct model with Modules</h3>

<p>Modules are used as a method of organizing related concepts and tasks in order to reduce complexity. Choose Modules that tell the story of the system and contain a cohesive set of concepts.</p>

<p>It is recommended to group highly related classes into modules to provide maximum cohesion possible. There are several types of cohesion. Two of the most used are <em>communicational cohesion *and *functional cohesion</em>.</p>

<ul>
  <li>Communicational cohesion is achieved when parts of the module operate on the same data. It makes sense to group them, because there is a strong relationship between them.</li>
  <li>Functional cohesion is achieved when all parts of the module work together to perform a well-defined task. This is considered the best type of cohesion.</li>
</ul>

<h3 id="manage-a-domain-object">Manage a domain object</h3>

<p><strong>Ownership &amp; Boundary: Aggregates</strong></p>

<p>Aggregate is a domain pattern used to define object ownership and boundaries.</p>

<p>An Aggregate is a group of associated objects which are considered as one unit with regard to data changes. The Aggregate is demarcated by a boundary which separates the objects inside from those outside. Each Aggregate has one root. The root is an Entity, which has global identity and it’s responsible for maintaining the invariants, and it is the only object accessible from outside.</p>

<ol>
  <li>Cluster the Entities and Value Objects into Aggregates and define boundaries around each.</li>
  <li>Choose one Entity to be the root of each Aggregate, and control all access to the objects inside the boundary through the root.</li>
</ol>

<p><strong>Creation: Factories</strong></p>

<p>Factories are used to encapsulate the knowledge necessary for object creation, and they are especially useful to create Aggregates. When the root of the Aggregate is created, all the objects contained by the Aggregate are created along with it, and all the invariants are enforced.</p>

<p>It is important for the creation process to be atomic. Another observation is that Factories need to create new objects from scratch.</p>

<p><strong>Storage: Repositories</strong></p>

<p>A client needs a practical means of acquiring references to preexisting domain objects. Use a Repository to encapsulate all the logic needed to obtain object references.</p>

<p>The Repository acts as a storage place for globally accessible objects.</p>

<ul>
  <li>For each type of object that needs global access, create an object that can provide the illusion of an in-memory collection of all objects of that type.</li>
  <li>Set up access through a well-known global interface.</li>
  <li>Provide methods to add and remove objects, which will encapsulate the actual insertion or removal of data in the data store.</li>
  <li>Provide methods that select objects based on some criteria and return fully instantiated objects or collections of objects whose attribute values meet the criteria, thereby encapsulating the actual storage and query technology. Use a Specification.</li>
  <li>Provide repositories only for Aggregate roots that actually need direct access.</li>
  <li>Keep the client focused on the model, delegating all object storage and access to the Repositories.
It can be noted that the implementation of a repository can be closely liked to the infrastructure, but that the repository interface will be pure domain model.</li>
</ul>

<p>There is a relationship between Factory and Repository. They are both patterns of the model-driven design, and they both help us to manage the life cycle of domain objects.</p>

<ul>
  <li>While the Factory should create new objects, while the Repository should find already created objects. When a new object is to be added to the Repository, it should be created first using the Factory, and then it should be given to the Repository which will store it like in the example below.</li>
  <li>Another way this is noted is that Factories are “pure domain”, but that Repositories can contain links to the infrastructure, e g the database.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-factories-and-repositories.png" alt="ddd-factories-and-repositories" /></p>

<h2 id="refactoring-toward-deeper-insight">Refactoring Toward Deeper Insight</h2>

<p>A good model is the result of deep thinking, insight, experience, and flair.
Refactoring is done in small steps. The result is also a series of small improvements. There are times when lots of small changes add very little value to the design, and there are times when few changes make a lot of difference. It’s a Breakthrough. Each refinement adds more clarity to the design. This creates in turn the premises for a Breakthrough.</p>

<p>To reach a Breakthrough, we need to make the implicit concepts explicit.</p>

<ul>
  <li>Listen to the language</li>
  <li>Use domain literature</li>
  <li>Constraint, Process and Specification.
    <ul>
      <li>A Constraint is a simple way to express an invariant. Whatever happens to the object data, the invariant is respected. This is simply done by putting the invariant logic into a Constraint.</li>
      <li>Processes are usually expressed in code with procedures. The best way to implement processes is to use a Service.</li>
      <li>a Specification is used to test an object to see if it satisfies a certain criteria.</li>
    </ul>
  </li>
</ul>

<h2 id="preserving-model-integrity">Preserving Model Integrity</h2>

<p>It is so easy to start from a good model and progress toward an inconsistent one. The internal consistency of a model is called <strong>unification</strong>. Instead of trying to keep one big model that will fall apart later, we should consciously divide it into several models. Several models well integrated can evolve independently as long as they obey the contract they are bound to.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-data-integrity-patterns.png" alt="ddd-data-integrity-patterns" /></p>

<h3 id="bounded-context">Bounded Context</h3>

<p>The main idea is to define the scope of a model, to draw up the boundaries of its context, then do the most possible to keep the model unified. Explicitly define the context within which a model applies. Explicitly set boundaries in terms of team organization, usage within specific parts of the application, and physical manifestations such as code bases and database schemas. A model should be small enough to be assigned to one team.</p>

<p>**A Bounded Context is not a Module. **A Bounded Context provides the logical frame inside of which the model evolves. Modules are used to organize the elements of a model, so Bounded Context encompasses the Module.</p>

<h3 id="continuous-integration">Continuous integration</h3>

<p>Continuous Integration is a necessary process within a Bounded Context. Another necessary requirement is to perform automated tests.</p>

<h3 id="context-map">Context map</h3>

<p>A Context Map is a document which outlines the different Bounded Contexts and the relationships between them.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-context-map.png" alt="ddd-context-map" /></p>

<p>Each Bounded Context should have a name which should be part of the Ubiquitous Language. A common practice is to define the contexts, then create modules for each context, and use a naming convention to indicate the context each module belongs to.</p>

<h3 id="shared-kernel">Shared Kernel</h3>

<p>The purpose is to reduce duplication, but still keep two separate contexts.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-shared-kernel.png" alt="ddd-shared-kernel" /></p>

<h3 id="customer-supplier">Customer-supplier</h3>

<p>There are times when two subsystems have a special relationship: one depends a lot on the other. The contexts in which those two subsystems exist are different, and the processing result of one system is fed into the other. Establish a clear customer/supplier relationship between the two teams.</p>

<p>A Customer-Supplier relationship is viable when both teams are interested in the relationship. When two development teams have a Customer-Supplier relationship in which the supplier team has no motivation to provide for the customer team’s needs, the customer team is helpless.</p>

<p>The customer team has few options:</p>

<ul>
  <li><strong>Conform</strong> entirely to supplier team’s model</li>
  <li>Protect itself by using an <strong>Anticorruption Layer</strong></li>
  <li><strong>Separate Ways</strong>. We need to closely evaluate the benefits of integration and use it only if there is real value in doing so. If we reach the conclusion that integration is more trouble than it is worth, then we should go the Separate Ways.</li>
</ul>

<h3 id="conformist">Conformist</h3>

<p>If the customer has to use the supplier team’s model, and if that is well done, it may be time for conformity. The customer team could adhere to the supplier team’s model, conforming entirely to it.
Compared to the Shared Kernel, but there is an important difference. The customer team cannot make changes to the kernel. They can only use it as part of their model, and they can build on the existing code provided.</p>

<h3 id="anticorruption-layer">Anticorruption layer</h3>

<p>From our model’s perspective, the Anticorruption Layer is a natural part of the model; it does not look like something foreign. It operates with concepts and actions familiar to our model. But the Anticorruption Layer talks to the external model using the external language not the client one. This layer works as a two way translator between two domains and languages.</p>

<p>How should we implement the Anticorruption Layer? A very good solution is to see the layer as a Service from the client model. The Service will be done as a Façade, along with a Adapter and translator.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/ddd/ddd-anticorruption-layer.png" alt="ddd-anticorruption-layer" /></p>

<p>Adapter is to convert the interface of a class to the one understood by the client. Translator is to do object and data conversion.</p>

<h3 id="separate-ways">Separate ways</h3>

<p>Before going on Separate Ways we need to make sure that we won’t be coming back to an integrated system.</p>

<p>The Separate Ways pattern addresses the case when an enterprise application can be made up of several smaller applications which have little or nothing in common from a modeling perspective. From the user’s perspective this is one application, but from a modeling and design point of view it may done using separate models with distinct implementations.</p>

<h3 id="open-host-service">Open host service</h3>

<p>When we try to integrate two subsystems, we usually create a translation layer between them. This layer acts as a buffer between the client subsystem and the external subsystem we want to integrate with. This layer can be a consistent one, depending on the complexity of relationships and how the external subsystem was designed. If the external subsystem turns out to be used not by one client subsystem, but by several ones, we need to create translation layers for all of them.</p>

<p>The solution is to see the external subsystem as a provider of services. If we can wrap a set of Services around it, then all the other subsystems will access these Services, and we won’t need any translation layer.</p>

<p>Define a protocol that gives access to your subsystem as a set of Services. Open the protocol so that all who need to integrate with you can use it.</p>

<h3 id="distillation">Distillation</h3>

<p>A large domain has a large model even after we have refined it and created many abstractions. It can remain big even after many refactorings. In situations like this, it may be time for a distillation. The idea is to define a Core Domain which represents the essence of the domain. The byproducts of the distillation process will be Generic Subdomains which will comprise the other parts of the domain.</p>

<p>When working with a large model, we should try to separate the essential concepts from generic ones. Identify cohesive subdomains that are not the motivation for your project. Factor out generic models of these subdomains and place them in separate Modules. There are different ways to implement a Generic Subdomain:</p>

<ul>
  <li>Off-the-shelf Solution</li>
  <li>Outsourcing</li>
  <li>Existing Model</li>
  <li>In-House Implementation</li>
</ul>

<h2 id="advices">Advices</h2>

<p>Keep in mind some of the pitfalls of domain modeling:</p>

<ol>
  <li>Stay hands-on. Modelers need to code.</li>
  <li>Focus on concrete scenarios. Abstract thinking has to be anchored in concrete cases.</li>
  <li>Don’t try to apply DDD to everything. Draw a context map and decide on where you will make a push for DDD and where you will not. And then don’t worry about it outside those boundaries.</li>
  <li>Experiment a lot and expect to make lots of mistakes. Modeling is a creative process.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] 学习乐观]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/08/30/review-xue-xi-le-guan/"/>
    <updated>2016-08-30T22:03:04+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/08/30/review-xue-xi-le-guan</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>学习乐观</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="https://en.wikipedia.org/wiki/Martin_Seligman">Martin Seligman</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="https://book.douban.com/subject/4934590/">book.douban.com/subject/4934590/</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#section" id="markdown-toc-section">前言</a></li>
  <li><a href="#section-1" id="markdown-toc-section-1">第1章 - 悲观者与乐观者的画像</a></li>
  <li><a href="#section-2" id="markdown-toc-section-2">第2章 - 悲观者的无助源自何处</a></li>
  <li><a href="#section-3" id="markdown-toc-section-3">第3章 - 悲观者严重的挫折</a></li>
  <li><a href="#section-4" id="markdown-toc-section-4">第4章 - 从悲观滑向抑郁</a></li>
  <li><a href="#section-5" id="markdown-toc-section-5">第5章 - 想法决定悲喜人生</a></li>
  <li><a href="#section-6" id="markdown-toc-section-6">第6~11章 - 乐观的好处</a></li>
  <li><a href="#section-7" id="markdown-toc-section-7">第6章 - 乐观奠定成功的事业</a></li>
  <li><a href="#section-8" id="markdown-toc-section-8">第7~8章 - 解释风格对孩子的影响</a></li>
  <li><a href="#section-9" id="markdown-toc-section-9">第12章 - 怎样习得乐观</a></li>
  <li><a href="#section-10" id="markdown-toc-section-10">第15章 - 乐观可以有弹性</a></li>
  <li><a href="#section-11" id="markdown-toc-section-11">总结</a></li>
</ul>

<h3 id="section">前言</h3>

<p>前两天，看了占东分享的这个关于 Optimism 的演讲 <a href="https://github.com/raganwald/presentations/blob/master/optimism.md">raganwald/presentations</a>。有点感触，一直自认为是悲观主义者。所以在这个演讲的介绍下，找到了 Martin Seligman 博士的这本书 Learned Optimism，中文名为《活出最乐观的自己/学习乐观》。</p>

<p>如果感兴趣，可以尝试下这个在线测试 <a href="https://www.authentichappiness.sas.upenn.edu/questionnaires/optimism-test">optimism-test</a>。如果已经足够乐观，那大可不必阅读这本书。</p>

<p><strong>思维导图</strong></p>

<p><img src="https://raw.githubusercontent.com/ifyouseewendy/ifyouseewendy.github.io/source/image-repo/learned_optimism.png" alt="learned_optimism" /></p>

<h3 id="section-1">第1章 - 悲观者与乐观者的画像</h3>

<blockquote>
  <p>悲观的人的特征是，他相信坏事都是因为自己的错，这件事会毁掉他的一切，会持续很久。而乐观的人在遇到同样的厄运时，会认为现在的失败是暂时性的，每个失败都有它的原因，不是自己的错，可能是环境、运气或其他人为原因的后果。这种人不会被失败击倒。在面对恶劣环境时，他们会把它看成是一种挑战，更努力地去克服它。</p>
</blockquote>

<p>心理学在过去20年最显著的发现就是：人可以选择他想要的思维模式。</p>

<p>我们从个人控制理论开始，主要涉及两个彼此交错的概念：</p>

<ul>
  <li>习惯性无助 Learned helplessness 是一个放弃的反应，是源自『无论你怎么努力都于事无补』的想法的行为。</li>
  <li>解释风格 Explanatory style 是习惯性无助的核心，是你对为什么这件事会发生的习惯性的解释方式。（每个人在碰到不如意的事情时，第一件事就是去解释这个不如意。这些解释决定了我们下一步怎么做。我们的解释风格和想法不仅影响着我们的行动，也影响着我们的感觉和情绪。）</li>
</ul>

<h3 id="section-2">第2章 - 悲观者的无助源自何处</h3>

<p>关于心理学简史：</p>

<ul>
  <li>心理分析学派、佛洛依德派，强调精神活动的隐意识方面，并大胆的以性为切入点，将人的精神问题归因为童年的心理创伤（佛洛依德认为梦到牙齿掉下来代表着阉割以及手淫的罪恶感，做梦的人害怕他的父亲用阉割的方式惩罚他手淫的罪恶。）</li>
  <li>行为主义学派，坚持一个人的所有行为只受他得到的奖励和惩罚所决定：一个被奖励的行为可能会重复，而一个被惩罚的行为则可能会被压抑，如此是已。</li>
  <li>认知心理学，它保留了对改变的乐观信念，认为『自我』可以改进自己。</li>
</ul>

<h3 id="section-3">第3章 - 悲观者严重的挫折</h3>

<p>解释风格有三种维度——永久性、普遍性和人格化。普遍性和永久性控制着你的行为，你的无助感的持久性，以及无助感涉及的层面；人格化控制着你如何看待自己，对自己的感受。当不好的事情发生时，悲观的人怪罪自己，乐观的人怪罪旁人或环境；当好事情发生时，悲观的人归功于旁人或环境，而乐观的人归功于自己。</p>

<p>根据书中第3章的调查问卷，自己做了如下笔记：</p>

<p><img src="https://raw.githubusercontent.com/ifyouseewendy/ifyouseewendy.github.io/source/image-repo/learned_optimism_questionaire_0826.jpeg" alt="learned_optimism_questionaire_0826" /></p>

<p>将最后的总结内容誊写在这：</p>

<ul>
  <li>总体上，我得到了『中等悲观』的分数。</li>
  <li>我只在 PvB 上表现出了乐观，即我不会认为事情会普遍的不好。</li>
  <li>但是我在其他『解释风格』，即永久性和人格化上，我的表现非常糟糕。这说明了
    <ul>
      <li>我惯于把坏的事情当做永久、一成不变的状态，而好的事情只作一时之幸</li>
      <li>我的自尊非常低，关于将错误归咎自己，而将成功归因环境与他人</li>
    </ul>
  </li>
  <li>我理解自己的得分，不禁对自己一直以来希望秉承的『谦虚』和『客观』表示强烈怀疑！</li>
</ul>

<h3 id="section-4">第4章 - 从悲观滑向抑郁</h3>

<p>抑郁的人通常会在思想、情绪、行为和身体四个方面发生消极的变化。</p>

<blockquote>
  <p>抑郁症患者通常在刚醒来时情绪处于最低潮，你躺在床上想着过去种种的失败，想着今天可能要面临的失败，假如你躺在床上不起身，这些失败的想法就像一床棉被一样把你包裹着，如果你爬起床，开始一天的活动，通常情绪会变好些。到了下午3点到6点，情绪又会低落下去。晚上通常是一天中最不抑郁的时间，清晨3点到5点，如果你没睡着，则是情绪最差的时候，情绪在一天中是有所变化的。</p>
</blockquote>

<p>看到这段话，惊得我一身冷汗，好像回到曾经大学宿舍那个冰冷的小屋。回想那段日子，只是觉着自己从悬崖边走过，今天找到了背后真正的含义。</p>

<h3 id="section-5">第5章 - 想法决定悲喜人生</h3>

<p>认知疗法有五种策略</p>

<ul>
  <li>学会去认识在情绪最低沉时自动冒出来的想法</li>
  <li>学会与这个自动冒出来的想法抗争</li>
  <li>学会用不同的解释——重新归因（reattribution）去对抗原有的想法</li>
  <li>学会如何把自己从抑郁的思绪中引开</li>
  <li>学会去认识并且质疑那些控制你并引起你抑郁的假设</li>
</ul>

<blockquote>
  <p>男人碰到事情会去做，而不会反复去想；而女人喜欢钻牛角尖，把事情翻来覆去的想——反刍（rumination），去分析它为什么是这样。女性看待事情的方式造成女性得抑郁症的比例是男性的两倍。</p>
</blockquote>

<h3 id="section-6">第6~11章 - 乐观的好处</h3>

<p>作者从个人事业、身体、孩子、体育赛事、企业组织、竞选几个角度阐释了，乐观的人相对表现更好。</p>

<blockquote>
  <p>你可能会想，谁会碰到较多不幸的事件呢？答案是悲观的人。因为他们比较被动，较少主动采取行动来避免不好的事，而且在事情发生之后也较少采取行动来终止这些事。</p>
</blockquote>

<p>但是我们需要找到平衡点，即『不盲目乐观』。</p>

<ul>
  <li>习惯性乐观不是教你自私、自大，让别人不能忍受，它要教你在遇到失败挫折时如何与自己对话。你要学会如何在收到打击时，从更具鼓励性的角度来考虑挫折或困境。</li>
  <li>使用乐观技术的基本原则是先问，在某一个特定情况下失败的代价是什么。如果失败的代价很高，那么就不应该乐观。</li>
</ul>

<h3 id="section-7">第6章 - 乐观奠定成功的事业</h3>

<p>之前的阅读，让我提出怀疑：自己暂时偏悲观的人格，一定是坏的吗？如果我是个乐观的人，会比现在活得更好吗？一直以来我要求自己『谦虚』和『客观』，但是这同样部分导致了我在测试中体现的悲观分数。</p>

<p>这一章的阅读可以回答我之前的疑问，主要有以下几个观点：</p>

<ul>
  <li>传统的成功观点并不完善。要成功，除了具备能力和动机之外，还需要坚持，遇到挫折也不放弃的坚持，而乐观的解释风格则是坚持的灵魂。</li>
  <li>成功的生活需要大部分时间的乐观和偶尔的悲观。我理解这个平衡点位于『不盲目乐观』。</li>
  <li>悲观不是一无是处，就像头脑特工队里试图阐释忧忧（sadness）存在的意义一样，悲观让我们更冷静、对事情有更清楚的判断。所以轻度悲观是可以接受的状态。</li>
</ul>

<p>这一章同样回答了心中另一个问题，国家动用百万资金支持心理研究，除了为解决各体的心理问题外，有什么更实际（物质层面）的意义？</p>

<p>由于销售是个时常遭受挫折的职业，公司花费大量的人力物力对新人进行培训，但是高离职率导致公司需要承担不菲的损失。大都会保险，就是这样一家企业。Seligman 博士帮助大都会筛选入职人员时，将乐观悲观的人格作为一项新的考量，降低了离职率，为公司在财务层面节省了不菲的资金。</p>

<h3 id="section-8">第7~8章 - 解释风格对孩子的影响</h3>

<ul>
  <li>孩子8岁时，乐观或悲观的解释风格基本定型了。</li>
  <li>孩子的解释风格会受到三种因素的影响：
    <ul>
      <li>孩子从父母身上学到的各种事件的因果分析，尤其是妈妈的</li>
      <li>孩子听到的批评方式，即如果这些批评是永久的、普遍的、内在的，那么他对自己的看法就会转向悲观</li>
      <li>孩子早期生活经验中的生离死别和巨大变故。如果这个事故是永久和普遍的，那么绝望的种子会深埋孩子心中；如果这些事件好转了，他会比较乐观。父母离异或经常争吵是最容易引发孩子抑郁的时间。</li>
    </ul>
  </li>
  <li>失去乐观，传统意义上的聪明才智对成功没有没有什么意义。</li>
</ul>

<h3 id="section-9">第12章 - 怎样习得乐观</h3>

<p>改变悲观的解释风格有两种方法：</p>

<ul>
  <li>转移注意。当不好的事情发生时，安排一个时间去想它；当消极的想法一出现，就立刻把它写下来。</li>
  <li>反驳。从长远来看，反驳更有效
    <ul>
      <li>保持距离。我们一般都会和别人无理的指责保持一个距离。但是要我们与自己的指责保持距离就很困难，因为我们会认为，如果这个指责来自内心，那么它一定是真的。大错特错！我们在遭受挫折时对自己说的话可能和不怀好意的人说的一样毫无根据。</li>
      <li>反驳的四个过程
        <ul>
          <li>寻找证据</li>
          <li>其他可能性</li>
          <li>暗示。即使我的消极想法是对的，那么这个想法的暗示是什么？</li>
          <li>用处。有时候，这些想法的后果比想法的真实性更重要。回问自己，这些想法是否有破坏性？</li>
        </ul>
      </li>
      <li>记录整个过程，找朋友说出自己的反驳，不断练习。</li>
    </ul>
  </li>
</ul>

<h3 id="section-10">第15章 - 乐观可以有弹性</h3>

<p>依作者观点，抑郁症在美国泛滥的机制是习惯性无助，主要原因则是：</p>

<ul>
  <li>个人主义的兴起，即『特大号』的自我。这是一个强调个人控制的时代，自我膨胀已经到了危险的地步，个体的无助需要被治疗。我们的欲望、奢求越来越多，失望、无助便随之而来。</li>
  <li>公共意识的消失。人类需要生活在意义和希望中。关于生命的无意义感，有意义的一个必要条件就是它必须依附到一个比自我更大的东西上去。你可以依附的东西越大，你所拥有的意义越多。而不断发生的政治事件削弱了人们对国家的期望，社会趋势也减弱了人们对神袛和家庭的依赖。公共意识即指对国家、神袛和家庭的看法。</li>
  <li>简要来说，即对自己的过度关怀和对团体的不够关心。</li>
</ul>

<p>出路何在：</p>

<ul>
  <li>找出特大号自我的优势，运用习惯性乐观。</li>
  <li>改变个人与团体之间的关系，寻找新的平衡点。</li>
</ul>

<p>如何实践，作者提到了一个词『道德慢跑』，基于这样的观点：我们慢跑是因为我们期望用用每一天的自我牺牲去换取长远的自我利益。同样，我们可以利用增加公共意识来平衡自我和团体的关系，做一场道德慢跑：</p>

<ul>
  <li>放弃一些享乐行为，把一周中的一个晚上用于从事一件对团体、社区有利的活动</li>
  <li>当你读到某个领你感动的事迹时，写信给那个人。写信去鼓励那些值得你尊敬的人。一个星期花三个小时来写这种信，惩恶扬善</li>
  <li>教导你的孩子如何施舍。叫他们把自己零用钱的1/4留下来捐出去，让他们自己去找值得捐的人或事情。</li>
</ul>

<blockquote>
  <p>如果你为社区、为团体服务得够久的话，你就会找到生命的意义，你会发现你越来越不容易抑郁，也变得不容易得感冒，你越来越喜欢参加团体的活动而不是关起门来独乐乐。更重要的是，你心灵中的那块空虚，那个个人主义所滋养的无意义感会被填的满满的。</p>
</blockquote>

<h3 id="section-11">总结</h3>

<p>女朋友发现我在看这本书，一脸坏笑，闪烁其词。『少年，你心理有病啊？』说实话，我确实觉着自己心理有点问题。我的常态是偏失落的，没有大喜大悲，但总是对自己不满。『读鸡汤文有用吗？』每个人看到的不一样吧。我相信，好多时候我们的行为都是处于”the zombie mode”，习惯的行为模式，或者说心理上的条件反射。不经过思考与剖析，有太多想法不可察觉。这本书至于我最大的意义在于提供了一个全新的角度，让我更加了解自己。它说服我让我真正相信乐观是一个优点，而不是一句口号。我也愿意自此调整，摆脱现在『中度悲观』的心理状态。作者最后给出的调节办法，我不觉着深刻有效，本质上还是在认可乐观的前提条件下，进行观察、反思、自我调节的良性循环。书中关于如何培养孩子的乐观心态，如何将乐观应用在企业管理以及预测总统等活动的事例，都非常耐人寻味。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Concurrency - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces/"/>
    <updated>2015-12-26T11:33:57+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#concurrency" id="markdown-toc-concurrency">Concurrency</a>    <ul>
      <li><a href="#chapter-26---introduction" id="markdown-toc-chapter-26---introduction">Chapter 26 - Introduction</a></li>
      <li><a href="#chapter-27---interlude-thread-api" id="markdown-toc-chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</a></li>
      <li><a href="#chapter-28---locks" id="markdown-toc-chapter-28---locks">Chapter 28 - Locks</a></li>
      <li><a href="#chapter-29---lock-based-concurrent-data-structures" id="markdown-toc-chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</a></li>
      <li><a href="#chapter-30---condition-variables" id="markdown-toc-chapter-30---condition-variables">Chapter 30 - Condition Variables</a></li>
      <li><a href="#chapter-31---semaphores" id="markdown-toc-chapter-31---semaphores">Chapter 31 - Semaphores</a></li>
      <li><a href="#chapter-32---common-concurrency-problems" id="markdown-toc-chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</a></li>
      <li><a href="#chapter-33---event-based-concurrency-advanced" id="markdown-toc-chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</a></li>
    </ul>
  </li>
</ul>

<h1 id="concurrency">Concurrency</h1>

<h2 id="chapter-26---introduction">Chapter 26 - Introduction</h2>

<p><strong>Background</strong></p>

<p>With time sharing, we can take a single physical CPU and turn it into multiple virtual CPUs, thus enabling the illusion of multiple programs running at the same time, through time sharing.</p>

<p>With paging (base and bounds, segmentation), we can create the illusion of a large, private virtual memory for each process; this abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk).</p>

<p>But the abstraction of running program we use along is the process, and it’s a classic view of a single point of execution within a program. Now we introduce a new abstraction, thread. And  a <strong>multi-threaded</strong> program has more than one point of execution.</p>

<p>Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus ca access the same data.</p>

<p><strong>Thread vs. Process</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_vs_process.png" alt="os-thread_vs_process.png" /></p>

<p>Address space</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_address_space.png" alt="os-thread_address_space.png" /></p>

<p><strong>Advantage</strong></p>

<p>Efficiency, as they share the same address space.</p>

<ul>
  <li>Save storage</li>
  <li>Easy context switching (no need to change page)</li>
</ul>

<p><strong>Issues</strong></p>

<ul>
  <li><strong>Sharing data</strong>, that of accessing shared variables and the need to support atomicity for critical sections.</li>
  <li><strong>Waiting for another</strong>, sleeping and waking interaction, where one thread must wait for another to complete some action before it continues.</li>
</ul>

<p><strong>Shared Data</strong></p>

<p>The heart of the problem is <strong>uncontrolled scheduling</strong>.</p>

<p>It is a wonderful and hard problem, and should make your mind hurt (a bit). If it doesn’t, then you don’t understand! Keep working until your head hurts; you then know you’re headed in the right directinn.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_sharing_data.png" alt="os-thread_sharing_data.png" /></p>

<p><strong>Key Concurrency Terms</strong> (from Edsger Dijkstra)</p>

<p>A <strong>critical section</strong> is a piece of code that accesses a shared resource, usually a variable or data structure.</p>

<p>A <strong>race condition</strong> arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps un- desirable) outcome. The results depend on the timing execution of the code.</p>

<p>An <strong>indeterminate</strong> program consists of one or more races onditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems.</p>

<p>To avoid these problems, threads should use some kind of <strong>mutual exclusion primitives</strong>; doing so guarantees that only a single thread ever enters a critical section, thus avoiding racoes, and resulting in deterministic program outputs.</p>

<p><strong>Atomic</strong></p>

<p>Atomic operations are one of the most powerful underlying techniques in building computer systems.</p>

<p>The idea behind making a series of actions <strong>atomic</strong> is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a <strong>transaction</strong>.</p>

<p>In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution.</p>

<p><strong>The Wish For Atomicity</strong></p>

<p>Hardware guarantees the instructions is atomic, and provide a general set we call <strong>synchronisation primitives</strong> to ensure atomicity.</p>

<p>Hardware guarantees that the instructions execute atomically. It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state.</p>

<p>But, would we really want the hardware to support an “atomic update of B-tree” instruction?</p>

<p>No. Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call <strong>synchronization primitives</strong>. By using these hardware synchronization primitives, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution.</p>

<p><strong>Why in OS Class?</strong></p>

<p>“History” is the one-word answer; the OS was the first concurrent program, and many techniques were created for use within the OS. Later, with multi-threaded processes, application programmers also had to consider such things.</p>

<p>OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.</p>

<h2 id="chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</h2>

<p><strong>Guidelines</strong></p>

<p>There are a number of small but important things to remember when you use the POSIX thread library.</p>

<ul>
  <li><strong>Keep it simple</strong>. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interactions lead to bugs.</li>
  <li>Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum.</li>
  <li><strong>Each thread has its own stack</strong>. If you have a locally-allocated variable inside of some function a thread is exe- cuting, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the values must be in the heap or otherwise some locale that is globally accessible.</li>
  <li><strong>Be careful with how you pass arguments to, and return values from, threads.</strong> In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong.</li>
  <li><strong>Check your return codes.</strong> Of course, in any C and UNIX program- ming you do, you should be checking each and every return code, and it’s true here as well.</li>
  <li><strong>Always use condition variables to signal between threads.</strong> While it is often tempting to use a simple flag, don’t do it.</li>
  <li><strong>Initialize locks and condition variables.</strong> Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways.</li>
  <li><strong>Use the manual pages.</strong> On Linux, in particular, the pthread man pages (man -k pthread) are highly informative and discuss much of the nuances pre- sented here, often in even more detail.</li>
</ul>

<p><strong>Thread Creation</strong></p>

<p><code>c
#include &lt;pthread.h&gt;
int pthread_create(pthread_t * thread,
                     const pthread_attr_t *  attr,
                     void * (*start_routine)(void*),
                     void *  arg);
</code></p>

<ul>
  <li><code>thread</code>, is a pointer to a structure of type pthread t; we’ll use this structure to interact with this thread</li>
  <li><code>attr</code>, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread.</li>
  <li>The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (<code>start routine</code>), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer).</li>
  <li><code>arg</code>, is exactly the argument to be passed to the function where the thread begins execution.</li>
</ul>

<p><strong><em>Why do we need these void pointers?</em></strong></p>

<p>Having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result.</p>

<p><strong>Thread Completion</strong></p>

<p><code>c
int pthread_join(pthread_t thread, void **value_ptr);
</code></p>

<ul>
  <li><code>thread</code> is used to specify which thread to wait for</li>
  <li><code>value_ptr</code> is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo.png" alt="os-thread_waiting_demo.png" /></p>

<p>Note that one has to be extremely careful with how values are returned from a thread. In particular, never return a pointer which refers to something allocated on the thread’s call stack.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo_wrong.png" alt="os-thread_waiting_demo_wrong.png" /></p>

<p>However, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results.</p>

<p>Not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely. Such long-lived programs thus may not need to join.</p>

<p><strong>Locks</strong></p>

<p>Providing mutual exclusion to a critical section via locks.</p>

<p><code>c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
</code></p>

<p>When you have a region of code you realize is a critical section, and thus needs to be pro- tected by locks in order to operate as desired.</p>

<p>```c
pthread_mutex_t lock;</p>

<p>Pthread_mutex_init(&amp;lock);</p>

<p>Pthread_mutex_lock(&amp;lock);
x = x + 1; // or whatever your critical section is
Pthread_mutex_unlock(&amp;olock);</p>

<p>// Always check for failure
void Pthread_mutex_init(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_init(&amp;lock, NULL); // dynamic initialisation, or PTHREAD_MUTEX_INITIALIZER
    assert(rc == 0); // always check success!
}
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_lock(mutex);
    assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_unlock(mutex);
    assert(rc == 0);
}
```</p>

<p><strong>Condition Variables</strong></p>

<p>Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue.</p>

<p>To use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of the above routines, this lock should be held.</p>

<p><code>c
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
</code></p>

<p>pthread_cond_wait(), puts the calling thread to sleep, ad thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about.</p>

<p><code>c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t  cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&amp;lock);
while (ready == 0)
    Pthread_cond_wait(&amp;cond, &amp;lock);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>After initialization of the relevant lock and condition, a thread checks to see if the variable ready has yet been set to something other than zero. If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.</p>

<p><code>c
Pthread_mutex_lock(&amp;lock);
ready = 1;
Pthread_cond_signal(&amp;cond);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>Notice 1</p>

<p>When signaling (as well as when modifying the global variable ready), we always make sure to have the lock held. This ensures that we don’t accidentally introduce a race condition into our code.</p>

<p>Notice 2</p>

<p>Notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep.</p>

<p>Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However, before returning after being woken, the pthread_cond_wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.</p>

<p>Notice 3</p>

<p>The waiting thread re-checks the condition in a while loop, instead of a simple if statement. Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not.</p>

<p>Notice 4</p>

<p>Don’t ever use these ad hoc synchronisations.</p>

<p>```c
// waitingnwhile (ready == 0)
    ; // spin</p>

<p>// signaling
ready = 1;
```</p>

<p>First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone.</p>

<p><strong>Others</strong></p>

<p>On the link line, you must also explicitly link with the pthreads library, by adding the -pthread flag.</p>

<p><code>sh
prompt&gt; gcc -o main main.c -Wall -pthread
</code></p>

<h2 id="chapter-28---locks">Chapter 28 - Locks</h2>

<p><strong>The Basic Idea</strong></p>

<p>Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction.</p>

<p>This lock variable (or just “lock” for short) holds the state of the lock at any instant in time. It is either available (or unlocked or free) and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_demo.png" alt="os-lock_demo.png" /></p>

<p>In general, we view thre
ads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code.</p>

<p>The name that the <strong>POSIX</strong> library uses for a lock is a <strong>mutex</strong>, as it is used to provide <strong>mutual exclusion</strong> between threads.</p>

<p><strong>Building A Lock</strong></p>

<p>Some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux).</p>

<p><strong>Evaluating Locks</strong></p>

<ul>
  <li>The first is whether the lock does its basic task, which is to provide <strong>mutual exclusion</strong>. Basically, does the lock work, preventing multiple threads from entering a critical section?</li>
  <li>The second is <strong>fairness</strong>. Does each thread contending for the lock get a fair shot at acquiring it once it is free?</li>
  <li>The final criterion is <strong>performance</strong>, specifically the time overheads added by using the lock.</li>
</ul>

<p><strong>Controlling Interrupts</strong></p>

<p>Turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow.</p>

<p><strong>Plain Solution</strong></p>

<p>Without hardware support, just use a flag.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_plain_solution.png" alt="os-lock_plain_solution.png" /></p>

<p>The core issue is that the testing and setting part can be interrupted by context switch, and both thread enters the critical section.</p>

<p>You should get used to this thinking about concurrent programming. Maybe pretend yourself as a <strong>malicious scheduler</strong> to understand the concurrent execution.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_no_mutal_exclusion.png" alt="os-lock_no_mutal_exclusion.png" /></p>

<p><strong>Test And Set (Atomic Exchange)</strong></p>

<p>Let hardware provides a transaction-like instrument to ensure the sequence of operations is performed <strong>atomically</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_test_and_set.png" alt="os-lock_test_and_set.png" /></p>

<p>The key, of course, is that this sequence of operations is performed atomically. The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_test_and_set.png" alt="os-lock_spin_lock_by_test_and_set.png" /></p>

<p>By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock.</p>

<p><strong>Compare-And-Swap</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_compare_and_swap.png" alt="os-lock_compare_and_swap.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_compare_and_swap.png" alt="os-lock_spin_lock_by_compare_and_swap.png" /></p>

<p>compare-and-swap is a more powerful instruction than test-and-set. We will make some use of this power in the future when we briefly delve into <strong>wait-free synchronisation</strong>.</p>

<p><strong>Load-Linked and Store-Conditional</strong></p>

<p>Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture, for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_load_linked_store_conditional.png" alt="os-lock_load_linked_store_conditional.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_load_linked_store_conditional.png" alt="os-lock_spin_lock_by_load_linked_store_conditional.png" /></p>

<p><strong>Fetch-And-Add</strong></p>

<p>Fetch-and-add atomically increments a value while returning the old value at a particular address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_fetch_and_add.png" alt="os-lock_fetch_and_add.png" /></p>

<p>Fetch-and-add could build a <em>ticket lock</em>, this solution uses a ticket and turn variable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (myturn). The globally shared lock-&gt;turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. It has the advantage of the fairness, ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_ticket_lock_by_fetch_and_add.png" alt="os-lock_ticket_lock_by_fetch_and_add.png" /></p>

<p><strong>Spin Lock</strong></p>

<p>We use a while loop to endlessly check the value of a flag, this technique is known as <strong>spin-waiting</strong>. Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)!</p>

<p><strong>Spin lock</strong> is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a <strong>preemptive scheduler</strong>. (Remember that SJF is non-preemptive, but STCF is preemptive, which means permitting one thread to be interrupted).</p>

<p>Evaluating</p>

<ul>
  <li>√ correctness, the spin lock only allows a single thread to enter the critical section at a time.</li>
  <li>X fairness, spin locks don’t provide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Spin locks are not fair and may lead to starvation.</li>
  <li>X performance, bad in the single CPU case. The problem gets worse with N threads contending for a lock; N − 1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock.</li>
</ul>

<p><strong>Avoid Spinning by Yield</strong></p>

<blockquote>
  <p>“just yield, baby!”</p>
</blockquote>

<p>Hardware support alone cannot solve the problem. We’ll need OS support too! Assume an operating system primitive <strong>yield()</strong> which a thread can call when it wants to give up the CPU and let another thread run. A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller from the <strong>running</strong> state to the <strong>ready</strong> state, and thus promotes another thread to running. Thus, the yielding process essentially <strong>deschedules</strong> itself.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield.png" alt="os-lock_with_test_and_set_and_yield.png" /></p>

<p>This approach eliminates the spinning time, but still costly when context switching. And we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section.</p>

<p><strong>Avoid Spnning by Queues</strong></p>

<p>The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread runs that must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation.</p>

<p>Thus, we must explicitly exert some control over who gets to acquire the lock next after the current holder releases it.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield_and_queue.png" alt="os-lock_with_test_and_set_and_yield_and_queue.png" /></p>

<p>This approach thus doesn’t avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.</p>

<p>With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the first thread would then sleep forever (potentially). This problem is sometimes called the <strong>wakeup/waiting race</strong>.</p>

<p>Solaris solves this problem by adding a third system call: <strong>setpark()</strong>. By calling this routine, a thread can indicate it is about to park. If it then happens t be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.</p>

<p>You might also notice the interesting fact that the flag does not get set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from park(); however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; flag is not set to 0 in-between.</p>

<p><strong>Linux Support</strong></p>

<p>Linux provides something called a <strong>futex</strong> which is similar to the Solaris interface but provides a bit more in-kernel functionality. Specifically, each futex has associated with it a specific physical memory location; associated with each such memory location is an in-kernel queue.</p>

<ul>
  <li><code>futex_wait(address, expected)</code> puts the calling thread to sleep, assouming the value at address is equal to expected. If it is not equal, the call returns immediately.</li>
  <li><code>futex_wake(address)</code> wakes one thread that is wait- ing on the queue.</li>
</ul>

<p>Linux approach has the flavor of an old approach that has been used on and off for years, , and is now referred to as a <strong>two-phase lock</strong>. A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So in the first phase, the lock spins for a while, hoping that it can acquire the lock. However, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.</p>

<h2 id="chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</h2>

<p><strong>Background</strong></p>

<p>Adding locks to a data structure to make it usable by threads makes the structure <strong>thread safe</strong>. There is always a standard method to make a concurrent data structure: add a big lock. But sometimes we need to ensure the scalability.</p>

<p>To evaluate the concurrent data structures, theres are two factors to concern:</p>

<ul>
  <li>Correctness</li>
  <li>Performance. MORE CONCURRENCY ISN’T NECESSARILY FASTER. If the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important. Build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do.</li>
</ul>

<p>Ideally, you’d like to see the threads complete just as quickly on multiple processors as the single thread does on one. Achieving this end is called <strong>perfect scaling</strong>.</p>

<p><strong>Guidelines</strong></p>

<ul>
  <li>Be careful with acquisition and release of locks around control flow changes</li>
  <li>Enabling more concurrency does not necessarily increase performance</li>
  <li>Performance problems should only be remedied once they exist, avoiding premature optimization, is central to any performance-minded developer</li>
  <li>There is no value in making something faster if doing so will not improve the overall performance of the application.</li>
</ul>

<p><strong>Concurrent Counters</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_performance_concurrent_counters.png" alt="os-lock_performance_concurrent_counters.png" /></p>

<p>Traditional Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_traditional_counter.png" alt="os-lock_traditional_counter.png" /></p>

<p>In this manner, it is similar to a data structure built with <strong>monitors</strong>, where locks are acquired and released automatically as you call and return from object methods.</p>

<p>The performance of the synchronized counter scales poorly.</p>

<p>Sloppy Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter.png" alt="os-lock_sloppy_counter.png" /></p>

<p>The sloppy counter works by representing a single logical counter via numerous local physical counters, one per CPU core, as well as a single global counter.
When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock.
How often this local-to-global transfer occurs is determined by a threshold, which we call S here (for sloppiness). The smaller S is, the more the counter behaves like the non-scalable counter above; the bigger S is, the more scalable the counter, but the further off the global value might be from the actual count.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter_scaling.png" alt="os-lock_sloppy_counter_scaling.png" /></p>

<p><strong>Concurrent Linked Lists</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list.png" alt="os-lock_concurrent_link_list.png" /></p>

<p>One small tricky issue arises if malloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert. This kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths.</p>

<p>BE WARY OF LOCKS AND CONTROL FLOW</p>

<p>Many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone. Thus, it is best to structure code to minimize this pattern.</p>

<p>Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list_optimized.png" alt="os-lock_concurrent_link_list_optimized.png" /></p>

<p>Once again we are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency within a list is something called <strong>hand-over-hand locking</strong> (a.k.a. <strong>lock coupling</strong>).</p>

<p>Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing the list, the code first grabs the next node’s lock and then releases the current node’s lock.</p>

<p>It enables a high degree of concurrency in list operations. However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive. Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating.</p>

<p><strong>Concurrent Queues</strong></p>

<p>Look at a slightly more concurrent queue designed by Michael and Scott.</p>

<p>There are two locks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. One trick used by the Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_queue.png" alt="os-lock_concurrent_queue.png" /></p>

<p><strong>Concurrent Hash Table</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_hash_table.png" alt="os-lock_concurrent_hash_table.png" /></p>

<p>This concurrent hash table is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well. The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_scaling_hash_table.png" alt="os-lock_scaling_hash_table.png" /></p>

<p>AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)</p>

<blockquote>
  <p>“Premature optimization is the root of all evil.”</p>
</blockquote>

<p>Many operating systems utilized a single lock when first transitioning to multiprocessors, including Sun OS and Linux. In the latter, this lock even had a name, the <strong>big kernel lock (BKL)</strong>. When multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck. Thus, it was finally time to add the optimization of improved concurrency to these systems. Within Linux, the more straightforward approach was taken: replace one lock with many. Within Sun, a more radical decision was made: build a brand new operating system, known as Solaris, that incorporates concurrency more fundamentally from day one.</p>

<h2 id="chapter-30---condition-variables">Chapter 30 - Condition Variables</h2>

<p><strong>Background</strong></p>

<p>There are many cases where a thread wishes to check whether a condition is true before continuing its execution. For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a <code>join()</code>).</p>

<p>In multi-threaded programs, it is often useful for a thread to wait for some conditio to become true before proceeding. The simple approach, of just spinning until the condition becomes true, is grossly inefficient and wastes CPU cycles, and in some cases, can be incorrect.</p>

<p><strong>Definition and Routines</strong></p>

<p>To wait for a condition to become true, a thread can make use of what is known as a condition variable. A <strong>condition variable</strong> is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition) is not as desired (by <strong>waiting</strong> on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by <strong>signaling</strong> on the condition).</p>

<p>By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the famous (and still important) producer/consumer problem, as well as covering conditions.</p>

<p>A condition variable has two operations associated with it: <strong>wait()</strong> and <strong>signal()</strong>.</p>

<ul>
  <li>The <strong>wait()</strong> call is executed when a thread wishes to put itself to sleep</li>
  <li>The <strong>signal()</strong> call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo.png" alt="os-cv_waiting_demo.png" /></p>

<p><strong><em>Is the state variable <code>done</code> necessary?</em></strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo_2.png" alt="os-cv_waiting_demo_2.png" /></p>

<p>Yes. Imagine the case where the child runs immediately and calls thr exit() immediately; in this case, the child will signal, but there is no thread asleep on the condition. When the parent runs, it will simply call wait and be stuck; no thread will ever wake it. From this example, you should appreciate the importance of the state variable done; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it.</p>

<p><strong><em>Is there a need to hold the lock while singaling?</em></strong></p>

<p>Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables. The generalization of this tip is correct: hold the lock when calling signal or wait, and you will always be in good shape.</p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>The producer/consumer problem, or sometimes as the bounded buffer problem, which was first posed by Dijkstra. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized <strong>semaphore</strong> (which can be used as either a lock or a condition variable).</p>

<p>A bounded buffer is also used when you pipe the output of one program into another, e.g.,</p>

<p><code>sh
// grep process is the producer
// wc process is the consumer
// between them is an in-kernel bounded buffer
grep foo file.txt | wc -l
</code></p>

<p>Basic operations: <code>put()</code> and <code>get()</code></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_put_and_get_v1.png" alt="os-cv_put_and_get_v1.png" /></p>

<p><strong>Plain Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_v1.png" alt="os-cv_producer_and_consumer_v1.png" /></p>

<p><strong>Single CV and If</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if.png" alt="os-cv_producer_and_consumer_single_cv_and_if.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_if_trace.png" /></p>

<p><strong>Single CV and While</strong></p>

<p>Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired. This interpretation of what a signal means is often referred to as <strong>Mesa semantics</strong>, after the first research that built a condition variable in such a manner. Virtually every system ever built employs Mesa semantincs.</p>

<p>Thanks to Mesa semantics, a simple rule to remember with condition variables is to <strong>always use while loops</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while.png" alt="os-cv_producer_and_consumer_single_cv_and_while.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_while_trace.png" /></p>

<p><strong>Two CVs and While</strong></p>

<p>Signaling is clearly needed, but must be more directed. <strong>A consumer should not wake other consumers, only producers</strong>, and vice-versa.</p>

<p>Use two condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Producer threads wait on the condition empty, and signals fill. Conversely, consumer threads wait on fill and signal empty.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_two_cv_and_while.png" alt="os-cv_producer_and_consumer_single_two_cv_and_while.png" /></p>

<p><strong>Final Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_final_solution.png" alt="os-cv_producer_and_consumer_final_solution.png" /></p>

<p><strong>Covering Conditions</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_covering_conditions.png" alt="os-cv_producer_and_consumer_covering_conditions.png" /></p>

<p>Assume there are zero bytes free; thread Ta calls <code>allocate(100)</code>, followed by thread Tb which asks for less memory by calling <code>allocate(10)</code>. Both Ta and Tb thus wait on the condition and go to sleep; there aren’t enough free bytes to satisfy either of these requests. At that point, assume a third thread, Tc, calls <code>free(50)</code>. Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed; Ta should remain waiting, as not enough memory is yet free. Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.</p>

<p>The solution suggested by Lampson and Redell is straightforward: replace the <code>pthread_cond_signal()</code> call in the code above with a call to <code>pthread_cond_broadcast()</code>, which wakes up all waiting threads. Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.</p>

<p>Lampson and Redell call such a condition a <strong>covering condition</strong>, as it covers all the cases where a thread needs to wake up (conservatively); the cost, is that too many threads might be woken.</p>

<p>In general, if you find that your program only works when you change your signals to broadcasts (but you don’t think it should need to), you probably have a bug; fix it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available.</p>

<h2 id="chapter-31---semaphores">Chapter 31 - Semaphores</h2>

<p><strong>Background</strong></p>

<p>As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems. One of the first people to realize this years ago was Edsger Dijkstra. Dijkstra and colleagues invented the semaphore as a single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables.</p>

<p><strong>Definition</strong></p>

<p>A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem <code>wait()</code> and sem <code>post()</code>. The initial value of the semaphore determines its behaviour.</p>

<p>Semaphores are a powerful and flexible primitive for writing concurrent programs. Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility.</p>

<p>In my view, semaphore is an primitive, which can be made by locks and condition variables, also can’t be used as locks and condition variables.</p>

<p>Initialization</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_init.png" alt="os-semaphore_init.png" /></p>

<p>Usage
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_definition.png" alt="os-semaphore_definition.png" /></p>

<ul>
  <li><code>sem_wait()</code> will either return right away (because the value of the semaphore was one or higher when we called <code>sem_wait()</code>), or it will cause the caller to suspend execution waiting for a subsequent post.</li>
  <li><code>sem_post()</code> does not wait for some particular condition to hold like <code>sem_wait()</code> does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.</li>
  <li>The value of the semaphore, when negative, is equal to the number of waiting threads</li>
</ul>

<p><strong>Semaphores As Locks</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_locks.png" alt="os-semaphore_as_locks.png" /></p>

<p>Because locks only have two states (held and not held), this usage is sometimes known as a <strong>binary semaphore</strong>.</p>

<p><strong>Semaphores As Condition Variables</strong></p>

<p>Semaphores are also useful when a thread wants to halt its progress waiting for a
 condition to become true. In this pattern of usage, we often find a thread waiting for something to happen, and a different thread making that something happen and then signaling that it has happened, thus waking the waiting thread.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_cv.png" alt="os-semaphore_as_cv.png" /></p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>Plain Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_plain.png" alt="os-semaphore_producer_and_consumer_plain.png" /></p>

<p>The condition variable (semaphore based) controls the execution order, which can let multiple threads enter the critical section at the same time. It still needs a lock.</p>

<p>Adding Mutual Exclusion</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex.png" alt="os-semaphore_producer_and_consumer_add_mutex.png" /></p>

<p>The consumer holds the mutex and is waiting for the someone to signal full. The producer could si!gnal full but is waiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.</p>

<p>To avoid the deadlock, we can simply move the mutex acquire and release to be just around the critical section. The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex_correctly.png" alt="os-semaphore_producer_and_consumer_add_mutex_correctly.png" /></p>

<p><strong>Reader-Writer Locks</strong></p>

<p>Another classic problem stems from the desire for a more flexible <strong>locking primitive</strong> that admits that different data structure accesses might require different kinds of locking.</p>

<p>Imagine a number of concurrent list operations, including inserts and simple lookups. While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a <strong>reader-writer lock</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_reader_writer_lock.png" alt="os-semaphore_reader_writer_lock.png" /></p>

<p>Once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until all readers are finished; the last one to exit the critical section calls sem <code>post()</code> on “writelock” and thus enables a waiting writer to acquire the lock.</p>

<p>This approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it would be relatively easy for readers to starve writers. It should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives.</p>

<p>SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)</p>

<p>You should never underestimate the notion that the simple and dumb approach can be the best one. Always try the simple and dumb approach first.</p>

<p><strong>The Dining Philosophers</strong></p>

<p>One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem.</p>

<p>There are five “philosophers” sitting around a table. Between each pair of philosophers is a single fork (and thus, five total). The philosophers each have times where they think, and don’t need any forks, and times where they eat. In order to eat, a philosopher needs two forks, both the one on their left and the one on their right. The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers.png" alt="os-semaphore_dinning_philosophers.png" /></p>

<p>Broken Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_deadlock_solution.png" alt="os-semaphore_dinning_philosophers_deadlock_solution.png" /></p>

<p>The problem is deadlock. If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever.</p>

<p>A Solution: Breaking The Dependency</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_solution.png" alt="os-semaphore_dinning_philosophers_solution.png" /></p>

<p><strong>Implement Semaphores</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_implementation.png" alt="os-semaphore_implementation.png" /></p>

<h2 id="chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</h2>

<p><strong>Background</strong></p>

<p>Lu et al has made a study, which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_bugs.png" alt="os-concurrency_bugs.png" /></p>

<p><strong>Non-Deadlock Bugs</strong></p>

<ul>
  <li>Atomicity violation bugs. The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution). Solve by locks.</li>
  <li>Order violation bugs. The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution). Solve by condition variables.</li>
</ul>

<p><strong>Deadlock Bugs</strong></p>

<p>Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_dependency.png" alt="os-concurrency_deadlock_dependency.png" /></p>

<p><strong>Caused by</strong></p>

<p>One reason is that in large code bases, complex dependencies arise between cmponents. The design of locking strategies in large systems must be carefully done to avoid deadlock in the case of <strong>circular dependencies</strong> that may occur naturally in the code.</p>

<p>Another reason is due to the nature of <strong>encapsulation</strong>. As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_by_encapsulation.png" alt="os-concurrency_deadlock_by_encapsulation.png" /></p>

<p><strong>Conditions for Deadlock</strong></p>

<ul>
  <li><strong>Mutual exclusion</strong>: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).</li>
  <li><strong>Hold-and-wait</strong>: Threads hold resources allocated to them (e.g.,locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).</li>
  <li><strong>No preemption (hold)</strong>: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.</li>
  <li><strong>Circular wait (wait)</strong>: There exists a coircular chain of threads such that each thread holds one more resources (e.g., locks) that are being requested by the next thread in the chain.</li>
</ul>

<p><strong>Prevention Based on Four Conditions</strong></p>

<p>Mutual Exclusion</p>

<p>To avoid the need for mutual exclusion at all. Herlihy had the idea that one could design various data structures to be <strong>wait-free</strong>. The idea here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_wait_free.png" alt="os-concurrency_deadlock_wait_free.png" /></p>

<p>However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.</p>

<p>Hold-and-wait</p>

<p>The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_hold_and_wait.png" alt="os-concurrency_deadlock_hold_and_wait.png" /></p>

<p>By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided.</p>

<p>Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.</p>

<p>No Preemption</p>

<p>Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, a <code>trylock()</code> routine will grab the lock (if it is available) or return -1 indicating that the lock is held right now and that you should try again later if you want to grab that lock.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_no_preemption.png" alt="os-concurrency_deadlock_no_preemption.png" /></p>

<p>One new problem does arise, however: <strong>livelock</strong>. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name lovelock. One could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.</p>

<p>Another issues arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement.</p>

<p>Circular Wait</p>

<p>The best solution in practice is to be careful, develop a lock acquisition order, and thus prevent deadlock from occurring in the first place.</p>

<ul>
  <li>The most straightforward way to do that is to provide a <strong>total ordering</strong> on lock acquisition. For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.</li>
  <li>A <strong>partial ordering</strong> can be a useful way to structure lock acquisition so as to avoid deadlock.</li>
</ul>

<p><strong>Avoidance via Scheduling</strong></p>

<p>Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_avoid_via_scheduling.png" alt="os-concurrency_deadlock_avoid_via_scheduling.png" /></p>

<p>Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution</p>

<p><strong>Detect and Recover</strong></p>

<p>One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected.</p>

<p>Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.</p>

<p>DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)</p>

<p>Tom West says famously, “Not everything worth doing is worth doing well”, which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small.</p>

<p><strong>Others</strong></p>

<p>Perhaps the best solution is to develop new concurrent programming models: in systems such as <strong>MapReduce</strong> (from Google), programmers can describe certain types of parallel computations without any locks whatsoever.</p>

<h2 id="chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</h2>

<p><strong>Background</strong></p>

<p>A different style of concurrent programming is often used in both GUI-based applications as well as some types of internet servers. This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js, but its roots are found in C/UNIX systems that we’ll discuss below.</p>

<p>Event-based servers give control of scheduling to the application itself, but do so at some cost in complexity and difficulty of integration with other aspects of modern systems (e.g., paging). Because of these challenges, no single approach has emerged as best; thus, both threads and events are likely to persist as two different approaches to the same concurrency problem for many years to come.</p>

<p>The problem that event-based concurrency addresses is two-fold.</p>

<ul>
  <li>The first is that managing concurrency correctly in multi-threaded applications can be challenging.</li>
  <li>The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs.</li>
</ul>

<p><strong>The Basic Idea: An Event Loop</strong></p>

<p>The approach is quite simple: you simply wait for something (i.e., an “event”) to occur; when it does, you check what type of  event it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.). That’s it!</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_loop.png" alt="os-event_loop.png" /></p>

<p>Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle ext is equivalent to scheduling. This explicit control over scheduling is one of the fundamental advantages of the event- based approach.</p>

<p>But there is a big question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O? Specifically, how can an event server tell if a message has arrived for it?</p>

<p><strong>An Important API: select() (or poll())</strong></p>

<p>In most systems, a basic API is available, via either the <strong>select()</strong> or <strong>poll()</strong> system calls. Either way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed.</p>

<p>What these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_api.png" alt="os-event_select_api.png" /></p>

<p>First, note that it lets you check whether descriptors can be reand from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).</p>

<p>Second, note the timeout argument. One common usage here is to set the timeout to <code>NULL</code>, which causes <code>select()</code> to block indefinitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to <code>select()</code> to return immediately.</p>

<p>Now linux uses <strong>epoll</strong>, FreeBSD (Mac OS) uses <strong>kqueue</strong>, and Windows uses <strong>IOCP</strong>.</p>

<p>BLOCKING VS. NON-BLOCKING INTERFACES</p>

<ul>
  <li>Blocking (or synchronous) interfaces do all of their work before returning to the caller. The usual culprit in blocking calls is I/O of some kind.</li>
  <li>Non-blocking (or asynchronous) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background. Non-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.</li>
</ul>

<p>DON’T BLOCK IN EVENT-BASED SERVERS</p>

<p>Event-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution the caller can ever be made; failing to obey this design tip will result in a blocked event-based server.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_code_demo.png" alt="os-event_select_code_demo.png" /></p>

<p>Advantage</p>

<p>With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Specifically, because only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread because it is decidedly single threaded. Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach.</p>

<p><strong>Issue: Blocking System Calls</strong></p>

<p>For example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request). Both the open() and read() calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service.</p>

<p>With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress. Indeed, this natural <strong>overlap</strong> of I/O and other computation is what makes thread-based programming quite natural and straight-forward.</p>

<p>With an event-based approach, however, there are no other threads to run: just the main event loop. And this implies that if an event handler issues a call that blocks, the entire server will do just that: block until the call completes.</p>

<p>We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed.</p>

<p>Solution: Asynchronous I/O</p>

<p>To overcome this limit, many modern operating systems have intro- duced new ways to issue I/O requests to the disk system, referred to generically as asynchronous I/O. These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed; additional interfaces enable an application to determine whether various I/Os have completed.</p>

<p>The APIs revolve around a basic structure, the struct aiocb or <strong>AIO control block</strong> in common terminology.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_aio_control_block.png" alt="os-event_aio_control_block.png" /></p>

<ul>
  <li>An application can periodically poll the system via a call to aio error() to determine whether said I/O has yet completed.</li>
  <li>Some systems provide an approach based on the interrupt. This method uses UNIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system.</li>
</ul>

<p>In systems without asynchronous I/O, the pure event-based approach cannot be implemented. However, clever researchers have derived methods that work fairly well in their place. For example, Pai et al describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os.</p>

<p>UNIX SIGNALS</p>

<p>A huge and fascinating infrastructure known as <strong>signals</strong> is present in all mod ern UNIX variants. At its simplest, signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a <strong>signal handler</strong>, i.e., some code in the application to handle that signal. When finished, the process just resumes its previous behaviour. A program can be configured to catch that signal. Or when a signal is sent to a process not config- ured to handle that signal, some default behavior is enacted; for SEGV, the process is killed.</p>

<p><strong>Issue: State Management</strong></p>

<p>When an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread. Adya et al. call this work <strong>manual stack management</strong>, and it is fundamental to event-based programming.</p>

<p>Solution: Continuation</p>

<p>Use an old programming language construct known as a <strong>continuation</strong>. Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_state_management.png" alt="os-event_state_management.png" /></p>

<p>Record the socket descriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (fd). When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller.</p>

<p><strong>What Is Still Difficult With Events</strong></p>

<p>Multiple CPUS. When systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared. Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel; when doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed. Thus, on modern multicore systems, simple event handling without locks is no longer possible.</p>

<p>Implicit blocking. It does not integrate well with certain kinds of systems activity, such as paging. For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes. Even though the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.</p>

<p>API changes all the time. That event-based code can be hard to manage over time, as the exact semantics of various routines changes]. For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces. Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.</p>

<p>Async network I/O. Though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there, and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think. For example, while one would simply like to use the select() interface to manage all outstanding I/Os, usually some combination of select() for networking and the AIO calls for disk I/O are required.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Virtualization - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces/"/>
    <updated>2015-11-22T13:44:38+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#cpu-virtualisation" id="markdown-toc-cpu-virtualisation">CPU Virtualisation</a>    <ul>
      <li><a href="#process" id="markdown-toc-process">Process</a>        <ul>
          <li><a href="#chapter-4---the-abstraction-the-process" id="markdown-toc-chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</a></li>
          <li><a href="#chapter-5---interlude-process-api" id="markdown-toc-chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</a></li>
        </ul>
      </li>
      <li><a href="#mechanism" id="markdown-toc-mechanism">Mechanism</a>        <ul>
          <li><a href="#chapter-6---mechanism-limited-direct-execution" id="markdown-toc-chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</a></li>
        </ul>
      </li>
      <li><a href="#scheduling" id="markdown-toc-scheduling">Scheduling</a>        <ul>
          <li><a href="#chapter-7---scheduling-introduction" id="markdown-toc-chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</a></li>
          <li><a href="#chapter-8---scheduling-the-multi-level-feedback-queue" id="markdown-toc-chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</a></li>
          <li><a href="#chapter-9---scheduling-proportional-share" id="markdown-toc-chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</a></li>
          <li><a href="#chapter-10---multiprocessor-scheduling" id="markdown-toc-chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#memory-virtualisation" id="markdown-toc-memory-virtualisation">Memory Virtualisation</a>    <ul>
      <li><a href="#address-space" id="markdown-toc-address-space">Address Space</a>        <ul>
          <li><a href="#chapter-13---the-abstraction-address-spaces" id="markdown-toc-chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</a></li>
          <li><a href="#chapter-14---interlude-memory-api" id="markdown-toc-chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</a></li>
        </ul>
      </li>
      <li><a href="#dynamic-allocation-and-segmentation" id="markdown-toc-dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</a>        <ul>
          <li><a href="#chapter-15---mechanism-address-translation" id="markdown-toc-chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</a></li>
          <li><a href="#chapter-16-segmentation" id="markdown-toc-chapter-16-segmentation">Chapter 16 Segmentation</a></li>
          <li><a href="#chapter-17---free-space-management" id="markdown-toc-chapter-17---free-space-management">Chapter 17 - Free-Space Management</a></li>
        </ul>
      </li>
      <li><a href="#paging" id="markdown-toc-paging">Paging</a>        <ul>
          <li><a href="#chapter-18---paging-introduction" id="markdown-toc-chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</a></li>
          <li><a href="#chapter-19---paging-faster-translations-tlbs" id="markdown-toc-chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</a></li>
          <li><a href="#note-on-cache-management" id="markdown-toc-note-on-cache-management">Note on Cache Management</a></li>
          <li><a href="#chapter-20---paging-smaller-tables" id="markdown-toc-chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</a></li>
        </ul>
      </li>
      <li><a href="#beyond-physical-memory" id="markdown-toc-beyond-physical-memory">Beyond Physical Memory</a>        <ul>
          <li><a href="#chapter-21---beyond-physical-memory-mechanisms" id="markdown-toc-chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</a></li>
          <li><a href="#chapter-22---beyond-physical-memory-policies" id="markdown-toc-chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</a></li>
          <li><a href="#chapter-23---the-vaxvms-virtual-memory-system" id="markdown-toc-chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>

<p><strong>The Crux of the whole book</strong></p>

<p>How does the operating system virtualize resources?
What mechanisms and policies are implemented by the OS to attain virtualization?
How does the OS do so efficiently?</p>

<p><strong>The Von Neumann model of computing</strong></p>

<p>Many millions (and these days, even billions) of times every second, the processor <strong>fetches</strong> an instruction from memory, <strong>decodes</strong> it (i.e., figures out which instruction this is), and <strong>executes</strong> it.</p>

<p><strong>The OS is sometimes known as a resource manager</strong></p>

<p>The primary way the OS does this is through a general technique that we call virtualization. That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more general, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a <strong>virtual machine</strong>.</p>

<p><strong>Virtualizing the CPU</strong></p>

<p>Turning a single CPU (or small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to seemingly run at once is what we call virtualizing the CPU.</p>

<p><strong>Virtualizing the Memory</strong></p>

<p>Memory is just an array of bytes; to <strong>read</strong> memory, one must specify an <strong>address</strong> to be able to access the data stored there; to <strong>write</strong> (or update) memory, one must also specify the data to be written to the given address.</p>

<p>The OS is virtualizing memory. Each process accesses its own private <strong>virtual address space</strong> (sometimes just called its address space)</p>

<p><strong>Concurrency</strong></p>

<p>Three instructions: one to <strong>load</strong> the value of the counter from memory into a register, one to <strong>increment</strong> it, and one to <strong>store</strong> it back into memory. Because these three instructions do not execute atomically (all at once), strange things can happen.</p>

<p><strong>Persistence</strong></p>

<p>The software in the operating system that usually manages the disk is called the <strong>file system</strong>; it is thus responsible for storing any files the user creates in a reliable and efficient manner on the disks of the system.</p>

<p>For performance reasos, most file systems first <strong>delay</strong> such writes for a while, hoping to batch them into larger groups. To handle the problems of system crashes during writes, most file systems incorporate some kind of intricate write protocol, such as <strong>journaling</strong> or <strong>copy-on-write</strong>, carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards.</p>

<p><strong>Design Goals</strong></p>

<p>What an OS actually does: it takes physical <strong>resources</strong>, such as a CPU, memory, or disk, and <strong>virtualizes</strong> them. It handles tough and tricky issues related to <strong>concurrency</strong>. And it stores files <strong>persistently</strong>, thus making them safe over the long-term.</p>

<ol>
  <li>To build up some <strong>abstractions</strong> in order to make the system convenient and easy to use.</li>
  <li>To provide high <strong>performance</strong>, another way to say this is our goal is to minimize the overheads of the OS.</li>
  <li>To provide <strong>protection</strong> between applications, as well as between the OS and applications. Protection is at nthe heart of one of the main principles underlying an operating system, which is that of <strong>isolation</strong>; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.</li>
</ol>

<p><strong>Some History</strong></p>

<ol>
  <li>Early Operating Systems: Just Libraries.  This mode of computing was known as <strong>batch</strong> processing.</li>
  <li>Beyond Libraries: Protection. The idea of a system call was invented. The key difference between a <strong>system call</strong> and a <strong>procedure call</strong> is that a system call transfers control (i.e., jumps) into the OS while simultaneously raising the hardware privilege level. User applications run in what is referred to as user mode which means the hardware restricts what applications can do; When a system call is initiated (usually through a special hardware instruction called a trap), the hardware transfers control to a pre-specified trap handler (that the OS set up previously) and simultaneously raises the privilege level to kernel mode.</li>
  <li>The Era of Multiprogramming by minicomputer. In particular, multiprogramming became commonplace due to the desire to make better use of machine resources. One of the major practical advances of the time was the introduction of the <strong>UNIX</strong> operating system, primarily thanks to <strong>Ken Thompson</strong> (and <strong>Dennis Ritchie</strong>) at Bell Labs (yes, the phone company). <strong>Bill Joy</strong>, made a wonderful distribution (the Berkeley Systems Distribution, or <strong>BSD</strong>) which had some advanced virtual memory, file system, and networking subsystems. Joy later co-founded Sun Microsystems.</li>
  <li>The Modern Era by PC with DOS, Mac OS.</li>
</ol>

<h1 id="cpu-virtualisation">CPU Virtualisation</h1>

<h2 id="process">Process</h2>

<h3 id="chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</h3>

<p><strong>Process</strong></p>

<p>The definition of a process, informally, is quite simple: it is a running program.</p>

<p><strong>How to provide the illusion of many CPUs?</strong></p>

<p>This basic technique, known as <strong>time sharing</strong> of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.</p>

<p><strong>Mechanisms</strong></p>

<p>Mechanisms are low-level methods or protocols that implement a needed piece of functionality.</p>

<p><strong>Policies</strong></p>

<p>On top of these mechanisms resides some of the intelligence in the OS, in the form of policies.</p>

<p><strong>Tip: Separate policy and mechanism</strong></p>

<p>In many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms. You can think of the mechanism as providing the answer to a <strong>how</strong> question about a system; for example, how does an operating system perform a context switch? The policy provides the answer
 to a <strong>which</strong> question; for example, which process should the operating system run right now?</p>

<p><strong>Machine State</strong></p>

<p>To understand what constitutes a process, we thus have to understand its <strong>machine state</strong>: what a program can read or update when it is running. At any given time, what parts of the machine are important to the execution of this program?</p>

<ol>
  <li>Memory. The memory that the process can address (called its <strong>address space</strong>) is part of the process.</li>
  <li>Registry. There are some particularly special registers that form part of this machine state. For example, the <strong>program counter</strong> (PC) (sometimes called the instruction pointer or IP). similarly a stack pointer and associated <strong>frame pointer</strong> are used to manage the stack for function parameters, local variables, and return addresses.</li>
  <li>I/O information. Programs often access persistent storage devices too. Such I/O information might include a list of the files the process currently has open.</li>
</ol>

<p><strong>Process API</strong></p>

<ol>
  <li>Create</li>
  <li>Destroy</li>
  <li>Wait</li>
  <li>Miscellaneous Control (suspend, resume)</li>
  <li>Status</li>
</ol>

<p><strong>How does the OS get a program up and running?</strong></p>

<ol>
  <li>To <strong>load</strong> its code and any static data (e.g., initialized variables) into memory, into the <strong>address space</strong> of the process. In early (or simple) operating systems, the loading process is done <strong>eagerly</strong>; modern OSes perform the process <strong>lazily</strong>, i.e., by loading pieces of code or data only as they are needed during program execution. To truly understand how lazy loading of pieces of code and data works, you’ll have to understand more about the machinery of <strong>paging</strong> and <strong>swapping</strong>.</li>
  <li>Once the code and static data are loaded into memory, there are a few other things the OS needs to do before running the process. Some memory must be allocated for the program’s <strong>run-time stack</strong> (or just stack). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.</li>
  <li>The OS may also allocate some memory for the program’s <strong>heap</strong>. In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling malloc() and free it explicitly by calling free(). The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures.</li>
  <li>The OS will also do some other initialization tasks, particularly as related to input/output (I/O). For example, in UNIX systems, each process by default has three open <strong>file descriptors</strong>.</li>
  <li>To start the program running at the entry point, namely main(), the OS transfers control of the CPU to the newly-created process, and thus the program begins its execution.</li>
</ol>

<p><strong>Process States</strong></p>

<ol>
  <li>Running</li>
  <li>Ready</li>
  <li>Blocked</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-process_state_transitions.png" alt="os-process_state_transitions.png" /></p>

<p><strong>Data Structures</strong></p>

<p>To track the state of each process, for example, the OS likely will keep some kind of <strong>process list</strong> for all processes that are ready, as well as some additional information to track which process is currently running.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-the_xv6_proc_structure.png" alt="os-the_xv6_proc_structure.png" /></p>

<p>The <strong>register context</strong> will hold, for a stopped process, the contents of its registers. When a process is stopped, its registers will be saved to this memory location; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running the process.</p>

<p>Sometimes people refer to the individual structure that stores information about a process as a <strong>Process Control Block (PCB)</strong>.</p>

<h3 id="chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</h3>

<p>UNIX presents one of the most intriguing ways to create a new process with a pair of system calls:</p>

<p><strong>fork()</strong></p>

<p>The newly-created process (called the <strong>child</strong>, in contrast to the creating <strong>parent</strong>) desn’t start running at main(), like you might expect (note, the “hello, world” message only got printed out once); rather, it just comes into life as if it had called fork() itself. You might have noticed: the child isn’t an exact copy. Specifically, al- though it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of fork() is different.</p>

<p>The output is <strong>not deterministic</strong>. When the child process is created, there are now two active processes in the system that we care about: the parent and the child.</p>

<p><strong>wait()</strong></p>

<p>Adding a wait() call to the code above makes the output <strong>deterministic</strong>.</p>

<p><strong>exec()</strong></p>

<p>It does not create a new process; rather, it transforms the currently running program (formerly p3) into a different running program (wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns.</p>

<p><strong>Why? Motivating The API</strong></p>

<p>Why would we build sucho an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of fork() and exec() is essential in building a UNIX shell, because it lets the shell run code after the call to fork() but before the call to exec(); this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.</p>

<p><strong>How Does Shell Utilise The API?</strong></p>

<p>The shell is just a user program.</p>

<ol>
  <li>It shows you a prompt and then waits for you to type something into it.</li>
  <li>You then type a command (i.e., the name of an executable program, plus any arguments) into it;</li>
  <li>In most cases, the shell then figures out where in the file system the executable resides</li>
  <li>calls fork() to create a new child process to run the command</li>
  <li>calls some variant of exec() to run the command</li>
  <li>waits for the command to complete by calling wait().</li>
  <li>When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command.</li>
</ol>

<p>eg. prompt&gt; wc p3.c &gt; newfile.txt</p>

<p>When the child is created, before calling exec(), the shell closes standard output and opens the file newfile.txt.</p>

<h2 id="mechanism">Mechanism</h2>

<h3 id="chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</h3>

<p><strong>The Crux</strong></p>

<ul>
  <li>performance: how can we implement virtualization without adding excessive overhead to the system?</li>
  <li>control: how can we run processes efficiently while retaining control over the CPU?</li>
</ul>

<p>Attaining performance while maintaining control is thus one of the central challenges in building an operating system.</p>

<p><strong>Basic Technique: Limited Direct Execution</strong></p>

<p>The basic idea is straightforward: just run the program you want to run on the CPU, but first make sure to set up the hardware so as to limit what the process can do without OS assistance.</p>

<p>In an analogous manner, the OS “baby proofs” the CPU, by first (during boot time) setting up the <strong>trap handlers</strong> and starting an <strong>interrupt timer</strong>, and then by only running processes in a restricted mode. By doing so, the OS can feel quite assured that processes can run efficiently, only requir- ing OS intervention to perform privileged operations or when they have monopolized the CPU for too long and thus need to be switched out.</p>

<p><strong>Problem #1: Restricted Operations</strong></p>

<p>Use Protected Control Transfer</p>

<p>The hardware assists the OS by providing different modes of execution. In <strong>user mode</strong>, applications do not have full access to hardware resources. In <strong>kernel mode</strong>, the OS has access to the full resources of the machine. When the user process wants to perform some kinds of privileged operation, it can perform a <strong>system call</strong>.</p>

<p><strong>System Call</strong></p>

<p>To execute a system call, a program must execute a special <strong>trap</strong> instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now per- form whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When finished, the OS calls a special <strong>return-from-trap</strong> instruction</p>

<p><strong>Why System Calls Look Like Procedure Calls?</strong></p>

<p>It is a procedure call, but hidden inside that procedure call is the famous trap instruction. More specifically, when you call open() (for example), you are executing a procedure call into the C library. The parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and return values correctly, as well as execute the hardware-specific trap instruction. And now you know why you personally don’t have to write assembly code to trap into an OS; somebody has already written that assembly for you.</p>

<p><strong>How does the trap know which code to run inside the OS?</strong></p>

<p>The kernel does so by setting up a <strong>trap table</strong> at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be. The OS informs the hardware of the locations of these <strong>trap handlers</strong>.</p>

<p><strong>Limited Direct Execution Protocol</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-limited_directed_execution_protocol.png" alt="os-limited_directed_execution_protocol.png" /></p>

<p>There are two phases in the LDE protocol:</p>

<p>In the first (at boot time), the kernel initializes the <strong>trap table</strong>, and the CPU remembers its location for subsequent use.</p>

<p>In the second (when running a process), the kernel sets up a few things (e.g., allocating a node on the process list, allocating memory) before using a <strong>return-from-trap</strong> instruction to start the execution of the process; this switches the CPU to user mode and begins running the process.</p>

<p>Normal flow:</p>

<p>When the process wishes to issue a system call, it traps back into the OS, which handles it and once again returns control via a return-from-trap to the process. The process then completes its work, and returns from main(); this usually will return into some stub code which will properly exit the program (say, by calling the exit() system call, which traps into the OS).</p>

<p><strong>Problem #2: Switching Between Processes</strong></p>

<p>How can the operating system regain control of the CPU so that it can switch between processes?</p>

<p>In a <strong>cooperative</strong> scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some kind to take place.</p>

<p>How can the OS gain control of the CPU even if processes are not being cooperative? What can the OS do to ensure a rogue process does not take over the machine?</p>

<p><strong>Timer Interrupt</strong></p>

<p>A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.</p>

<p>The OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is of course a privileged operation.</p>

<p><strong>Scheduler</strong></p>

<p>Whether to continue running the currently-running process, or switch to a different one. This decision is made by a part of the operating system known as the scheduler.</p>

<p>If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a <strong>context switch</strong>. A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its kernel stack).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-timer_interrupt.png" alt="os-timer_interrupt.png" /></p>

<h2 id="scheduling">Scheduling</h2>

<h3 id="chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</h3>

<p><strong>Scheduling Metrics</strong></p>

<ul>
  <li>performance
    <ul>
      <li>turnaround = T(completion) - T(arrival)</li>
      <li>responsive time = T(first run) - T(arrival)</li>
    </ul>
  </li>
  <li>fairness</li>
</ul>

<p>Performance and fairness are often at odds in scheduling.</p>

<p>The introduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born: response time.</p>

<p><strong>Assumption</strong></p>

<ol>
  <li>Each job runs for the same amount of time.</li>
  <li>All jobs arrive at the same time.</li>
  <li>Once started, each job runs to completion.</li>
  <li>All jobs only use the CPU (i.e., they perform no I/O)</li>
  <li>The run-time (length) of each job is known.</li>
</ol>

<p><strong>Policy 1-1 FIFO</strong></p>

<p>under assumption: 1,2,3,4,5</p>

<p>Given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm.</p>

<p><strong>Policy 1-2 SJF (Shortest Job First)</strong></p>

<p>under assumption: <del>1,</del>2,3,4,5</p>

<p>Why is FIFO not good?</p>

<p>If Assumption(1) is false, there will be the <strong>convoy effect</strong>, where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.</p>

<p>Is SJF preemptive?</p>

<p>No, it’s <strong>non-preemptive</strong>. In the old days of batch computing, a number of non-preemptive scheulers were developed; such systems would run each job to completi before considering whether to run a new job. Virtually all modern schedulers are <strong>preemptive</strong>, and quite willing to stop one process from running in order to run another.</p>

<p><strong>Policy 1-3 STCF (Shortest Time-to-Completion First) or PSJF (Preemptive Shortest Job First)</strong></p>

<p>under assumption: <del>1,2,3,</del>4,5</p>

<p>Notice that there a significant difference between SJF and STCF. As SJF is non-preemptive, system would run each job to completion before running other jobs. But STCF prefers the shortest time-to-completion jobs, which should preempt CPU to make sense. That’s why STCF also has another name, PSFJ, Preemptive Shortest Job First.</p>

<p><strong>Policy 2 RR (Round-Robin)</strong></p>

<p>The basic idea is simple: instead of running jobs to completion, RR runs a job for a <strong>time slice</strong> (sometimes called a scheduling quantum) and then switches to the next job in the run queue.</p>

<p>The length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance. Thus, de- ciding on the length of the time slice presents a trade-off to a system de- signer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive.</p>

<p>RR, with a reonasonable time slice, is thus an excellent scheduler if response time is our only metric. It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric.</p>

<p><strong>Policy 1 vs. Policy 2</strong></p>

<p>There is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to com- pletion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time. This type of trade-off is common in systems</p>

<p><strong>Incorporate I/O by overlap</strong></p>

<p>under assumption: 4</p>

<p>We see how a scheduler might incorporate I/O. By treating each CPU burst as a job, the scheduler makes sure processes that are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.</p>

<h3 id="chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</h3>

<p><strong>MLFQ</strong></p>

<p>it has <strong>multiple levels of queues</strong>, and <strong>uses feedback to determine the priority</strong> of a given job.</p>

<p>Instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads.</p>

<p><em>Multi-Level</em></p>

<p>The MLFQ has a number of distinct queues, each assigned a different <strong>priority level</strong>. At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run. Of course, more than one job may be on a given queue, and thus have the same priority. In this case, we will just use round-robin scheduling among those jobs.</p>

<p><em>Feedback</em></p>

<p>Thus, the key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.</p>

<p><strong>How To Change Priority</strong></p>

<p>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).
Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).
Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.</p>

<p><em>Problems</em></p>

<ol>
  <li>Starvation</li>
  <li>Smart user could rewrite their program to game the scheduler.</li>
  <li>A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.</li>
</ol>

<p><strong>How to prevent gaming of our scheduler?</strong></p>

<p>Rules 4a and 4b, let a job retain its priority by relinquishing the CPU before the time slice expires. The solution here is to perform better <strong>accounting</strong> of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue.</p>

<p>Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</p>

<p><strong>Priority Boost</strong></p>

<p>The simple idea here is to periodically boost the priority of all the jobs in system.</p>

<p>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</p>

<p><strong>Tuning MLFQ</strong></p>

<p>One big question is how to <strong>parameterize</strong> such a scheduler.</p>

<ul>
  <li>How many queues should there be?</li>
  <li>How big should the time slice be per queue?</li>
  <li>How often should priority be boosted in order to avoid starvation and account for changes in behavior?</li>
</ul>

<p><em>Some Variants</em></p>

<p>Most MLFQ variants allow for <strong>varying time-slice length</strong> across different queues. The high-priority queues are usually given short time slices; the low-priority queues, in contrast, contain long-running jobs that are CPU-bound; hence, longer time slices work well.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lower_priority_longer_quanta.png" alt="os-lower_priority_longer_quanta.png" /></p>

<p>The FreeBSD scheduler (version 4.3) uses a formula to calculate the current priority level of a job, basing it on how much CPU the process has used.</p>

<p>Some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow some user advice to help set priorities; for example, by using the command-line utility nice.</p>

<p><strong>Refined Rules</strong></p>

<ul>
  <li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t).</li>
  <li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li>
  <li>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).</li>
  <li>Rule 4: Once a job uses up its time allotment at a given level (re- gardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</li>
  <li>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</li>
</ul>

<h3 id="chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</h3>

<p><strong>0. Basic Idea</strong></p>

<p><strong>Proportional-share scheduler</strong>, also sometimes referred to as a <strong>fair-share scheduler</strong>. Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.</p>

<p><strong>Implementations</strong></p>

<ul>
  <li><strong>lottery</strong> scheduling, lottery uses randomness in a clever way to achieve proportional share</li>
  <li><strong>stride</strong> scheduling, stride does so deterministically</li>
</ul>

<p><strong>Application</strong></p>

<p>One is that such approaches do not particularly mesh well with I/O [AC97]; another is that they leave open the hard problem of ticket assignment, i.e., how do you know how many tickets your browser should be allocated?</p>

<p>As a result, proportional-share schedulers are more useful in domains where some of these problems (such as assignment of shares) are rela- tively easy to solve. For example, in a virtualized data centre.</p>

<p><strong>1. Lottery Scheduling</strong></p>

<p>The basic idea is quite simple: every so often, hold a lottery to determine which process should get to run next; processes that should run more often should be given more chances to win the lottery. One of the most beautiful aspects of lottery scheduling is its use of randomness.</p>

<p><strong>Advantage</strong></p>

<ul>
  <li>randomness
    <ul>
      <li>First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling.</li>
      <li>Second, random also is lightweight, requiring little state to track alternatives.</li>
      <li>Finally, random can be quite fast.</li>
    </ul>
  </li>
  <li>simplicity of implementation</li>
  <li>no global state</li>
</ul>

<p><strong>Disadvantage</strong></p>

<ul>
  <li>Hard to assign tickets to jobs</li>
  <li>Not deterministic. Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired outcome.</li>
</ul>

<p><strong>Ticket</strong></p>

<p>Tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question.</p>

<p><strong>Ticket Mechanisms</strong></p>

<p>Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways.</p>

<ul>
  <li>ticket currency</li>
  <li>ticket transfer</li>
  <li>ticket inflation</li>
</ul>

<p><strong>Implementation</strong></p>

<p>Probably the most amazing thing about lottery scheduling is the simplicity of its implementation.</p>

<ul>
  <li>a good random number generator to pick the winning ticket</li>
  <li>a data structure to track the processes of the system (e.g., a list)</li>
  <li>the total number of tickets.</li>
</ul>

<p><strong>2. Stride Scheduling</strong></p>

<p>a <strong>deterministic</strong> fair-share scheduler.</p>

<p>Respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. We call this value the <strong>stride</strong> of each process.</p>

<p>Jobs A, B, and C, with 100, 50, and 250 tickets. if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40.</p>

<p>Every time a process runs, we will increment a counter for it (called its <strong>pass</strong> value) by its stride to track its global progress. The scheduler then uses the stride and pass to determine which process should run next.</p>

<p>The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride.</p>

<p><strong>Advantage</strong></p>

<p>Lottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right at the end of each scheduling cycle.</p>

<p><strong>Disadvantage</strong></p>

<p>Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there. In this way, lottery makes it much easier to incorporate new processes in a sensible manner.</p>

<h3 id="chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</h3>

<p><em>TODO after reading Concurrency</em></p>

<h1 id="memory-virtualisation">Memory Virtualisation</h1>

<h2 id="address-space">Address Space</h2>

<h3 id="chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</h3>

<p><strong>Multiprogramming</strong> (多道程序), in which multiple processes were ready to run at a given time, and the OS would switch between them.</p>

<p><strong>Time sharing</strong>, One way to implement time sharing would be to run one process for a short while, giving it full access to all memory, then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process’s state, run it for a while, and thus implement some kind of crude sharing of the machine. Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows.</p>

<p><strong>Address space</strong></p>

<p>Address space, easy to use abstraction of physical memory, and it is the running program’s view of memory in the system. Understanding this fundamental OS ab- straction of memory is key to understanding how memory is virtualized.</p>

<p>When the OS does this, we say the OS is <strong>virtualizing memory</strong>.</p>

<p><strong>Goals</strong></p>

<p>The VM system is responsible for providing the illusion of a large, sparse, private address space to programs, which hold all of their instructions and data therein.</p>

<ul>
  <li>transparency</li>
  <li>efficiency</li>
  <li>protection (isolation)</li>
</ul>

<p><strong>EVERY ADDRESS YOU SEE IS VIRTUAL</strong></p>

<p>Any address you can see as a programmer of a user-level program is a virtual address, if you print out an address in a program, it’s a virtual one.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-every_address_you_see_is_virtual.png" alt="os-every_address_you_see_is_virtual.png" /></p>

<h3 id="chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</h3>

<p><strong>Types of Memory</strong></p>

<ul>
  <li><strong>stack memory</strong>, allocations and deallocations of it are managed implicitly by the compiler for you, the programmer.</li>
  <li><strong>heap memory</strong>, it is this need for long-lived memory, where all allocations and deallocations are explicitly handled by you, the programmer.</li>
</ul>

<p>Example</p>

<p><code>c
void func() {     int *x = (int *) malloc(sizeof(int));     ... }
</code></p>

<p>First, you might no- tice that both stack and heap allocation occur on this line: first the com- piler knows to make room for a pointer to an integer when it sees your declaration of said pointer (int *x); subsequently, when the program calls malloc(), it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program.</p>

<p><strong>API</strong></p>

<ul>
  <li><strong>malloc()</strong></li>
  <li><strong>free()</strong></li>
</ul>

<p>There are really two levels of memory management in the system. The first is level of memory management is performed by the OS, which hands out memory to processes when they run, and takes them back when processes exit (or otherwise die). The second level of management is within each process, for example within the heap when you call malloc() and free().</p>

<p>They are not system calls, but rather library calls. Thus the malloc library manages space within your virtual address space, but itself is built on top of some system calls.</p>

<ul>
  <li><strong>mmap()</strong></li>
</ul>

<p>You can also obtain memory from the operating system via the <code>mmap()</code> call. By passing in the correct arguments, mmap() can create an anonymous memory region within your program — a region which is not associated with any particular file but rather with swap space. This memory can then also be treated like a heap and managed as such.</p>

<ul>
  <li><strong>calloc()</strong></li>
</ul>

<p>Allocates memory and also zeroes it before returning; this prevents some errors where you assume that memory is zeroed and forget to initialize it yourself.</p>

<ul>
  <li><strong>realloc()</strong></li>
</ul>

<p>when you’ve allocated space for something (say, an array), and then need to add something to it: realloc() makes a new larger region of memory, copies the old region into it, and returns the pointer to the new region.</p>

<p><strong>Common Errors</strong></p>

<ul>
  <li>Forgetting To Allocate Memory - <strong>segmentation fault</strong>, which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY. Forget to allocate memory.</li>
  <li>Not Allocating Enough Memory - <strong>buffer overflow</strong></li>
  <li>Forgetting to Initialize Allocated Memory - <strong>uninitialized read</strong></li>
  <li>Forgetting To Free Memory - <strong>memory leak</strong></li>
  <li>Freeing Memory Before You Are Done With It - <strong>dangling pointer</strong></li>
  <li>Freeing Memory Repeatedly - <strong>double free</strong></li>
</ul>

<p><strong>Tools</strong></p>

<ul>
  <li><strong>gdb</strong>, add -g flag to gcc, then run it with gdb. eg. gcc -g null.c -o null -Wall &amp;&amp; gdb null</li>
  <li><strong>valgrind</strong>, eg. valgrind —leak-check=yes null</li>
</ul>

<h2 id="dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</h2>

<h3 id="chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</h3>

<p><strong>hardware-based address translation</strong></p>

<p>With address translation, the hardware transforms each memory access (e.g., an instruction fetch, load, or store), changing the <strong>virtual</strong> address provided by the instruction to a <strong>physical</strong> address where the desired information is actually located.</p>

<p>Transforming a virtual address into a physical address is exactly the technique we refer to as address translation.</p>

<p>Key to the efficiency of this technique is hardware support, which performs the translation quickly for each access, turning virtual addresses (the process’s view of memory) into physical ones (the actual view).</p>

<p><strong>Static (Software-based) Relocation</strong></p>

<p>A piece of software known as the loader takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memory.</p>

<p><strong>Dynamic (Hardware-based) Relocation</strong></p>

<p>The <strong>base and bounds</strong> technique is also referred to as dynamic relocation. With dynamic relocation, a little hardware goes a long way. Namely, a <strong>base</strong> register is used to transform virtual addresses (generated by the program) into physical addresses. A <strong>bounds</strong> (or <strong>limit</strong>) register ensures that such addresses are within the confines of the address space. Together they provide a simple and efficient virtualization of memory.</p>

<p>Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as dynamic relocation.</p>

<p>We should note that the base and bounds registers are hardware stru tures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the <strong>memory management unit (MMU)</strong>.</p>

<p><strong>Disadvantage</strong></p>

<p>The simple approach of using a base and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t fit into memory; thus, base and bounds is not as flexible as we would like.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-base_and_bounds.png" alt="os-base_and_bounds.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>The hardware should provide special instructions to modify the base and bounds registers, allowing the OS to change them when different processes run. These instructions are privileged; only in kernel (or privileged) mode can the registers be modified.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynaimic_relocation_hardware_requirement.png" alt="os-dynaimic_relocation_hardware_requirement.png" /></p>

<p><strong>Operating System Support</strong></p>

<p>The combination of hardware support and OS management leads to the implementation of a simple virtual memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_os_responsibility.png" alt="os-dynamic_relocation_os_responsibility.png" /></p>

<p><strong>Limited Direct Execution Protocol (Dynamic Relocation)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_LDE.png" alt="os-dynamic_relocation_LDE.png" /></p>

<h3 id="chapter-16-segmentation">Chapter 16 Segmentation</h3>

<p><strong>Segmentation: Generalized Base/Bounds</strong></p>

<p>Considering the disadvantage of the simple base and bounds, instead of having just one base and bounds pair in our <strong>MMU</strong>, why not <strong>have a base and bounds pair per logical segment of the address space</strong>? A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different segments: code, stack, and heap.</p>

<p>The hardware structure in our <strong>MMU</strong> required to support segmenta- tion is just what you’d expect: in this case, a set of three base and bounds register pairs.</p>

<p><strong>Advantage</strong></p>

<p>Remove the Inner Fragmentation.</p>

<p>What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation.png" alt="os-segmentation.png" /></p>

<p><strong>THE SEGMENTATION FAULT</strong></p>

<p>The term segmentation fault or violation arises from a memory access on a segmented machine to an illegal address. Humorously, the term persists, even on machines with no support for segmentation at all. Or not so humorously, if you can’t figure why your code keeps faulting</p>

<p><strong>Implementation</strong></p>

<p>One common approach, sometimes referred to as an explicit approach, is to chop up the address space into segments based on the top few bits of the virtual address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_implementation.png" alt="os-segmentation_implementation.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>Negative growth for stack, and protection bits for code sharing. (to save memory, sometimes it is useful to share certain memory segments between address spaces. In particular, <strong>code sharing</strong> is common and still in use in systems today.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_register_with_protection.png" alt="os-segmentation_register_with_protection.png" /></p>

<p><strong>Fine-grained vs. Coarse-grained Segmentation</strong></p>

<ul>
  <li>Coarse-grained, with just a few segments (i.e., code, stack, heap).</li>
  <li>Fine-grained, to consist of a large number smaller segments, with (further hardware support) a <strong>segment table</strong> of some kind stored in memory.</li>
</ul>

<p><strong>Disadvantage</strong></p>

<p>The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones. We call this problem <strong>external fragmentation</strong>.</p>

<p>Because segments are variablesized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be difficult. One can try to use smart algorithms or periodically compact memory, but the problem is fundamental and hard to avoid. (compact physical memory by rearranging the existing segments, is memory-intensive and generally uses a fair amount of processor time.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_compact_memory.png" alt="os-segmentation_compact_memory.png" /></p>

<p>Segmentation still isn’t flexible enough to support our fully generalized, sparse address space.</p>

<h3 id="chapter-17---free-space-management">Chapter 17 - Free-Space Management</h3>

<p>Managing free space can certainly be easy, as we will see when we discuss the concept of paging. It is easy when the space you are managing is divided into fixed-sized units; in such a case, you just keep a list of these fixed-sized units; when a client requests one of them, return the first entry.</p>

<p>Where free-space management becomes more difficult (and interesting) is when the free space you are managing consists of variable-sized units; this arises in a user-level memory-allocation library (as in malloc() and free()) and in an OS managing physical memory when using segmentation to implement virtual memory. In either case, the problem that exists is known as <strong>external fragmentation</strong>: the free space gets chopped into little pieces of different sizes and is thus fragmented; subsequent requests may fail because there is no single contiguous space that can satisfy the request, even though the total amount of free space exceeds the size of the request.</p>

<p><strong>Target</strong></p>

<p>The more you know about the exact workload presented to an <strong>allocator</strong>, the more you could do to tune it to work better for that workload.</p>

<p><strong>Assumptions</strong></p>

<p>Focus on the great history of allocators found in user-level memory-allocation libraries. The space that this library manages is known historically as the heap, and the geeric data structure used to manage free space in the heap is some kind of <strong>free list</strong>. This structure contains references to all of the free chunks of space in the managed region of memory.</p>

<p>Example</p>

<p>void free(void *ptr) takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when freeing the space, does not inform the library of its size; thus, the library must be able to figure out how big a chunk of memory is when handed just a pointer to it.</p>

<p><strong>Splitting and Coalescing</strong></p>

<ul>
  <li>The split is commonly used in allocators when requests are smaller than the size of any particular free chunk.</li>
  <li>Coalesce free space when a chunk of memory is freed.</li>
</ul>

<p><strong>Tracking The Size Of Allocated Regions</strong></p>

<p>To accomplish this task, most allocators store a little bit of extra information in a <strong>header</strong> block which is kept in memory, usually just before the handed-out chunk of memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewenndy.github.io/raw/source/image-repo/os-free_space_management_non_coalesced_free_list.png" alt="os-free_space_management_non_coalesced_free_list.png" /></p>

<h2 id="paging">Paging</h2>

<h3 id="chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</h3>

<p><strong>Background</strong></p>

<p>The operating system takes one of two approaches when solving most any space-management problem.</p>

<ol>
  <li>The first approach is to chop things up into <strong>variable-sized</strong> pieces, as we saw with segmenta- tion in virtual memory.</li>
  <li>To chop up space into <strong>fixed-sized</strong> pieces. In virtual memory, we call this idea paging.</li>
</ol>

<p><strong>Page vs. Page Frame</strong></p>

<ul>
  <li>From perspective of address space, the fixed-sized unit is called page.</li>
  <li>From perspective of physical space, the fixed-sized unit is called page frame.</li>
</ul>

<p>So, the address translation is to translate page to relevant page frame.</p>

<p><strong>32 bits vs. 64 bits</strong></p>

<p>Sometimes we say the OS is 32 bits or 64 bits, we may infer that</p>

<ul>
  <li>32 bits OS has 4GB address space</li>
  <li>64 bits OS has 10mGB address space</li>
</ul>

<p><strong>Advantage</strong></p>

<ul>
  <li>First, it does not lead to external fragmentation, as paging (by design) divides memory into fixed-sized units.</li>
  <li>Second, it is quite flexible, enabling the sparse use of virtual address spaces.</li>
</ul>

<p><strong>Translation</strong></p>

<p>To translate this virtual address that the process generated, we have to first split it into two components: the <strong>virtual page number (VPN)</strong>, and the <strong>offset</strong> within the page.</p>

<p>With our virtual page number, we can now index our page table, to get the <strong>physical frame number (PFN)</strong> (also sometimes called the <strong>physical page number or PPN</strong>).</p>

<p>Note the offset stays the same (i.e., it is not translated), because the offset just tells us which byte within the page we want.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_address_translation_process.png" alt="os-paging_address_translation_process.png" /></p>

<p><strong>Page Table</strong></p>

<p>The operating system usually keeps a per-process data structure known as a page table.</p>

<p>One of the most important data structures in the memory management subsystem of a modern OS is the page table. In general, a page table stores virtual-to-physical address translations</p>

<p>The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical addresses (physical frame numbers). The OS indexes the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_page_table.png" alt="os-paging_page_table.png" /></p>

<p><strong>Storage</strong></p>

<p>Because page tables are so big, we don’t keep any special on-chip hard- ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in memory somewhere.</p>

<p><strong>Page Table Entry (PTE)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_x86_pte_example.png" alt="os-paging_x86_pte_example.png" /></p>

<p><strong>Page Table Base Register (PTBR)</strong></p>

<p>PTBR contains the physical address of the starting location of the page table.</p>

<p>Code Example</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_access_memory_code_demo.png" alt="os-paging_access_memory_code_demo.png" /></p>

<h3 id="chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</h3>

<p><strong>Background</strong></p>

<p>Using paging as the core mechanism to support virtual memory can lead to high performance overheads. By chopping the address space into small, fixed-sized units (i.e., pages), paging requires a large amount of mapping information. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow.</p>

<p><strong>Translation Lookaside Buffer (TLB)</strong></p>

<p>To speed address translation, we are going to add what is called (for historical reasons) a <strong>translation-lookaside buffer</strong>, or <strong>TLB</strong>. A TLB is part of the chip’s <strong>memory-management unit (MMU)</strong>, and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an <strong>address-translation cache</strong>.</p>

<p><strong>Advantage</strong></p>

<p>By providing a small, dedicated on-chip TLB as an address-translation cache, most memory references will hopefully be handled without having to access the page table in main memory.</p>

<p><strong>Algorithm</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow.png" alt="os-paging_tlb_control_flow.png" /></p>

<p>Goal is to improve the TLB <strong>hit rate</strong>.</p>

<p><strong>TLB Content</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_content.png" alt="os-paging_tlb_content.png" /></p>

<p>TLB contains both VPN and PFN in each entry, in hardware terms, the TLB is known as a <strong>fully-associative</strong> cache.</p>

<p><strong>TLB Miss Handling</strong></p>

<p>Two answers are possible: the hardware, or the software (OS).</p>

<p>A modern system that uses <strong>software-managed TLBs</strong>. On a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler. As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow_os_handled.png" alt="os-paging_tlb_control_flow_os_handled.png" /></p>

<p><strong>Performance Matters</strong></p>

<p>Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties. The idea behind hardware caches is to take advantage of <strong>locality</strong> in instruction and data references. Hardware caches, whether for instructions, data, or address translations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory.</p>

<ol>
  <li><strong>spatial locality</strong>, the idea is that if a program accesses memory at address x, it will likely soon access memory near x.</li>
  <li><strong>temporal locality</strong>, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future.</li>
  <li>page size, why don’t we just make bigger caches and keep all of our data in them? Because any large cache by definition is slow, and thus defeats the purpose.</li>
</ol>

<p><strong>Issue 1: Context Switch</strong></p>

<p>Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.</p>

<ol>
  <li><strong>flush</strong> the TLB on context switches, thus emptying it before running the next process. But there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high.</li>
  <li><strong>address space identifier (ASID)</strong>, which you can think of the ASID as a process identifier (PID), to enable sharing of the TLB across context switches.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_with_asid.png" alt="os-paging_tlb_with_asid.png" /></p>

<p><strong>Issue 2: Replacement Policy</strong></p>

<p>When we are installing a new entry in the TLB, we have to replace an old one, which one to replace?</p>

<ul>
  <li><strong>least-recently-used (LRU)</strong></li>
  <li><strong>random policy</strong></li>
</ul>

<p>LRU tries to take advantage of locality in the memory-reference stream, and what the random policy exists for?</p>

<p>Random policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over n + 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better.</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>
    <p>Exceeding the TLB coverage, and it can be quite a problem for certain programs. Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed.</p>

    <p><strong>RAM isn’t always RAM</strong>. Sometimes randomly accessing your address space, particular if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties. Because one of our advisors, David Culler, used to always point to the TLB as the source of many performance problems, we name this law in his honor: <strong>Culler’s Law</strong>.</p>
  </li>
  <li>
    <p>TLB access can easily become a bottleneck in the CPU pipeline, in particular with what is called a <strong>physically-indexed cache</strong>. With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. A <strong>virtually-indexed cach</strong>e solves some performance problems, but introduces new issues into hardware design as well.</p>
  </li>
</ol>

<h3 id="note-on-cache-management">Note on Cache Management</h3>

<p>Define cache miss and hit, and goal is to improve the cache rate. Normally, better <strong>replacement policy</strong> lead to higher cache rate.</p>

<p><strong>Find the best replacement policy</strong></p>

<ul>
  <li>Find the optimal</li>
  <li>Find the easiest</li>
  <li>Improve toward optimal, considering Principle of Locality</li>
  <li>Think about corner case</li>
</ul>

<p><strong>Reference: Optimal Replacement Policy</strong></p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Reference: Easiest Replacement Policy</strong></p>

<p>Random policy, with an extraordinary advantage, can avoid corner case.</p>

<p><strong>Reference: Principle of Locality</strong></p>

<p>Programs tend to access certain code sequences (e.g., in a loop) and data structures (e.g., an array accessed by the loop) quite frequently.</p>

<ul>
  <li>spatial locality</li>
  <li>temporal locality, e.g., LRU</li>
  <li>operation expense, e.g., When swapping out pages, dirty pages are much more expensive</li>
</ul>

<p><strong>Reference: Types of Cache Misses</strong></p>

<p>In the computer architecture world, architects sometimes find it useful to characterize misses by type, into one of three categories, sometimes called the Three C’s.</p>

<ul>
  <li><strong>Compulsory miss</strong> (cold-start miss) occurs because the cache is empty to begin with and this is the first reference to the item.</li>
  <li><strong>Capacity miss</strong> occurs because the cache ran out of space and had to evict an item to bring a new item into the cache.</li>
  <li><strong>Conflict miss</strong> arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as set-associativity; it does not arise in the OS page cache because such caches are always fully-associative, i.e., there are no restrictions on where in memory a page can be placed.</li>
</ul>

<h3 id="chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</h3>

<p><strong>Crux</strong></p>

<p>How to get rid of all those invalid regions in the page table instead of keeping them all in memory?</p>

<p><strong>Background</strong></p>

<p>Page tables are t big and thus consume too much memory.</p>

<p>Assume again a 32-bit address space (2^32 bytes), with 4KB (2^12 byte) pages and a 4-byte page-table entry. An address space thus has roughly one million virtual pages in it ( 2^20 ); multiply by the page-table entry size and you see that our page table is 4MB in size. Recall also: we usually have one page table for every process in the system! With a hundred active processes (not uncommon on a modern system), we will be allocating hundreds of megabytes of memory just for page tables!</p>

<p><strong>Solution 1 - Bigger Pages</strong></p>

<p>Big pages lead to waste within each page, a problem known as internal fragmentation. Thus, most systems use relatively small page sizes in the common case: 4KB (as in x86).</p>

<p><strong>Solution 2 - Hybrid Approach: Paging and Segments</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_hybrid_approach.png" alt="os-paging_tlb_hybrid_approach.png" /></p>

<p><strong>Algorithm</strong></p>

<p>Instead of having a single page table for the entire address soopace of the process, have one per logical segment. In this example, we might thus have three page tables.</p>

<p>Remember with segmentation, we had a <strong>base</strong> register that told us where each segment lived in physical memory, and a <strong>bound</strong> or limit register that told us the size of said segment.</p>

<ol>
  <li>Each logical segment (code, stack, and heap) has one page table.</li>
  <li>Each segment has one pair of base and bounds resisters.</li>
  <li>Base register points to the page table of the segment, and bounds is used to indicate the end of the page table.</li>
</ol>

<p><strong>Advantage</strong></p>

<p>In this manner, our hybrid approach realizes a significant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>It still requires us to use segmentation, as it assumes a certain usage pattern of the address space; if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste.</li>
  <li>This hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, finding free space for them in memory is more complicated.</li>
</ol>

<p><strong>Solution 3 - Multi-level Page Tables</strong></p>

<p>It turns the linear page table into something like a tree (<strong>page directory</strong>). This approach is so effective that many modern systems employ it (e.g., x86).</p>

<p><strong>Algorithm</strong></p>

<p>First, chop up the page table into page-sized units; if an entire page of page-table entries (PTEs) is invalid, don’t allocate that page of the page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory. The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.</p>

<p>The page directory, in a simple two-level table, contains one entry per page of the page table. It consists of a number of <strong>page directory entries (PDE)</strong>. A PDE (minimally) has a <strong>valid bit</strong> <strong>and a page frame number (PFN)</strong>, similar to a PTE.</p>

<p>VA contains VPN and offset, and VPN can be splitted into <strong>page directory index</strong> and <strong>page table index</strong>.</p>

<ol>
  <li>Use <strong>page directory index</strong> to search page directory, to get <strong>page directory entry</strong>, to get <strong>page frame number</strong>, to get the specific <strong>page table</strong>.</li>
  <li>Use <strong>page table index</strong> to search the page table, to get <strong>page table entry</strong>, to get the real <strong>physical frame number</strong>.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo.png" alt="os-paging_multi_level_page_table_demo.png" /></p>

<p>Demo code</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo_code.png" alt="os-paging_multi_level_page_table_demo_code.png" /></p>

<p><strong>Advantage</strong></p>

<ol>
  <li>The multi-level table only allocates page-table space in proportion to the amount of address space you are usig; thus it is generally compact and supports sparse address spaces.</li>
  <li>
    <p>If carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page table.</p>

    <p>Contrast this to a simple (non-paged) linear page table, for a large page table (say 4MB), finding such a large chunk of unused contiguous free physical memory can be quite a challenge. With a multi-level structure, the indirection allows us to place page-table pages wherever we would like in physical memory.</p>
  </li>
</ol>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>Time-space trade-off. It should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself).</li>
  <li>Another obvious negative is complexity. Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubt- nedly more involved than a simple linear page-table lookup.</li>
</ol>

<p><strong>Example</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example.png" alt="os-paging_multi_level_page_table_example.png" /></p>

<p>Virtual Address format</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_va.png" alt="os-paging_multi_level_page_table_example_va.png" /></p>

<p>Explanation</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_explanation.png" alt="os-paging_multi_level_page_table_example_explanation.png" /></p>

<p><strong>Issues</strong></p>

<p><strong><em>What if the page directory gets too big?</em></strong></p>

<p>Make it more than two levels, add index to page directory index.</p>

<p><strong><em>How to make it extreme space savings?</em></strong></p>

<p>Inverted page tables. Instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each physical page of the system. The entry tells us which process is using this page, and which virtual page of that process maps to this physical page.</p>

<p>A hash table is often built over the base structure to speed lookups.</p>

<p><strong><em>How to choose page table size?</em></strong></p>

<p>In a memory-constrained system (like many older systems), small structures make sense; in a system with a reasonable amount of memory and with workloads that actively use a large number of pages, a bigger table that speeds up TLB misses might be the right choice.</p>

<p><strong><em>What if the page tables are too big to fit into memory all at once?</em></strong></p>

<p>Thus far, we have assumed that page tables reside in kernel-owned physical memory. Some systems place such page tables in <strong>kernel virtual memory</strong>, thereby allowing the system to swap some of these page tables to disk when memory pressure gets a little tight.</p>

<h2 id="beyond-physical-memory">Beyond Physical Memory</h2>

<h3 id="chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</h3>

<p><strong>Background</strong></p>

<p>In fact, we’ve been assuming that every address space of every running process fits into memory. We will now relax these big assumptions, and assume that we wish to support many concurrently-running large address spaces.</p>

<p>To support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren’t in great demand. In modern systems, this role is usually served by a hard disk drive.</p>

<p><strong>Mechanism</strong></p>

<p>To do so requires more complexity in page-table structures, as a <strong>present bit</strong> (of some kind) must be included to tell us whether the page is present in memory or not. When not, the operating system <strong>page-fault handler</strong> runs to service the <strong>page fault</strong>, and thus arranges for the transfer of the desired page from disk to memory, perhaps first replacing some pages in memory to make room for those soon to be swapped in.</p>

<p><strong>Swap Space</strong></p>

<p>To reserve some space on the disk for moving pages back and forth. We will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to remember the <strong>disk address</strong> of a given page (PTE).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_example.png" alt="os-swap_example.png" /></p>

<p>The size of the swap space is important, as ultimately it determines the <strong>maximum number of memory pages</strong> that can be in use by a system at a given time.</p>

<p>We should note that swap space is not the only on-disk location for swapping traffic.</p>

<blockquote>
  <p>For example, assume you are running a program binary (e.g., ls, or your own compiled main program). The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution, or, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re-use the memry space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system.</p>
</blockquote>

<p><strong>Present Bit</strong></p>

<p>OS use this piece of information in each page-table entry to flag if the page is in physical memory or swap space.</p>

<p>If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above; if it is set to zero, the page is not in memory but rather on disk somewhere.</p>

<p><strong>Page Faut</strong></p>

<p>The act of accessing a page that is not in physical memory is commonly referred to as a <strong>page fault</strong> (it should be called a <strong>page miss</strong>. But when something the hardware doesn’t know how to handle occurs, the hardware simply transfers control to the OS. In perspective of the hardware it is a page fault).</p>

<p><strong>Page Fault Handler</strong></p>

<p>Upon a page fault, the OS is invoked to service the page fault. A particular piece of code, known as a <strong>page-fault handler</strong>, runs, and must service the page fault.</p>

<p>The appropriately-named <strong>OS page-fault handler</strong> runso to determine what to do. Virtually all systems handle page faults in software; even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.</p>

<p><strong>Page Fault Control Flow</strong></p>

<p>Hardware</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow.png" alt="os-swap_page_fault_control_flow.png" /></p>

<p>Software</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow_software.png" alt="os-swap_page_fault_control_flow_software.png" /></p>

<p>How to handle or how will the OS know where to find the desired page?</p>

<ol>
  <li>The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.</li>
  <li>When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN field of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction.</li>
  <li>Then generate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB when servicing the page fault to avoid this step)</li>
  <li>Finally, a last restart would find the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address.</li>
</ol>

<p>Note that while the I/O is in flight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes while the page fault is being serviced.</p>

<p><strong><em>What If Memory Is Full?</em></strong></p>

<p>OS might like to first page out one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or replace is known as the <strong>page-replacement policy</strong>.</p>

<p><strong><em>When Replacements Really Occur?</em></strong></p>

<p>There are many reasons for the OS to keep a small portion of memory free more proactively. To keep a small amount of memory free, most operating systems thus have some kind of <strong>high watermark (HW)</strong> and <strong>low watermark (LW)</strong> to help decide when to start evicting pages from memory.</p>

<p>When the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The background thread, sometimes called the <strong>swap daemon</strong> or <strong>page daemon</strong>, then goes to sleep, happy that it has freed some memory for running processes and the OS to use.</p>

<p>So, instead of performing a replacement directly, the algorithm would instead simply check if there are any free pages available. If not, it would inform the <strong>page daemon</strong> that free pages are needed; when the thread frees up some pages, it would re-awaken the original thread, which could then page in the desired page and go about its work.</p>

<p><strong><em>How To Make Replacement Efficient?</em></strong></p>

<p>Many systems will cluster or group a number of pages and write them out at once to the swap partition, thus increasing the efficiency of the disk.</p>

<h3 id="chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</h3>

<p><strong>Background</strong></p>

<p>In such a case, this memory pressure forces the OS to start <strong>paging out</strong> pages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the <strong>replacement policy</strong> of the OS.</p>

<p><strong>Cache Management</strong></p>

<p>Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. And our goal as maximizing the number of <strong>cache hits</strong>.</p>

<p>Knowing the number of cache hits and misses let us calculate the <strong>average memory access time (AMAT)</strong> for a program.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_amat.png" alt="os-replacement_amat.png" /></p>

<p>Example</p>

<p>Suppose T(M) = 100ns (10^-7), T(D) = 10ms (10^-2)</p>

<ul>
  <li>P(Hit) = 90%, P(Miss) = 10%, AMAT = 1ms + 90ns</li>
  <li>P(Hit) = 99.9%, P(Miss) = 0.1%, AMAT = 0.01ms + 99.9ns</li>
</ul>

<p>The cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs.</p>

<p><strong>Polices</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_summary.png" alt="os-replacement_summary.png" /></p>

<p><strong>Policy 1. Optimal Replacement Policy</strong></p>

<p>Replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses.</p>

<p>In the development of scheduling policies, the future is not generally known; you can’t build the optimal policy for a general-purpose operating system.</p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Policy 2. FIFO</strong></p>

<p>Normal efficiency, easy to implement, and has corner case.</p>

<p>In some cases, when increasing the cache size, hit rate may get lower. This odd behavior is generally referred to as <strong>Belady’s Anomaly</strong>.</p>

<p><strong>Policy 3. Random</strong></p>

<p>Normal efficiency, easy to implement, but remember, it can avoid corner case.</p>

<p><strong>Policy 4. LRU</strong></p>

<p>LRU has what is known as a stack property. When increasing the cache size, hit rate will either stay the same or improve.</p>

<p><strong>Comparison with Workload</strong></p>

<p>No locality workload</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_no_locality_workload.png" alt="os-replacement_no_locality_workload.png" /></p>

<p>The 80-20 Workload, 80% of the references are made to 20% of the pages (the “hot” pages).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload.png" alt="os-replacement_80_20_workload.png" /></p>

<p>The Looping-Sequential Workload</p>

<p>Looping sequential workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, …, up to page 49, and then we lp, repeating those accesses.</p>

<p>It represents a worst-case for both LRU and FIFO, but no influence on Random. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_looping_sequential_workload.png" alt="os-replacement_looping_sequential_workload.png" /></p>

<p><strong>Implementation - Approximating LRU</strong></p>

<p>To keep track of which pages have been least- and most-recently used, the system has to do some accounting work on every memory reference. Unfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive.</p>

<p>Idea</p>

<p>Approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a <strong>use bit</strong> (sometimes called the <strong>reference bit</strong>).</p>

<ul>
  <li>Whenever a page is referenced (i.ooe., read or written), the use bit is set by hardware to 1.</li>
  <li>The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS.</li>
</ul>

<p>Implementation by Clock Algorithm</p>

<ul>
  <li>Imagine all the pages of the system arranged in a circular list. A clock hand points to some particular page to begin with.</li>
  <li>When a replacement must occur, the OS iterating the circular list checking on use bit.
    <ul>
      <li>If 1, clear use bit to 0, and find next</li>
      <li>If 0, use it</li>
    </ul>
  </li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload_with_clock.png" alt="os-replacement_80_20_workload_with_clock.png" /></p>

<p><strong>Considering Dirty Pages</strong></p>

<p>Consider the locality by the expense on swapping out pages.</p>

<ul>
  <li>If a page has been <strong>modified</strong> and is thus <strong>dirty</strong>, it must be written back to disk to evict it, which is expensive.</li>
  <li>If it has not been modified (and is thus clean), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O.
Idea</li>
</ul>

<p>To support this behavior, the hardware should include a <strong>modified bit</strong> (a.k.a. <strong>dirty bit</strong>).</p>

<p>Implementation by Clock Algorithm</p>

<p>The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; failing to find those, then for unused pages that are dirty, and so forth.</p>

<p><strong>Other VM Policies</strong></p>

<p><strong><em>When the OS bring a page into memory?</em></strong></p>

<p>Page selection policy. The OS simply uses <strong>demand paging</strong>, which means the OS brings the page into memory when it is accessed, “on demand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as <strong>prefetching</strong>.</p>

<p><strong><em>How the OS writes pages out to disk?</em></strong></p>

<p>Any systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write. This behavior is usually called <strong>clustering</strong> or simply <strong>grouping</strong> of writes, and is effective because of the nature of disk drives.</p>

<p><strong><em>What about
 the memory demands of the set of running processes simply exceeds the available physical memory? (condition sometimes referred to as thrashing)</em></strong></p>

<p>Given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes working sets (the pages that they are using actively) fit in memory and thus can make progress. This approach, generally known as <strong>admission control</strong>, states that it is sometimes better to do less work well than to try to do everything at once poorly.</p>

<p>Some versions of Linux run an <strong>out-of-memory killer</strong> when memory is oversubscribed; this daemon chooses a memory- intensive process and kills it, thus reducing memory in a none-too-subtle manner.</p>

<h3 id="chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</h3>

<p><strong>Background</strong></p>

<p>The VAX-11 minicomputer architecture was introduced in the late 1970’s by Digital Equipment Corporation (DEC).</p>

<p>As an additional issue, VMS is an excellent example of software innovations used to hide some of the inheret flaws of the architecture.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-vax_vms_address_space.png" alt="os-vax_vms_address_space.png" /></p>

<p><strong>Reduce Page Table Pressure</strong></p>

<p>First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process; thus, no page-table space is needed for the unused portion of the address space between the stack and the heap.</p>

<p>Second, the OS reduces memory pressure even further by placing user page tables (for P0 and P1, thus two per process) in kernel virtual memory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S. If memory comes undersevere pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.</p>

<p><strong>Replacement policy: Segmented FIFO with Page Clustering</strong></p>

<p>Each process has a maximum number of pages it can keep in memory, known as its <strong>residentn set size (RSS)</strong>. Each of these pages is kept on a FIFO list; when a process exceeds its RSS, the “first-in” page is evicted. FIFO clearly does not need any support from the hardware (no use bit), and is thus easy to implement.</p>

<p>To improve FIFO’s performance, VMS introduced two <strong>second-chance lists</strong> where pages are placed before getting evicted from memory, specifically a global clean-page free list and dirty-page list. The bigger these global second-chance lists are, the closer the segmented FIFO algorithm performs to LRU.</p>

<p>Clustering is used in most modern systems, as the freedom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve performance.</p>

<p><strong>Optimisation: Be Lazy</strong></p>

<p>Laziness can put off work until later, which is beneficial within an OS for a number of reasons.</p>

<ul>
  <li>First, putting off work might reduce the latency of the current operation, thus improving responsiveness; for example, operating systems often report that writes to a file succeeded immediately, and only write them to disk later in the background.</li>
  <li>Second, and more importantly, laziness sometimes obviates the need to do the work at all; for example, delaying a write until the file is deleted removes the need to do the write at all.</li>
</ul>

<p><strong>Lazy Optimisation: Demanding Zero</strong></p>

<p>With demand zeroing, the OS instead does very little work when the page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or writes the page, a trap into the OS takes place. When handling the trap, the OS notices that this is actually a demand-zero page; at this point, the OS then does the needed work of finding a physical page, zeroing it, and mapping it into the process’s address space. If the process never accesses the page, all of this work is avoided, and thus the virtue of demand zeroing.</p>

<p><strong>Lazy Optimisation: Copy-on-write</strong></p>

<p>When the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces.</p>

<ul>
  <li>If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.</li>
  <li>If, however, one of the address spaces does indeed try to write to the page, it will trap into the OS. The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process. The process then continues and now has its own private copy of the page.</li>
</ul>

<p>In UNIX systems, COW is even more critical, due to the semantics of <code>fork()</code> and <code>exec()</code>. <code>fork()</code> creates an exact copy of the address space of the caller; with a large address space, making such a copy is slow and data intensive. Even worse, most of the address space is immediately over-written by a subsequent call to <code>exec()</code>, which overlays the calling process’s address space with that of the soon-to-be-exec’d program. By instead performing a copy-on-write <code>fork()</code>, the OS avoids much of the needless copying and thus retains the correct semantics while improving performance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Guidance from POODR]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/01/29/guidance-from-poodr/"/>
    <updated>2015-01-29T15:20:13+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/01/29/guidance-from-poodr</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Practical Object Oriented Design in Ruby</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td>Sandi Metz</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://www.poodr.com/">www.poodr.com</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#object-oriented-design" id="markdown-toc-object-oriented-design">Object-Oriented Design</a>    <ul>
      <li><a href="#the-tools-of-design" id="markdown-toc-the-tools-of-design">The Tools of Design</a>        <ul>
          <li><a href="#design-principles" id="markdown-toc-design-principles">Design Principles</a></li>
          <li><a href="#design-patterns" id="markdown-toc-design-patterns">Design Patterns</a></li>
        </ul>
      </li>
      <li><a href="#the-act-of-design" id="markdown-toc-the-act-of-design">The Act of Design</a>        <ul>
          <li><a href="#how-design-fails" id="markdown-toc-how-design-fails">How Design Fails</a></li>
          <li><a href="#when-to-design" id="markdown-toc-when-to-design">When to Design</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#guidance" id="markdown-toc-guidance">Guidance</a>    <ul>
      <li><a href="#designing-classes-with-a-single-responsibility" id="markdown-toc-designing-classes-with-a-single-responsibility">Designing Classes with a Single Responsibility</a>        <ul>
          <li><a href="#depend-on-behavior-not-data" id="markdown-toc-depend-on-behavior-not-data">Depend on Behavior, Not Data</a></li>
          <li><a href="#enforce-single-responsibility-everywhere" id="markdown-toc-enforce-single-responsibility-everywhere">Enforce Single Responsibility Everywhere</a></li>
        </ul>
      </li>
      <li><a href="#manage-dependencies" id="markdown-toc-manage-dependencies">Manage Dependencies</a>        <ul>
          <li><a href="#inject-dependencies" id="markdown-toc-inject-dependencies">Inject Dependencies</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#after" id="markdown-toc-after">after</a>    <ul>
      <li><a href="#isolate-dependencies" id="markdown-toc-isolate-dependencies">Isolate Dependencies</a></li>
      <li><a href="#remove-argument-order-dependencies" id="markdown-toc-remove-argument-order-dependencies">Remove Argument-Order Dependencies</a></li>
      <li><a href="#managing-dependency-direction" id="markdown-toc-managing-dependency-direction">Managing Dependency Direction</a></li>
      <li><a href="#creating-flexible-interfaces" id="markdown-toc-creating-flexible-interfaces">Creating Flexible Interfaces</a>        <ul>
          <li><a href="#finding-the-public-interface" id="markdown-toc-finding-the-public-interface">Finding the Public Interface</a>            <ul>
              <li><a href="#focus-messages-between-domain-objects" id="markdown-toc-focus-messages-between-domain-objects">Focus Messages between Domain Objects</a></li>
              <li><a href="#use-sequence-diagrams" id="markdown-toc-use-sequence-diagrams">Use Sequence Diagrams</a></li>
              <li><a href="#asking-for-what-instead-of-telling-how" id="markdown-toc-asking-for-what-instead-of-telling-how">Asking for “What” Instead of Telling “How”</a></li>
              <li><a href="#seeking-context-independence" id="markdown-toc-seeking-context-independence">Seeking Context Independence</a></li>
            </ul>
          </li>
          <li><a href="#the-law-of-demeter" id="markdown-toc-the-law-of-demeter">The Law of Demeter</a></li>
        </ul>
      </li>
      <li><a href="#reduction-costs-with-duck-typing" id="markdown-toc-reduction-costs-with-duck-typing">Reduction Costs with Duck Typing</a>        <ul>
          <li><a href="#polymorphism" id="markdown-toc-polymorphism">Polymorphism</a></li>
          <li><a href="#recognizing-hidden-ducks" id="markdown-toc-recognizing-hidden-ducks">Recognizing Hidden Ducks</a></li>
          <li><a href="#guidance-1" id="markdown-toc-guidance-1">Guidance</a></li>
        </ul>
      </li>
      <li><a href="#acquiring-behavior-through-inheritance" id="markdown-toc-acquiring-behavior-through-inheritance">Acquiring Behavior Through Inheritance</a>        <ul>
          <li><a href="#inheritance" id="markdown-toc-inheritance">Inheritance</a></li>
          <li><a href="#recognizing-where-to-use-inheritance" id="markdown-toc-recognizing-where-to-use-inheritance">Recognizing Where to Use Inheritance</a>            <ul>
              <li><a href="#finding-the-abstraction" id="markdown-toc-finding-the-abstraction">Finding the Abstraction</a></li>
            </ul>
          </li>
          <li><a href="#using-templage-methods" id="markdown-toc-using-templage-methods">Using Templage Methods</a>            <ul>
              <li><a href="#template-method" id="markdown-toc-template-method">Template Method</a></li>
              <li><a href="#implementing-every-template-method" id="markdown-toc-implementing-every-template-method">Implementing Every Template Method</a></li>
            </ul>
          </li>
          <li><a href="#manging-coupling" id="markdown-toc-manging-coupling">Manging Coupling</a>            <ul>
              <li><a href="#decoupling-subclasses-using-hook-messages" id="markdown-toc-decoupling-subclasses-using-hook-messages">Decoupling Subclasses Using Hook Messages</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#sharing-role-behavior-with-modules" id="markdown-toc-sharing-role-behavior-with-modules">Sharing Role Behavior with Modules</a>        <ul>
          <li><a href="#understanding-roles" id="markdown-toc-understanding-roles">Understanding Roles</a></li>
          <li><a href="#writing-inheritable-code" id="markdown-toc-writing-inheritable-code">Writing Inheritable Code</a>            <ul>
              <li><a href="#recognize-the-antipatterns" id="markdown-toc-recognize-the-antipatterns">Recognize the Antipatterns</a></li>
              <li><a href="#insist-on-the-abstraction" id="markdown-toc-insist-on-the-abstraction">Insist on the Abstraction</a></li>
              <li><a href="#honor-the-contract" id="markdown-toc-honor-the-contract">Honor the Contract</a></li>
              <li><a href="#use-the-template-method-pattern" id="markdown-toc-use-the-template-method-pattern">Use the Template Method Pattern</a></li>
              <li><a href="#preemptively-decouple-classes" id="markdown-toc-preemptively-decouple-classes">Preemptively Decouple Classes</a></li>
              <li><a href="#create-shallow-hierarchies" id="markdown-toc-create-shallow-hierarchies">Create Shallow Hierarchies</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#combining-objects-with-composition" id="markdown-toc-combining-objects-with-composition">Combining Objects with Composition</a>        <ul>
          <li><a href="#aggregation-a-special-kind-of-composition" id="markdown-toc-aggregation-a-special-kind-of-composition">Aggregation: A Special Kind of Composition</a></li>
          <li><a href="#deciding-between-inheritance-and-composition" id="markdown-toc-deciding-between-inheritance-and-composition">Deciding Between Inheritance and Composition</a>            <ul>
              <li><a href="#inheritance-1" id="markdown-toc-inheritance-1">Inheritance</a></li>
              <li><a href="#composition" id="markdown-toc-composition">Composition</a></li>
            </ul>
          </li>
          <li><a href="#guidance-2" id="markdown-toc-guidance-2">Guidance</a></li>
        </ul>
      </li>
      <li><a href="#designing-cost-effective-tests" id="markdown-toc-designing-cost-effective-tests">Designing Cost-Effective Tests</a>        <ul>
          <li><a href="#intentional-testing" id="markdown-toc-intentional-testing">Intentional Testing</a>            <ul>
              <li><a href="#knowing-your-intentions" id="markdown-toc-knowing-your-intentions">Knowing Your Intentions</a></li>
              <li><a href="#knowing-what-to-test" id="markdown-toc-knowing-what-to-test">Knowing What to Test</a>                <ul>
                  <li><a href="#remove-the-duplicate" id="markdown-toc-remove-the-duplicate">Remove the Duplicate</a></li>
                  <li><a href="#message-model" id="markdown-toc-message-model">Message Model</a></li>
                </ul>
              </li>
              <li><a href="#knowing-when-to-test" id="markdown-toc-knowing-when-to-test">Knowing When to Test</a></li>
              <li><a href="#knowing-how-to-test" id="markdown-toc-knowing-how-to-test">Knowing How to Test</a></li>
            </ul>
          </li>
          <li><a href="#testing-incoming-messages" id="markdown-toc-testing-incoming-messages">Testing Incoming Messages</a></li>
          <li><a href="#testing-private-methods" id="markdown-toc-testing-private-methods">Testing Private Methods</a></li>
          <li><a href="#testing-outgoing-messages" id="markdown-toc-testing-outgoing-messages">Testing Outgoing Messages</a></li>
          <li><a href="#testing-duck-types" id="markdown-toc-testing-duck-types">Testing Duck Types</a></li>
          <li><a href="#testing-inherited-code" id="markdown-toc-testing-inherited-code">Testing Inherited Code</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="object-oriented-design">Object-Oriented Design</h1>

<p>Object-oriented design (OOD) requires that you shift from thinking of the world as a collection of predefined procedures to modeling the world as a series of messages that pass between objects.</p>

<p>Object-oriented applications are made up of parts that interact to produce the behavior of the whole. The parts are <em>objects</em>; interactions are embodied in the <em>messages</em> that pass between them.</p>

<p>Object-oriented design is about managing dependencies. In the absence of design, unmanaged dependencies wreak havoc because objects know too much about one another.</p>

<p>Design is thus an art, the art of arranging code, and design is more the art of preserving changeability than it is the act of achieving perfection. You must not only write code for the feature you plan to deliver today, you must also create code that is amenable to being changed later. It doesn’t guess the future; it preserves your options for accommodating the future. It doesn’t choose; it leaves you room to move.</p>

<p>The trick to getting the most bang for your design buck is to acquire an understanding of the theories of design and to apply these theories appropriately, at the right time, and in the right amounts.</p>

<p>Well-designed applications are constructed of reusable code. Small, trustworthy self-contained objects with minimal context, clear interfaces, and injected dependencies are inherently reusable.</p>

<h2 id="the-tools-of-design">The Tools of Design</h2>

<h3 id="design-principles">Design Principles</h3>

<ul>
  <li>
    <p><strong>SOLID</strong></p>

    <ul>
      <li>Single Responsibility</li>
      <li>Open-Closed</li>
      <li>Liskov Substitution</li>
      <li>Interface Segregation</li>
      <li>Dependency Inversion</li>
    </ul>
  </li>
  <li><strong>DRY</strong>, Don’t Repeat Yourself</li>
  <li><strong>LoD</strong>, Law of Demeter</li>
</ul>

<h3 id="design-patterns">Design Patterns</h3>

<p>by Gof</p>

<h2 id="the-act-of-design">The Act of Design</h2>

<h3 id="how-design-fails">How Design Fails</h3>

<ul>
  <li>Lack of it. Successful but undesigned applications carry the seeds of their own destruction; they are easy to write but gradually become impossible to change. “Yes, I can add that feature, but it will break everything.”</li>
  <li>Overdesign. Aware of OO design techniques but do not yet understand how to apply them. “No, I can’t add that feature; it wasn’t designed to do that.”</li>
  <li>Seperated from the act of programming. Design is a process of progressive discovery that relies on a feedback loop. The iterative techniques of the Agile software movement are thus perfectly suited to the creation of well-designed OO applications. The iterative nature of Agile development allows design to adjust regularly and to evolve naturally.</li>
</ul>

<h3 id="when-to-design">When to Design</h3>

<blockquote>
  <p>Agile believes that your customers can’t define the software they want before seeing it, so it’s best to show them sooner rather than later. If this premise is true, then it logically follows that you should build software in tiny increments, gradually iterating your way into an application that meets the customer’s true need. The Agile experience is that this collaboration produces software that differs from what was initially imagined; the resulting software could not have been anticipated by any other means.</p>
</blockquote>

<p>If Agile is correct, then</p>

<ol>
  <li>there is absolutely no point in doing a Big Up Front Design (BUFD) (because it cannot possibly be correct)</li>
  <li>no one can predict when the application will be done (because you don’t know in advance what it will eventually do)</li>
</ol>

<p>Agile processes guarantee change and your ability to make these changes depends on your application’s design. If you cannot write well-designed code you’ll have to rewrite your application during every iteration.</p>

<h1 id="guidance">Guidance</h1>

<p>Focus on object,</p>

<ul>
  <li>Single <strong>Responsibility</strong></li>
  <li>Manage <strong>Dependencies</strong></li>
</ul>

<p>Focus on message,</p>

<ul>
  <li><strong>Interface</strong>, creating flexible interfaces</li>
  <li><strong>Duck Typing</strong>, reducing costs with Duck Typing</li>
  <li><strong>Inheritance</strong>, acquiring behavior through inheritance</li>
  <li><strong>Module</strong>, sharing <strong>role</strong> behavior with modules</li>
  <li><strong>Composition</strong>, combining objects with composition</li>
  <li><strong>Tests</strong>, designing cost-effective tests</li>
</ul>

<h2 id="designing-classes-with-a-single-responsibility">Designing Classes with a Single Responsibility</h2>

<p>SRP requires that a class be <strong>cohesive</strong>, that everything in a class is related to its central purpose, the class is said to be highly cohesive or to have a single responsibility.</p>

<h3 id="depend-on-behavior-not-data">Depend on Behavior, Not Data</h3>

<p>“Don’t Repeat Yourself” (DRY) is a shortcut for this idea.</p>

<ul>
  <li>Hide instance variables</li>
  <li>Hide data structures</li>
</ul>

<h3 id="enforce-single-responsibility-everywhere">Enforce Single Responsibility Everywhere</h3>

<ul>
  <li>
    <p>Extract extra responsibilities from methods</p>

    <p>Methods, like classes, should have a single responsibility. All of the same reasons apply; having just one responsibility makes them easy to change and easy to reuse.</p>
  </li>
  <li>
    <p>Isolate extra responsibilities in classes</p>

    <p>Postponing decisions until you are absolutely forced to make them. Any decision you make in advance of an explicit requirement is just a guess. Don’t decide; preserve your ability to make a decision later.</p>
  </li>
</ul>

<h2 id="manage-dependencies">Manage Dependencies</h2>

<p>To collaborate, an object must know something know about others. <em>Knowing</em> creates a dependency, or <em>coupling</em> creates a dependency.</p>

<p>Dependency management is core to creating future-proof applications.</p>

<p>An object has a dependency when it knows</p>

<ul>
  <li>The name of another class.</li>
  <li>The name of a message that it intends to send to someone other than self.</li>
  <li>The arguments that a message requires.</li>
  <li>The order of those arguments.</li>
  <li>Knowing the name of a message you plan to send to someone other than self.</li>
  <li>Tests on code.</li>
</ul>

<h3 id="inject-dependencies">Inject Dependencies</h3>

<p>```ruby
# before
class Gear
  …</p>

<p>def gear_inches
    ratio * Wheel.new(rim, tire).diameter
  end
end</p>

<h1 id="after">after</h1>
<p>class Gear
  attr_reader :chainring, :cog, :wheel
  def initialize(chainring, cog, wheel)
    @chainring = chainring
    @cog       = cog
    @wheel     = wheel
  end</p>

<p>def gear_inches
    ratio * wheel.diameter
  end
end
```</p>

<p>Gear previously had explicit dependencies on the Wheel class and on the type and order of its initialization arguments, but through injection these dependencies have been reduced to a single dependency on the diameter method.</p>

<h3 id="isolate-dependencies">Isolate Dependencies</h3>

<p><strong>Isolate Instance Creation</strong></p>

<p>If you are so constrained that you cannot change the code to inject a Wheel into a Gear, you should isolate the creation of a new Wheel inside the Gear class.</p>

<p>```ruby
class Gear
  …</p>

<p>def gear_inches
    ratio * wheel.diameter
  end</p>

<p>def wheel
    @wheel ||= Wheel.new(rim, tire)
  end
```</p>

<ul>
  <li>Isolate Vulnerable External Messages</li>
</ul>

<p>External messages, that is, messages that are “sent to someone other than self.”</p>

<p>```ruby
class Gear
  …</p>

<p>def gear_inches
    ratio * diameter
  end</p>

<p>def diameter
    wheel.diameter
  end
```</p>

<h3 id="remove-argument-order-dependencies">Remove Argument-Order Dependencies</h3>

<ul>
  <li>Use Hashes for Initialization Arguments</li>
  <li>Explicitly Define Defaults</li>
  <li>Isolate Multiparameter Initialization, use a wrapper.</li>
</ul>

<h3 id="managing-dependency-direction">Managing Dependency Direction</h3>

<p>Depend on things that change less often than you do.</p>

<ul>
  <li>Some classes are more likely than others to have changes in requirements.</li>
  <li>Concrete classes are more likely to change than abstract classes.</li>
  <li>Changing a class that has many dependents will result in widespread consequences.</li>
</ul>

<p>Depend on abstractions.</p>

<h2 id="creating-flexible-interfaces">Creating Flexible Interfaces</h2>

<blockquote>
  <p>Interface within a class, make up its public interface.</p>
</blockquote>

<p>Public Interfaces</p>

<ul>
  <li>Reveal its primary responsibility</li>
  <li>Are expected to be invoked by others</li>
  <li>Will not change on a whim</li>
  <li>Are safe for others to depend on</li>
  <li>Are thoroughly documented in the tests</li>
</ul>

<p>Private Interfaces</p>

<ul>
  <li>Handle implementation details</li>
  <li>Are not expected to be sent by other objects</li>
  <li>Can change for any reason whatsoever</li>
  <li>Are unsafe for others to depend on</li>
  <li>May not even be referenced in the tests</li>
</ul>

<p>Well-defined public interfaces consist of stable methods that expose the responsibilities of their underlying classes (public methods should read like a description of responsibilities).</p>

<h3 id="finding-the-public-interface">Finding the Public Interface</h3>

<h4 id="focus-messages-between-domain-objects">Focus Messages between Domain Objects</h4>

<p>Nouns in the application that have both data and behavior are called domain objects. Domain objects are easy to find but they are not at the design center of your application. Design experts notice domain objects without concentrating on them; they focus not on these objects but on the messages that pass between them.</p>

<h4 id="use-sequence-diagrams">Use Sequence Diagrams</h4>

<p>They explicitly specify the messages that pass between objects, and because objects should only communicate using public interfaces, sequence diagrams are a vehicle for exposing, experimenting with, and ultimately defining those interfaces.</p>

<h4 id="asking-for-what-instead-of-telling-how">Asking for “What” Instead of Telling “How”</h4>

<h4 id="seeking-context-independence">Seeking Context Independence</h4>

<p>The best possible situation is for an object to be completely independent of its context. An object that could collaborate with others without knowing who they are or what they do could be reused in novel and unanticipated ways.</p>

<p>The technique for collaborating with others without knowing who they are—dependency injection.</p>

<h3 id="the-law-of-demeter">The Law of Demeter</h3>

<p>It prohibits routing a message to a third object via a second object of a different type. “Only talk to your immediate neighbors” or “use only one dot.”</p>

<p>Delegation is tempting as a solution to the Demeter problem because it removes the visible evidence of violations.</p>

<p>Listening to Demeter means paying attention to your point of view. If you shift to a message-based perspective, the messages you find will become public interfaces in the objects they lead you to discover. However, if you are bound by the shackles of existing domain objects, you’ll end up assembling their existing public interfaces into long message chains and thus will miss the opportunity to find and construct flexible public interfaces.</p>

<h2 id="reduction-costs-with-duck-typing">Reduction Costs with Duck Typing</h2>

<blockquote>
  <p>Interface, across classes and is independent of any single class. The interface represents a set of messages where the messages themselves define the interface. It’s almost as if the interface defines a virtual class; that is, any class that implements the required methods can act like the interface kind of thing.</p>
</blockquote>

<p><strong>Duck types</strong> are public interfaces that are not tied to any specific class. These across-class interfaces add enormous flexibility to your application by replacing costly dependencies on class with more forgiving dependencies on messages.</p>

<h3 id="polymorphism">Polymorphism</h3>

<p><strong>Polymorphism</strong> in OOP refers to the ability of many different objects to respond to the same message. Senders of the message need not care about the class of the receiver; receivers supply their own specific version of the behavior. Polymorphic methods honor an implicit bargain; they agree to be inter-changeable from the sender’s point of view.</p>

<p>A single message thus has many (poly) forms (morphs).</p>

<p>There are a number of ways to achieve polymorphism:</p>

<ul>
  <li>Duck Typing</li>
  <li>Inheritance</li>
  <li>Behavior Sharing (module)</li>
</ul>

<h3 id="recognizing-hidden-ducks">Recognizing Hidden Ducks</h3>

<ul>
  <li>Case statements that switch on class</li>
  <li><code>kind_of?</code> and <code>is_a?</code></li>
  <li><code>responds_to?</code></li>
</ul>

<h3 id="guidance-1">Guidance</h3>

<p>When you create duck types you must both document and test their public interfaces. Fortunately, good tests are the best documentation.</p>

<p>The decision to create a new duck type relies on judgment. The purpose of design is to lower costs; bring this measuring stick to every situation. If creating a duck type would reduce unstable dependencies, do so. Use your best judgment.</p>

<h2 id="acquiring-behavior-through-inheritance">Acquiring Behavior Through Inheritance</h2>

<h3 id="inheritance">Inheritance</h3>

<p>Inheritance is, at its core, a mechanism for <strong>automatic message delegation</strong>. It defines a forwarding path for not-understood messages. It creates relationships such that, if one object cannot respond to a received message, it delegates that message to another. You don’t have to write code to explicitly delegate the message, instead you define an inheritance relationship between two objects and the forwarding happens automatically.</p>

<p>When your problem is one of needing numerous specializations of a stable, common abstraction, inheritance can be an extremely low-cost solution.</p>

<h3 id="recognizing-where-to-use-inheritance">Recognizing Where to Use Inheritance</h3>

<p>The inheritance exactly solves: that of highly related types that share common behavior but differ along some dimension.</p>

<p>Inheritance provides a way to define two objects as having a relationship such that when the first receives a message that it does not understand, it automatically forwards, or delegates, the message to the second. It’s as simple as that.</p>

<p>Duck types cut across classes, they do not use classical inheritance to share common behavior. Duck types share code via Ruby modules.</p>

<h4 id="finding-the-abstraction">Finding the Abstraction</h4>

<p>It almost never makes sense to create an abstract superclass with only one sub-class.</p>

<p>Creating a hierarchy has costs; the best way to minimize these costs is to maximize your chance of getting the abstraction right before allowing subclasses to depend on it. While the two bikes you know about supply a fair amount of information about the common abstraction, three bikes would supply a great deal more. If you could put this decision off until FastFeet asked for a third kind of bike, your odds of finding the right abstraction would improve dramatically.</p>

<p>When deciding between refactoring strategies, indeed, when deciding between design strategies in general, it’s useful to ask the question: “What will happen if I’m wrong?”</p>

<h3 id="using-templage-methods">Using Templage Methods</h3>

<h4 id="template-method">Template Method</h4>

<p>This technique of defining a basic structure in the superclass and sending messages to acquire subclass-specific contributions is known as the template method pattern.</p>

<h4 id="implementing-every-template-method">Implementing Every Template Method</h4>

<p>Any class that uses the template method pattern must supply an implementation for every message it sends, and creating code that fails with reasonable error messages takes minor effort in the present but provides value forever.</p>

<p><code>ruby
class Bicycle
  #...
  def default_tire_size
    raise NotImplementedError, "This #{self.class} cannot respond to:"
  end 
end
</code></p>

<h3 id="manging-coupling">Manging Coupling</h3>

<p>When a subclass sends <code>super</code> it’s effectively declaring that it knows the algorithm; it depends on this knowledge. If the algorithm changes, then the subclasses may break even if their own specializations are not otherwise affected.</p>

<h4 id="decoupling-subclasses-using-hook-messages">Decoupling Subclasses Using Hook Messages</h4>

<p>Instead of allowing subclasses to know the algorithm and requiring that they send <code>super</code>, superclasses can instead send <code>hook</code> messages, ones that exist solely to provide subclasses a place to contribute information by implementing matching methods. This strategy removes knowledge of the algorithm from the subclass and returns control to the superclass.</p>

<p>```ruby
class Bicycle
  def initialize(args={})
    @size = args[:size]
    @chain = args[:chain] || default_chain
    @tire_size = args[:tire_size] || default_tire_size</p>

<pre><code>post_initialize(args)   # Bicycle both sends   end
</code></pre>

<p>def post_initialize(args) # and implements this 
    nil
  end
  # …
end</p>

<p>class RoadBike &lt; Bicycle</p>

<p>def post_initialize(args)         # RoadBike can 
    @tape_color = args[:tape_color] # optionally
  end                               # override it
  # …
end
```</p>

<p>This change allows RoadBike to know less about Bicycle, reducing the coupling between them and making each more flexible in the face of an uncertain future. New subclasses need only implement the <code>hook</code> methods.</p>

<h2 id="sharing-role-behavior-with-modules">Sharing Role Behavior with Modules</h2>

<h3 id="understanding-roles">Understanding Roles</h3>

<p>Modules thus provide a perfect way to allow objects of different classes to play a common role using a single set of code.</p>

<p>The rules for modules are the same as for classical inheritance. If a module sends a message it must provide an implementation, even if that implementation merely raises an error indicating that users of the module must implement the method.</p>

<p>This is-a versus behaves-like-a difference definitely matters, each choice has distinct consequences.</p>

<h3 id="writing-inheritable-code">Writing Inheritable Code</h3>

<p>The usefulness and maintainability of inheritance hierarchies and modules is in direct proportion to the quality of the code.</p>

<h4 id="recognize-the-antipatterns">Recognize the Antipatterns</h4>

<p>There are two antipatterns that indicate that your code might benefit from inheritance.</p>

<ul>
  <li>An object that uses a variable with a name like <code>type</code> or <code>category</code> to determine what message to send to <code>self</code> contains two highly related but slightly different types.</li>
  <li>When a sending object checks the class of a receiving object to determine what message to send, you have overlooked a duck type. In addition to sharing an interface, duck types might also share behavior. When they do, place the shared code in a module and include that module in each class or object that plays the role.</li>
</ul>

<h4 id="insist-on-the-abstraction">Insist on the Abstraction</h4>

<p>Superclasses should not contain code that applies to some, but not all, subclasses. This restriction also applies to modules: the code in a module must apply to all who use it.</p>

<p>Subclasses that override a method to raise an exception like “does not implement” are a symptom of this problem. When subclasses override a method to declare that they <em>do not do that thing</em> they come perilously close to declaring that they <em>are not that thing</em>.</p>

<h4 id="honor-the-contract">Honor the Contract</h4>

<p>Subclasses agree to a contract; they promise to be substitutable for their superclasses.</p>

<p>Subclasses that fail to honor their contract are difficult to use. They’re “special” and cannot be freely substituted for their superclasses. These subclasses are declaring that they are not really a kind-of their superclass</p>

<p><strong>Liskov Substitution Principle (LSP)</strong>, which in mathematical terms says that a subtype should be substitutable for its supertype. Named after Barbara Liskov.</p>

<h4 id="use-the-template-method-pattern">Use the Template Method Pattern</h4>

<p>The abstract code defines the algorithms and the concrete inheritors of that abstraction contribute specializations by overriding these template methods.</p>

<p>Modules, therefore, should use the template method pattern to invite those that include them to supply specializations, and should implement hook methods to avoid forcing includers to send <code>super</code>.</p>

<h4 id="preemptively-decouple-classes">Preemptively Decouple Classes</h4>

<p>Avoid writing code that requires its inheritors to send <code>super</code>; instead use hook messages to allow subclasses to participate while absolving them of responsibility for knowing the abstract algorithm. Writing code that requires subclasses to send <code>super</code> adds an additional dependency; avoid this if you can.</p>

<p>Hook methods solve the problem of sending <code>super</code>, but, unfortunately, only for adjacent levels of the hierarchy.</p>

<h4 id="create-shallow-hierarchies">Create Shallow Hierarchies</h4>

<p>The limitations of hook methods are just one of the many reasons to create shallow hierarchies.</p>

<p>Because objects depend on everything above them, a deep hierarchy has a large set of built-in dependencies, each of which might someday change.</p>

<p>Another problem with deep hierarchies is that programmers tend to be familiar with just the classes at their tops and bottoms; that is, they tend to understand only the behavior implemented at the boundaries of the search path.</p>

<h2 id="combining-objects-with-composition">Combining Objects with Composition</h2>

<p>Composition is the act of combining distinct parts into a complex whole such that the whole becomes more than the sum of its parts.</p>

<h3 id="aggregation-a-special-kind-of-composition">Aggregation: A Special Kind of Composition</h3>

<p>Delegation creates dependencies; the receiving object must recognize the message and know where to send it. Composition often involves delegation but the term means something more. A composed object is made up of parts with which it expects to interact via well-defined interfaces.</p>

<p>Composition indicates a <em>has-a</em> relationship where the contained object has no life independent of its container.</p>

<p>Aggregation is exactly like composition except that the contained object has an independent life.</p>

<h3 id="deciding-between-inheritance-and-composition">Deciding Between Inheritance and Composition</h3>

<ul>
  <li>Remember that classical inheritance is a code arrangement technique. For the cost of arranging objects in a hierarchy, you get message delegation for free.</li>
  <li>Composition is an alternative that reverses these costs and benefits. Composition allows objects to have structural independence, but at the cost of explicit message delegation.</li>
</ul>

<p>The general rule is that, faced with a problem that composition can solve, you should be biased towards doing so. If you cannot explicitly defend inheritance as a better solution, use composition.</p>

<h4 id="inheritance-1">Inheritance</h4>

<p><strong>Benefits</strong></p>

<p>Inheritance is a better solution when its use provides high rewards for low risk.</p>

<p>Use of inheritance results in code that can be described as open–closed; hierarchies are open for extension while remaining closed for modification.</p>

<p>You need look no farther than the source of object-oriented languages themselves to see the value of organizing code using inheritance.</p>

<p><strong>Costs</strong></p>

<p>You might be fooled into choosing inheritance to solve the wrong kind of problem. If you make this mistake a day will come when you need to add behavior but find there’s no easy way do so.</p>

<p>Even when inheritance makes sense for the problem, you might be writing code that will be used by others for purposes you did not anticipate.</p>

<p>The very high cost of making changes near the top of an incorrectly modeled hierarchy. In this case, the leveraging effect works to your disadvantage; small changes break everything.</p>

<p>The impossibility of adding behavior when new subclasses represent a mixture of types.</p>

<p>Inheritance, therefore, is a place where the question “<em>What will happen when I’m wrong?</em>” assumes special importance. Inheritance by definition comes with a deeply embedded set of dependencies. Subclasses depend on the methods defined in their superclasses and on the automatic delegation of messages to those superclasses. This is classical inheritance’s greatest strength and biggest weakness.</p>

<p><strong>Guidance</strong></p>

<p>Your consideration of the use of inheritance should be tempered by your <em>expectations about the population who will use your code</em>. If you are writing code for an in-house application in a domain with which you are intimately familiar, you may be able to predict the future well enough to be confident that your design problem is one for which inheritance is a cost-effective solution.</p>

<p>Avoid writing frameworks that require users of your code to subclass your objects in order to gain your behavior. Their application’s objects may already be arranged in a hierarchy; inheriting from your framework may not be possible.</p>

<h4 id="composition">Composition</h4>

<p>Composed objects do not depend on the structure of the class hierarchy, and they delegate their own messages.</p>

<p><strong>Benefits</strong></p>

<p>When using composition, the natural tendency is to create many small objects that contain straightforward responsibilities that are accessible through clearly defined interfaces. These small objects have a single responsibility and specify their own behavior. They are transparent.</p>

<p>By their very nature, objects that participate in composition are small, structurally independent, and have well-defined interfaces. This allows their seamless transition into pluggable, interchangeable components.</p>

<p><strong>Costs</strong></p>

<p>The composed object must explicitly know which messages to delegate and to whom. Identical delegation code may be needed by many different objects. Composition provides no way to share this code.</p>

<p>Composition is excellent at prescribing rules for assembling an object made of parts but doesn’t provide as much help for the problem of arranging code for a collection of parts that are very nearly identical.</p>

<h3 id="guidance-2">Guidance</h3>

<p>Composition, classical inheritance, and behavior sharing via modules are competing techniques for arranging code.</p>

<ul>
  <li>Use inheritance for <em>is-a</em> Relationships.</li>
  <li>Use Duck Types for <em>behaves-like-a</em> Relationships</li>
  <li>Use Composition for <em>has-a</em> Relationships</li>
</ul>

<h2 id="designing-cost-effective-tests">Designing Cost-Effective Tests</h2>

<p>An understanding of object-oriented design, good refactoring skills, and the ability to write efficient tests form a <strong>three-legged stool</strong> upon which changeable code rests.</p>

<p>Your overall goal is to create well-designed applications that have acceptable test coverage.</p>

<h3 id="intentional-testing">Intentional Testing</h3>

<h4 id="knowing-your-intentions">Knowing Your Intentions</h4>

<p>The true purpose of testing, just like the true purpose of design, is to reduce costs.</p>

<p>It is common for programmers who are new to testing to find themselves in the unhappy state where the tests they write do cost more than the value those tests provide, and who therefore want to argue about the worth of tests. The solution to the problem of costly tests, however, is not to stop testing but instead to get better at it.</p>

<ol>
  <li>Finding Bugs</li>
  <li>Supplying Documentation</li>
  <li>Deferring Design Decisions</li>
  <li>Supporting Abstractions</li>
  <li>Exposing Design Flaws. When the design is bad, testing is hard. The best way to achieve this goal is to write loosely coupled tests about only the things that matter.</li>
</ol>

<h4 id="knowing-what-to-test">Knowing What to Test</h4>

<h5 id="remove-the-duplicate">Remove the Duplicate</h5>

<p>One simple way to get better value from tests is to write fewer of them. The safest way to accomplish this is to test everything just once and in the proper place.</p>

<p>Removing duplication from testing lowers the cost of changing tests in reaction to application changes, and putting tests in the right place guarantees they’ll be forced to change only when absolutely necessary.</p>

<h5 id="message-model">Message Model</h5>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/origins_of_messages.png" alt="origins_of_messages" /></p>

<p>Think of an object-oriented application as a series of messages passing between a set of black boxes. Tests should concentrate on the incoming or outgoing messages that cross an object’s boundaries.</p>

<ul>
  <li>
    <p>Incoming Message</p>

    <p>Objects should make assertions about <em>state</em> only for messages in their own public interfaces.</p>
  </li>
  <li>
    <p>Outgoing Message</p>

    <ul>
      <li><em>query</em>, outgoing messages have no side effects and thus matter only to their senders.</li>
      <li><em>command</em>, outgoing messages do have side effects (a file gets written, a database record is saved, an action is taken by an observer). It is the responsibility of the sending object to prove that they are properly sent. Proving that a message gets sent is a test of behavior, not state.</li>
    </ul>
  </li>
</ul>

<p><strong>Conclusion</strong></p>

<p>Incoming messages should be tested for the state they return. Outgoing command messages should be tested to ensure they get sent. Outgoing query messages should not be tested.</p>

<h4 id="knowing-when-to-test">Knowing When to Test</h4>

<p>You should write tests first, whenever it makes sense to do so.</p>

<p>Done at the correct time and in the right amounts, testing, and writing code test-first, will lower your overall costs. Gaining these benefits requires applying object-oriented design principles everywhere, both to the code of your application and to the code in your tests.</p>

<p><em>What novices do?</em></p>

<p>Novices often write code that is far too coupled; they combine unrelated responsibilities and bind many dependencies into every object.</p>

<p>It is an unfortunate truth that the most complex code is usually written by the least qualified person.</p>

<p>Novice programmers don’t yet have the skills to write simple code.</p>

<h4 id="knowing-how-to-test">Knowing How to Test</h4>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/bdd_and_tdd.png" alt="bdd_and_tdd" /></p>

<ul>
  <li><strong>BDD</strong> takes an outside-in approach, creating objects at the boundary of an application and working its way inward, mock-ing as necessary to supply as-yet-unwritten objects.</li>
  <li><strong>TDD</strong> takes an inside-out approach, usually starting with tests of domain objects and then reusing these newly created domain objects in the tests of adjacent layers of code.</li>
</ul>

<p><strong>Testing point-of-view</strong></p>

<p>Your tests could stand completely inside of the object under test, with effective access to all of its internals. This is a bad idea.</p>

<p>It’s better for tests to assume a viewpoint that sights along the edges of the object under test, where they can know only about messages that come and go.</p>

<h3 id="testing-incoming-messages">Testing Incoming Messages</h3>

<ul>
  <li>
    <p>Deleting Unused Interfaces</p>

    <p>Do not test an incoming message that has no dependents; delete it.</p>
  </li>
  <li>
    <p>Proving the Public Interface</p>
  </li>
  <li>
    <p>Isolating the Object Under Test</p>
  </li>
  <li>
    <p>Injecting Dependencies as Roles</p>

    <p>Object-oriented design tells you to inject dependencies because it believes that specific concrete classes will vary more than these roles, or conversely, roles will be more stable than the classes from which they were abstracted.</p>

    <ul>
      <li>Creating Test Doubles</li>
      <li>Using Tests to Document Roles</li>
    </ul>
  </li>
</ul>

<h3 id="testing-private-methods">Testing Private Methods</h3>

<p>Dealing with private methods requires judgment and flexibility.</p>

<p>The rules-of-thumb for testing private methods are thus: Never write them, and if you do, never ever test them, unless of course it makes sense to do so.</p>

<h3 id="testing-outgoing-messages">Testing Outgoing Messages</h3>

<ul>
  <li>Ignoring Query Messages</li>
  <li>
    <p>Proving Command Messages</p>

    <p>The responsibility for testing a message’s return value lies with its receiver. <strong>Mocks</strong> are tests of behavior, as opposed to tests of state. Instead of making assertions about what a message returns, mocks define an expectation that a message will get sent.</p>
  </li>
</ul>

<h3 id="testing-duck-types">Testing Duck Types</h3>

<p>The desire to test duck types creates a need for shareable tests for roles, and once you acquire this role-based perspective you can use it to your advantage in many situations. From the point of view of the object under test, every other object is a role and dealing with objects as if they are representatives of the roles they play loosens coupling and increases flexibility, both in your application and in your tests.</p>

<ul>
  <li>Testing Roles. Extract a module, test it and include in every role.</li>
  <li>Using Role Tests to Validate Doubles.</li>
</ul>

<h3 id="testing-inherited-code">Testing Inherited Code</h3>

<ul>
  <li>
    <p>Specifying the Inherited Interface</p>

    <p>Write a shared test for the common contract and include this test in every object.</p>
  </li>
  <li>
    <p>Specifying Subclass Responsibilities</p>

    <ul>
      <li>Confirming Subclass Behavior. The <em>BicycleInterfaceTest</em> and the <em>BicycleSubclassTest</em>, combined, take all of the pain out of testing the common behavior of subclasses. These tests give you confidence that subclasses aren’t drifting away from the standard.</li>
      <li>Confirming Superclass Enforcement. Test the template method.</li>
    </ul>
  </li>
  <li>
    <p>Testing Unique Behavior</p>

    <ul>
      <li>Testing Concrete Subclass Behavior. It’s important to test these specializations without embedding knowledge of the superclass into the test.</li>
      <li>Testing Abstract Superclass Behavior. Because Bicycle used template methods to acquire concrete specializations you can stub the behavior that would normally be supplied by subclasses. Even better, because you understand the Liskov Substitution Principle, you can easily manufacture a testable instance of Bicycle by creating a new subclass for use solely by this test.</li>
    </ul>
  </li>
</ul>

]]></content>
  </entry>
  
</feed>
