<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Books | Wendi's Blog]]></title>
  <link href="http://blog.ifyouseewendy.com/blog/categories/books/atom.xml" rel="self"/>
  <link href="http://blog.ifyouseewendy.com/"/>
  <updated>2016-02-16T20:16:06+08:00</updated>
  <id>http://blog.ifyouseewendy.com/</id>
  <author>
    <name><![CDATA[wendi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Review] Concurrency - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces/"/>
    <updated>2015-12-26T11:33:57+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#concurrency">Concurrency</a>    <ul>
      <li><a href="#chapter-26---introduction">Chapter 26 - Introduction</a></li>
      <li><a href="#chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</a></li>
      <li><a href="#chapter-28---locks">Chapter 28 - Locks</a></li>
      <li><a href="#chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</a></li>
      <li><a href="#chapter-30---condition-variables">Chapter 30 - Condition Variables</a></li>
      <li><a href="#chapter-31---semaphores">Chapter 31 - Semaphores</a></li>
      <li><a href="#chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</a></li>
      <li><a href="#chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</a></li>
    </ul>
  </li>
</ul>

<h1 id="concurrency">Concurrency</h1>

<h2 id="chapter-26---introduction">Chapter 26 - Introduction</h2>

<p><strong>Background</strong></p>

<p>With time sharing, we can take a single physical CPU and turn it into multiple virtual CPUs, thus enabling the illusion of multiple programs running at the same time, through time sharing.</p>

<p>With paging (base and bounds, segmentation), we can create the illusion of a large, private virtual memory for each process; this abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk).</p>

<p>But the abstraction of running program we use along is the process, and it’s a classic view of a single point of execution within a program. Now we introduce a new abstraction, thread. And  a <strong>multi-threaded</strong> program has more than one point of execution.</p>

<p>Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus ca access the same data.</p>

<p><strong>Thread vs. Process</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_vs_process.png" alt="os-thread_vs_process.png" /></p>

<p>Address space</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_address_space.png" alt="os-thread_address_space.png" /></p>

<p><strong>Advantage</strong></p>

<p>Efficiency, as they share the same address space.</p>

<ul>
  <li>Save storage</li>
  <li>Easy context switching (no need to change page)</li>
</ul>

<p><strong>Issues</strong></p>

<ul>
  <li><strong>Sharing data</strong>, that of accessing shared variables and the need to support atomicity for critical sections.</li>
  <li><strong>Waiting for another</strong>, sleeping and waking interaction, where one thread must wait for another to complete some action before it continues.</li>
</ul>

<p><strong>Shared Data</strong></p>

<p>The heart of the problem is <strong>uncontrolled scheduling</strong>.</p>

<p>It is a wonderful and hard problem, and should make your mind hurt (a bit). If it doesn’t, then you don’t understand! Keep working until your head hurts; you then know you’re headed in the right directinn.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_sharing_data.png" alt="os-thread_sharing_data.png" /></p>

<p><strong>Key Concurrency Terms</strong> (from Edsger Dijkstra)</p>

<p>A <strong>critical section</strong> is a piece of code that accesses a shared resource, usually a variable or data structure.</p>

<p>A <strong>race condition</strong> arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps un- desirable) outcome. The results depend on the timing execution of the code.</p>

<p>An <strong>indeterminate</strong> program consists of one or more races onditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems.</p>

<p>To avoid these problems, threads should use some kind of <strong>mutual exclusion primitives</strong>; doing so guarantees that only a single thread ever enters a critical section, thus avoiding racoes, and resulting in deterministic program outputs.</p>

<p><strong>Atomic</strong></p>

<p>Atomic operations are one of the most powerful underlying techniques in building computer systems.</p>

<p>The idea behind making a series of actions <strong>atomic</strong> is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a <strong>transaction</strong>.</p>

<p>In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution.</p>

<p><strong>The Wish For Atomicity</strong></p>

<p>Hardware guarantees the instructions is atomic, and provide a general set we call <strong>synchronisation primitives</strong> to ensure atomicity.</p>

<p>Hardware guarantees that the instructions execute atomically. It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state.</p>

<p>But, would we really want the hardware to support an “atomic update of B-tree” instruction?</p>

<p>No. Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call <strong>synchronization primitives</strong>. By using these hardware synchronization primitives, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution.</p>

<p><strong>Why in OS Class?</strong></p>

<p>“History” is the one-word answer; the OS was the first concurrent program, and many techniques were created for use within the OS. Later, with multi-threaded processes, application programmers also had to consider such things.</p>

<p>OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.</p>

<h2 id="chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</h2>

<p><strong>Guidelines</strong></p>

<p>There are a number of small but important things to remember when you use the POSIX thread library.</p>

<ul>
  <li><strong>Keep it simple</strong>. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interactions lead to bugs.</li>
  <li>Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum.</li>
  <li><strong>Each thread has its own stack</strong>. If you have a locally-allocated variable inside of some function a thread is exe- cuting, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the values must be in the heap or otherwise some locale that is globally accessible.</li>
  <li><strong>Be careful with how you pass arguments to, and return values from, threads.</strong> In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong.</li>
  <li><strong>Check your return codes.</strong> Of course, in any C and UNIX program- ming you do, you should be checking each and every return code, and it’s true here as well.</li>
  <li><strong>Always use condition variables to signal between threads.</strong> While it is often tempting to use a simple flag, don’t do it.</li>
  <li><strong>Initialize locks and condition variables.</strong> Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways.</li>
  <li><strong>Use the manual pages.</strong> On Linux, in particular, the pthread man pages (man -k pthread) are highly informative and discuss much of the nuances pre- sented here, often in even more detail.</li>
</ul>

<p><strong>Thread Creation</strong></p>

<p><code>c
#include &lt;pthread.h&gt;
int pthread_create(pthread_t * thread,
                     const pthread_attr_t *  attr,
                     void * (*start_routine)(void*),
                     void *  arg);
</code></p>

<ul>
  <li><code>thread</code>, is a pointer to a structure of type pthread t; we’ll use this structure to interact with this thread</li>
  <li><code>attr</code>, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread.</li>
  <li>The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (<code>start routine</code>), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer).</li>
  <li><code>arg</code>, is exactly the argument to be passed to the function where the thread begins execution.</li>
</ul>

<p><strong><em>Why do we need these void pointers?</em></strong></p>

<p>Having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result.</p>

<p><strong>Thread Completion</strong></p>

<p><code>c
int pthread_join(pthread_t thread, void **value_ptr);
</code></p>

<ul>
  <li><code>thread</code> is used to specify which thread to wait for</li>
  <li><code>value_ptr</code> is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo.png" alt="os-thread_waiting_demo.png" /></p>

<p>Note that one has to be extremely careful with how values are returned from a thread. In particular, never return a pointer which refers to something allocated on the thread’s call stack.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo_wrong.png" alt="os-thread_waiting_demo_wrong.png" /></p>

<p>However, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results.</p>

<p>Not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely. Such long-lived programs thus may not need to join.</p>

<p><strong>Locks</strong></p>

<p>Providing mutual exclusion to a critical section via locks.</p>

<p><code>c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
</code></p>

<p>When you have a region of code you realize is a critical section, and thus needs to be pro- tected by locks in order to operate as desired.</p>

<p>```c
pthread_mutex_t lock;</p>

<p>Pthread_mutex_init(&amp;lock);</p>

<p>Pthread_mutex_lock(&amp;lock);
x = x + 1; // or whatever your critical section is
Pthread_mutex_unlock(&amp;olock);</p>

<p>// Always check for failure
void Pthread_mutex_init(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_init(&amp;lock, NULL); // dynamic initialisation, or PTHREAD_MUTEX_INITIALIZER
    assert(rc == 0); // always check success!
}
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_lock(mutex);
    assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_unlock(mutex);
    assert(rc == 0);
}
```</p>

<p><strong>Condition Variables</strong></p>

<p>Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue.</p>

<p>To use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of the above routines, this lock should be held.</p>

<p><code>c
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
</code></p>

<p>pthread_cond_wait(), puts the calling thread to sleep, ad thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about.</p>

<p><code>c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t  cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&amp;lock);
while (ready == 0)
    Pthread_cond_wait(&amp;cond, &amp;lock);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>After initialization of the relevant lock and condition, a thread checks to see if the variable ready has yet been set to something other than zero. If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.</p>

<p><code>c
Pthread_mutex_lock(&amp;lock);
ready = 1;
Pthread_cond_signal(&amp;cond);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>Notice 1</p>

<p>When signaling (as well as when modifying the global variable ready), we always make sure to have the lock held. This ensures that we don’t accidentally introduce a race condition into our code.</p>

<p>Notice 2</p>

<p>Notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep.</p>

<p>Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However, before returning after being woken, the pthread_cond_wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.</p>

<p>Notice 3</p>

<p>The waiting thread re-checks the condition in a while loop, instead of a simple if statement. Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not.</p>

<p>Notice 4</p>

<p>Don’t ever use these ad hoc synchronisations.</p>

<p>```c
// waitingnwhile (ready == 0)
    ; // spin</p>

<p>// signaling
ready = 1;
```</p>

<p>First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone.</p>

<p><strong>Others</strong></p>

<p>On the link line, you must also explicitly link with the pthreads library, by adding the -pthread flag.</p>

<p><code>sh
prompt&gt; gcc -o main main.c -Wall -pthread
</code></p>

<h2 id="chapter-28---locks">Chapter 28 - Locks</h2>

<p><strong>The Basic Idea</strong></p>

<p>Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction.</p>

<p>This lock variable (or just “lock” for short) holds the state of the lock at any instant in time. It is either available (or unlocked or free) and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_demo.png" alt="os-lock_demo.png" /></p>

<p>In general, we view thre
ads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code.</p>

<p>The name that the <strong>POSIX</strong> library uses for a lock is a <strong>mutex</strong>, as it is used to provide <strong>mutual exclusion</strong> between threads.</p>

<p><strong>Building A Lock</strong></p>

<p>Some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux).</p>

<p><strong>Evaluating Locks</strong></p>

<ul>
  <li>The first is whether the lock does its basic task, which is to provide <strong>mutual exclusion</strong>. Basically, does the lock work, preventing multiple threads from entering a critical section?</li>
  <li>The second is <strong>fairness</strong>. Does each thread contending for the lock get a fair shot at acquiring it once it is free?</li>
  <li>The final criterion is <strong>performance</strong>, specifically the time overheads added by using the lock.</li>
</ul>

<p><strong>Controlling Interrupts</strong></p>

<p>Turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow.</p>

<p><strong>Plain Solution</strong></p>

<p>Without hardware support, just use a flag.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_plain_solution.png" alt="os-lock_plain_solution.png" /></p>

<p>The core issue is that the testing and setting part can be interrupted by context switch, and both thread enters the critical section.</p>

<p>You should get used to this thinking about concurrent programming. Maybe pretend yourself as a <strong>malicious scheduler</strong> to understand the concurrent execution.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_no_mutal_exclusion.png" alt="os-lock_no_mutal_exclusion.png" /></p>

<p><strong>Test And Set (Atomic Exchange)</strong></p>

<p>Let hardware provides a transaction-like instrument to ensure the sequence of operations is performed <strong>atomically</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_test_and_set.png" alt="os-lock_test_and_set.png" /></p>

<p>The key, of course, is that this sequence of operations is performed atomically. The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_test_and_set.png" alt="os-lock_spin_lock_by_test_and_set.png" /></p>

<p>By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock.</p>

<p><strong>Compare-And-Swap</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_compare_and_swap.png" alt="os-lock_compare_and_swap.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_compare_and_swap.png" alt="os-lock_spin_lock_by_compare_and_swap.png" /></p>

<p>compare-and-swap is a more powerful instruction than test-and-set. We will make some use of this power in the future when we briefly delve into <strong>wait-free synchronisation</strong>.</p>

<p><strong>Load-Linked and Store-Conditional</strong></p>

<p>Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture, for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_load_linked_store_conditional.png" alt="os-lock_load_linked_store_conditional.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_load_linked_store_conditional.png" alt="os-lock_spin_lock_by_load_linked_store_conditional.png" /></p>

<p><strong>Fetch-And-Add</strong></p>

<p>Fetch-and-add atomically increments a value while returning the old value at a particular address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_fetch_and_add.png" alt="os-lock_fetch_and_add.png" /></p>

<p>Fetch-and-add could build a <em>ticket lock</em>, this solution uses a ticket and turn variable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (myturn). The globally shared lock-&gt;turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. It has the advantage of the fairness, ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_ticket_lock_by_fetch_and_add.png" alt="os-lock_ticket_lock_by_fetch_and_add.png" /></p>

<p><strong>Spin Lock</strong></p>

<p>We use a while loop to endlessly check the value of a flag, this technique is known as <strong>spin-waiting</strong>. Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)!</p>

<p><strong>Spin lock</strong> is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a <strong>preemptive scheduler</strong>. (Remember that SJF is non-preemptive, but STCF is preemptive, which means permitting one thread to be interrupted).</p>

<p>Evaluating</p>

<ul>
  <li>√ correctness, the spin lock only allows a single thread to enter the critical section at a time.</li>
  <li>X fairness, spin locks don’t provide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Spin locks are not fair and may lead to starvation.</li>
  <li>X performance, bad in the single CPU case. The problem gets worse with N threads contending for a lock; N − 1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock.</li>
</ul>

<p><strong>Avoid Spinning by Yield</strong></p>

<blockquote>
  <p>“just yield, baby!”</p>
</blockquote>

<p>Hardware support alone cannot solve the problem. We’ll need OS support too! Assume an operating system primitive <strong>yield()</strong> which a thread can call when it wants to give up the CPU and let another thread run. A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller from the <strong>running</strong> state to the <strong>ready</strong> state, and thus promotes another thread to running. Thus, the yielding process essentially <strong>deschedules</strong> itself.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield.png" alt="os-lock_with_test_and_set_and_yield.png" /></p>

<p>This approach eliminates the spinning time, but still costly when context switching. And we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section.</p>

<p><strong>Avoid Spnning by Queues</strong></p>

<p>The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread runs that must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation.</p>

<p>Thus, we must explicitly exert some control over who gets to acquire the lock next after the current holder releases it.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield_and_queue.png" alt="os-lock_with_test_and_set_and_yield_and_queue.png" /></p>

<p>This approach thus doesn’t avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.</p>

<p>With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the first thread would then sleep forever (potentially). This problem is sometimes called the <strong>wakeup/waiting race</strong>.</p>

<p>Solaris solves this problem by adding a third system call: <strong>setpark()</strong>. By calling this routine, a thread can indicate it is about to park. If it then happens t be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.</p>

<p>You might also notice the interesting fact that the flag does not get set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from park(); however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; flag is not set to 0 in-between.</p>

<p><strong>Linux Support</strong></p>

<p>Linux provides something called a <strong>futex</strong> which is similar to the Solaris interface but provides a bit more in-kernel functionality. Specifically, each futex has associated with it a specific physical memory location; associated with each such memory location is an in-kernel queue.</p>

<ul>
  <li><code>futex_wait(address, expected)</code> puts the calling thread to sleep, assouming the value at address is equal to expected. If it is not equal, the call returns immediately.</li>
  <li><code>futex_wake(address)</code> wakes one thread that is wait- ing on the queue.</li>
</ul>

<p>Linux approach has the flavor of an old approach that has been used on and off for years, , and is now referred to as a <strong>two-phase lock</strong>. A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So in the first phase, the lock spins for a while, hoping that it can acquire the lock. However, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.</p>

<h2 id="chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</h2>

<p><strong>Background</strong></p>

<p>Adding locks to a data structure to make it usable by threads makes the structure <strong>thread safe</strong>. There is always a standard method to make a concurrent data structure: add a big lock. But sometimes we need to ensure the scalability.</p>

<p>To evaluate the concurrent data structures, theres are two factors to concern:</p>

<ul>
  <li>Correctness</li>
  <li>Performance. MORE CONCURRENCY ISN’T NECESSARILY FASTER. If the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important. Build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do.</li>
</ul>

<p>Ideally, you’d like to see the threads complete just as quickly on multiple processors as the single thread does on one. Achieving this end is called <strong>perfect scaling</strong>.</p>

<p><strong>Guidelines</strong></p>

<ul>
  <li>Be careful with acquisition and release of locks around control flow changes</li>
  <li>Enabling more concurrency does not necessarily increase performance</li>
  <li>Performance problems should only be remedied once they exist, avoiding premature optimization, is central to any performance-minded developer</li>
  <li>There is no value in making something faster if doing so will not improve the overall performance of the application.</li>
</ul>

<p><strong>Concurrent Counters</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_performance_concurrent_counters.png" alt="os-lock_performance_concurrent_counters.png" /></p>

<p>Traditional Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_traditional_counter.png" alt="os-lock_traditional_counter.png" /></p>

<p>In this manner, it is similar to a data structure built with <strong>monitors</strong>, where locks are acquired and released automatically as you call and return from object methods.</p>

<p>The performance of the synchronized counter scales poorly.</p>

<p>Sloppy Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter.png" alt="os-lock_sloppy_counter.png" /></p>

<p>The sloppy counter works by representing a single logical counter via numerous local physical counters, one per CPU core, as well as a single global counter.
When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock.
How often this local-to-global transfer occurs is determined by a threshold, which we call S here (for sloppiness). The smaller S is, the more the counter behaves like the non-scalable counter above; the bigger S is, the more scalable the counter, but the further off the global value might be from the actual count.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter_scaling.png" alt="os-lock_sloppy_counter_scaling.png" /></p>

<p><strong>Concurrent Linked Lists</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list.png" alt="os-lock_concurrent_link_list.png" /></p>

<p>One small tricky issue arises if malloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert. This kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths.</p>

<p>BE WARY OF LOCKS AND CONTROL FLOW</p>

<p>Many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone. Thus, it is best to structure code to minimize this pattern.</p>

<p>Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list_optimized.png" alt="os-lock_concurrent_link_list_optimized.png" /></p>

<p>Once again we are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency within a list is something called <strong>hand-over-hand locking</strong> (a.k.a. <strong>lock coupling</strong>).</p>

<p>Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing the list, the code first grabs the next node’s lock and then releases the current node’s lock.</p>

<p>It enables a high degree of concurrency in list operations. However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive. Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating.</p>

<p><strong>Concurrent Queues</strong></p>

<p>Look at a slightly more concurrent queue designed by Michael and Scott.</p>

<p>There are two locks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. One trick used by the Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_queue.png" alt="os-lock_concurrent_queue.png" /></p>

<p><strong>Concurrent Hash Table</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_hash_table.png" alt="os-lock_concurrent_hash_table.png" /></p>

<p>This concurrent hash table is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well. The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_scaling_hash_table.png" alt="os-lock_scaling_hash_table.png" /></p>

<p>AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)</p>

<blockquote>
  <p>“Premature optimization is the root of all evil.”</p>
</blockquote>

<p>Many operating systems utilized a single lock when first transitioning to multiprocessors, including Sun OS and Linux. In the latter, this lock even had a name, the <strong>big kernel lock (BKL)</strong>. When multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck. Thus, it was finally time to add the optimization of improved concurrency to these systems. Within Linux, the more straightforward approach was taken: replace one lock with many. Within Sun, a more radical decision was made: build a brand new operating system, known as Solaris, that incorporates concurrency more fundamentally from day one.</p>

<h2 id="chapter-30---condition-variables">Chapter 30 - Condition Variables</h2>

<p><strong>Background</strong></p>

<p>There are many cases where a thread wishes to check whether a condition is true before continuing its execution. For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a <code>join()</code>).</p>

<p>In multi-threaded programs, it is often useful for a thread to wait for some conditio to become true before proceeding. The simple approach, of just spinning until the condition becomes true, is grossly inefficient and wastes CPU cycles, and in some cases, can be incorrect.</p>

<p><strong>Definition and Routines</strong></p>

<p>To wait for a condition to become true, a thread can make use of what is known as a condition variable. A <strong>condition variable</strong> is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition) is not as desired (by <strong>waiting</strong> on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by <strong>signaling</strong> on the condition).</p>

<p>By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the famous (and still important) producer/consumer problem, as well as covering conditions.</p>

<p>A condition variable has two operations associated with it: <strong>wait()</strong> and <strong>signal()</strong>.</p>

<ul>
  <li>The <strong>wait()</strong> call is executed when a thread wishes to put itself to sleep</li>
  <li>The <strong>signal()</strong> call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo.png" alt="os-cv_waiting_demo.png" /></p>

<p><strong><em>Is the state variable <code>done</code> necessary?</em></strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo_2.png" alt="os-cv_waiting_demo_2.png" /></p>

<p>Yes. Imagine the case where the child runs immediately and calls thr exit() immediately; in this case, the child will signal, but there is no thread asleep on the condition. When the parent runs, it will simply call wait and be stuck; no thread will ever wake it. From this example, you should appreciate the importance of the state variable done; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it.</p>

<p><strong><em>Is there a need to hold the lock while singaling?</em></strong></p>

<p>Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables. The generalization of this tip is correct: hold the lock when calling signal or wait, and you will always be in good shape.</p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>The producer/consumer problem, or sometimes as the bounded buffer problem, which was first posed by Dijkstra. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized <strong>semaphore</strong> (which can be used as either a lock or a condition variable).</p>

<p>A bounded buffer is also used when you pipe the output of one program into another, e.g.,</p>

<p><code>sh
// grep process is the producer
// wc process is the consumer
// between them is an in-kernel bounded buffer
grep foo file.txt | wc -l
</code></p>

<p>Basic operations: <code>put()</code> and <code>get()</code></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_put_and_get_v1.png" alt="os-cv_put_and_get_v1.png" /></p>

<p><strong>Plain Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_v1.png" alt="os-cv_producer_and_consumer_v1.png" /></p>

<p><strong>Single CV and If</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if.png" alt="os-cv_producer_and_consumer_single_cv_and_if.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_if_trace.png" /></p>

<p><strong>Single CV and While</strong></p>

<p>Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired. This interpretation of what a signal means is often referred to as <strong>Mesa semantics</strong>, after the first research that built a condition variable in such a manner. Virtually every system ever built employs Mesa semantincs.</p>

<p>Thanks to Mesa semantics, a simple rule to remember with condition variables is to <strong>always use while loops</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while.png" alt="os-cv_producer_and_consumer_single_cv_and_while.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_while_trace.png" /></p>

<p><strong>Two CVs and While</strong></p>

<p>Signaling is clearly needed, but must be more directed. <strong>A consumer should not wake other consumers, only producers</strong>, and vice-versa.</p>

<p>Use two condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Producer threads wait on the condition empty, and signals fill. Conversely, consumer threads wait on fill and signal empty.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_two_cv_and_while.png" alt="os-cv_producer_and_consumer_single_two_cv_and_while.png" /></p>

<p><strong>Final Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_final_solution.png" alt="os-cv_producer_and_consumer_final_solution.png" /></p>

<p><strong>Covering Conditions</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_covering_conditions.png" alt="os-cv_producer_and_consumer_covering_conditions.png" /></p>

<p>Assume there are zero bytes free; thread Ta calls <code>allocate(100)</code>, followed by thread Tb which asks for less memory by calling <code>allocate(10)</code>. Both Ta and Tb thus wait on the condition and go to sleep; there aren’t enough free bytes to satisfy either of these requests. At that point, assume a third thread, Tc, calls <code>free(50)</code>. Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed; Ta should remain waiting, as not enough memory is yet free. Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.</p>

<p>The solution suggested by Lampson and Redell is straightforward: replace the <code>pthread_cond_signal()</code> call in the code above with a call to <code>pthread_cond_broadcast()</code>, which wakes up all waiting threads. Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.</p>

<p>Lampson and Redell call such a condition a <strong>covering condition</strong>, as it covers all the cases where a thread needs to wake up (conservatively); the cost, is that too many threads might be woken.</p>

<p>In general, if you find that your program only works when you change your signals to broadcasts (but you don’t think it should need to), you probably have a bug; fix it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available.</p>

<h2 id="chapter-31---semaphores">Chapter 31 - Semaphores</h2>

<p><strong>Background</strong></p>

<p>As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems. One of the first people to realize this years ago was Edsger Dijkstra. Dijkstra and colleagues invented the semaphore as a single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables.</p>

<p><strong>Definition</strong></p>

<p>A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem <code>wait()</code> and sem <code>post()</code>. The initial value of the semaphore determines its behaviour.</p>

<p>Semaphores are a powerful and flexible primitive for writing concurrent programs. Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility.</p>

<p>In my view, semaphore is an primitive, which can be made by locks and condition variables, also can’t be used as locks and condition variables.</p>

<p>Initialization</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_init.png" alt="os-semaphore_init.png" /></p>

<p>Usage
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_definition.png" alt="os-semaphore_definition.png" /></p>

<ul>
  <li><code>sem_wait()</code> will either return right away (because the value of the semaphore was one or higher when we called <code>sem_wait()</code>), or it will cause the caller to suspend execution waiting for a subsequent post.</li>
  <li><code>sem_post()</code> does not wait for some particular condition to hold like <code>sem_wait()</code> does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.</li>
  <li>The value of the semaphore, when negative, is equal to the number of waiting threads</li>
</ul>

<p><strong>Semaphores As Locks</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_locks.png" alt="os-semaphore_as_locks.png" /></p>

<p>Because locks only have two states (held and not held), this usage is sometimes known as a <strong>binary semaphore</strong>.</p>

<p><strong>Semaphores As Condition Variables</strong></p>

<p>Semaphores are also useful when a thread wants to halt its progress waiting for a
 condition to become true. In this pattern of usage, we often find a thread waiting for something to happen, and a different thread making that something happen and then signaling that it has happened, thus waking the waiting thread.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_cv.png" alt="os-semaphore_as_cv.png" /></p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>Plain Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_plain.png" alt="os-semaphore_producer_and_consumer_plain.png" /></p>

<p>The condition variable (semaphore based) controls the execution order, which can let multiple threads enter the critical section at the same time. It still needs a lock.</p>

<p>Adding Mutual Exclusion</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex.png" alt="os-semaphore_producer_and_consumer_add_mutex.png" /></p>

<p>The consumer holds the mutex and is waiting for the someone to signal full. The producer could si!gnal full but is waiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.</p>

<p>To avoid the deadlock, we can simply move the mutex acquire and release to be just around the critical section. The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex_correctly.png" alt="os-semaphore_producer_and_consumer_add_mutex_correctly.png" /></p>

<p><strong>Reader-Writer Locks</strong></p>

<p>Another classic problem stems from the desire for a more flexible <strong>locking primitive</strong> that admits that different data structure accesses might require different kinds of locking.</p>

<p>Imagine a number of concurrent list operations, including inserts and simple lookups. While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a <strong>reader-writer lock</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_reader_writer_lock.png" alt="os-semaphore_reader_writer_lock.png" /></p>

<p>Once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until all readers are finished; the last one to exit the critical section calls sem <code>post()</code> on “writelock” and thus enables a waiting writer to acquire the lock.</p>

<p>This approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it would be relatively easy for readers to starve writers. It should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives.</p>

<p>SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)</p>

<p>You should never underestimate the notion that the simple and dumb approach can be the best one. Always try the simple and dumb approach first.</p>

<p><strong>The Dining Philosophers</strong></p>

<p>One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem.</p>

<p>There are five “philosophers” sitting around a table. Between each pair of philosophers is a single fork (and thus, five total). The philosophers each have times where they think, and don’t need any forks, and times where they eat. In order to eat, a philosopher needs two forks, both the one on their left and the one on their right. The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers.png" alt="os-semaphore_dinning_philosophers.png" /></p>

<p>Broken Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_deadlock_solution.png" alt="os-semaphore_dinning_philosophers_deadlock_solution.png" /></p>

<p>The problem is deadlock. If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever.</p>

<p>A Solution: Breaking The Dependency</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_solution.png" alt="os-semaphore_dinning_philosophers_solution.png" /></p>

<p><strong>Implement Semaphores</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_implementation.png" alt="os-semaphore_implementation.png" /></p>

<h2 id="chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</h2>

<p><strong>Background</strong></p>

<p>Lu et al has made a study, which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_bugs.png" alt="os-concurrency_bugs.png" /></p>

<p><strong>Non-Deadlock Bugs</strong></p>

<ul>
  <li>Atomicity violation bugs. The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution). Solve by locks.</li>
  <li>Order violation bugs. The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution). Solve by condition variables.</li>
</ul>

<p><strong>Deadlock Bugs</strong></p>

<p>Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_dependency.png" alt="os-concurrency_deadlock_dependency.png" /></p>

<p><strong>Caused by</strong></p>

<p>One reason is that in large code bases, complex dependencies arise between cmponents. The design of locking strategies in large systems must be carefully done to avoid deadlock in the case of <strong>circular dependencies</strong> that may occur naturally in the code.</p>

<p>Another reason is due to the nature of <strong>encapsulation</strong>. As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_by_encapsulation.png" alt="os-concurrency_deadlock_by_encapsulation.png" /></p>

<p><strong>Conditions for Deadlock</strong></p>

<ul>
  <li><strong>Mutual exclusion</strong>: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).</li>
  <li><strong>Hold-and-wait</strong>: Threads hold resources allocated to them (e.g.,locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).</li>
  <li><strong>No preemption (hold)</strong>: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.</li>
  <li><strong>Circular wait (wait)</strong>: There exists a coircular chain of threads such that each thread holds one more resources (e.g., locks) that are being requested by the next thread in the chain.</li>
</ul>

<p><strong>Prevention Based on Four Conditions</strong></p>

<p>Mutual Exclusion</p>

<p>To avoid the need for mutual exclusion at all. Herlihy had the idea that one could design various data structures to be <strong>wait-free</strong>. The idea here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_wait_free.png" alt="os-concurrency_deadlock_wait_free.png" /></p>

<p>However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.</p>

<p>Hold-and-wait</p>

<p>The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_hold_and_wait.png" alt="os-concurrency_deadlock_hold_and_wait.png" /></p>

<p>By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided.</p>

<p>Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.</p>

<p>No Preemption</p>

<p>Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, a <code>trylock()</code> routine will grab the lock (if it is available) or return -1 indicating that the lock is held right now and that you should try again later if you want to grab that lock.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_no_preemption.png" alt="os-concurrency_deadlock_no_preemption.png" /></p>

<p>One new problem does arise, however: <strong>livelock</strong>. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name lovelock. One could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.</p>

<p>Another issues arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement.</p>

<p>Circular Wait</p>

<p>The best solution in practice is to be careful, develop a lock acquisition order, and thus prevent deadlock from occurring in the first place.</p>

<ul>
  <li>The most straightforward way to do that is to provide a <strong>total ordering</strong> on lock acquisition. For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.</li>
  <li>A <strong>partial ordering</strong> can be a useful way to structure lock acquisition so as to avoid deadlock.</li>
</ul>

<p><strong>Avoidance via Scheduling</strong></p>

<p>Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_avoid_via_scheduling.png" alt="os-concurrency_deadlock_avoid_via_scheduling.png" /></p>

<p>Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution</p>

<p><strong>Detect and Recover</strong></p>

<p>One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected.</p>

<p>Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.</p>

<p>DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)</p>

<p>Tom West says famously, “Not everything worth doing is worth doing well”, which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small.</p>

<p><strong>Others</strong></p>

<p>Perhaps the best solution is to develop new concurrent programming models: in systems such as <strong>MapReduce</strong> (from Google), programmers can describe certain types of parallel computations without any locks whatsoever.</p>

<h2 id="chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</h2>

<p><strong>Background</strong></p>

<p>A different style of concurrent programming is often used in both GUI-based applications as well as some types of internet servers. This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js, but its roots are found in C/UNIX systems that we’ll discuss below.</p>

<p>Event-based servers give control of scheduling to the application itself, but do so at some cost in complexity and difficulty of integration with other aspects of modern systems (e.g., paging). Because of these challenges, no single approach has emerged as best; thus, both threads and events are likely to persist as two different approaches to the same concurrency problem for many years to come.</p>

<p>The problem that event-based concurrency addresses is two-fold.</p>

<ul>
  <li>The first is that managing concurrency correctly in multi-threaded applications can be challenging.</li>
  <li>The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs.</li>
</ul>

<p><strong>The Basic Idea: An Event Loop</strong></p>

<p>The approach is quite simple: you simply wait for something (i.e., an “event”) to occur; when it does, you check what type of  event it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.). That’s it!</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_loop.png" alt="os-event_loop.png" /></p>

<p>Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle ext is equivalent to scheduling. This explicit control over scheduling is one of the fundamental advantages of the event- based approach.</p>

<p>But there is a big question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O? Specifically, how can an event server tell if a message has arrived for it?</p>

<p><strong>An Important API: select() (or poll())</strong></p>

<p>In most systems, a basic API is available, via either the <strong>select()</strong> or <strong>poll()</strong> system calls. Either way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed.</p>

<p>What these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_api.png" alt="os-event_select_api.png" /></p>

<p>First, note that it lets you check whether descriptors can be reand from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).</p>

<p>Second, note the timeout argument. One common usage here is to set the timeout to <code>NULL</code>, which causes <code>select()</code> to block indefinitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to <code>select()</code> to return immediately.</p>

<p>Now linux uses <strong>epoll</strong>, FreeBSD (Mac OS) uses <strong>kqueue</strong>, and Windows uses <strong>IOCP</strong>.</p>

<p>BLOCKING VS. NON-BLOCKING INTERFACES</p>

<ul>
  <li>Blocking (or synchronous) interfaces do all of their work before returning to the caller. The usual culprit in blocking calls is I/O of some kind.</li>
  <li>Non-blocking (or asynchronous) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background. Non-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.</li>
</ul>

<p>DON’T BLOCK IN EVENT-BASED SERVERS</p>

<p>Event-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution the caller can ever be made; failing to obey this design tip will result in a blocked event-based server.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_code_demo.png" alt="os-event_select_code_demo.png" /></p>

<p>Advantage</p>

<p>With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Specifically, because only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread because it is decidedly single threaded. Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach.</p>

<p><strong>Issue: Blocking System Calls</strong></p>

<p>For example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request). Both the open() and read() calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service.</p>

<p>With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress. Indeed, this natural <strong>overlap</strong> of I/O and other computation is what makes thread-based programming quite natural and straight-forward.</p>

<p>With an event-based approach, however, there are no other threads to run: just the main event loop. And this implies that if an event handler issues a call that blocks, the entire server will do just that: block until the call completes.</p>

<p>We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed.</p>

<p>Solution: Asynchronous I/O</p>

<p>To overcome this limit, many modern operating systems have intro- duced new ways to issue I/O requests to the disk system, referred to generically as asynchronous I/O. These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed; additional interfaces enable an application to determine whether various I/Os have completed.</p>

<p>The APIs revolve around a basic structure, the struct aiocb or <strong>AIO control block</strong> in common terminology.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_aio_control_block.png" alt="os-event_aio_control_block.png" /></p>

<ul>
  <li>An application can periodically poll the system via a call to aio error() to determine whether said I/O has yet completed.</li>
  <li>Some systems provide an approach based on the interrupt. This method uses UNIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system.</li>
</ul>

<p>In systems without asynchronous I/O, the pure event-based approach cannot be implemented. However, clever researchers have derived methods that work fairly well in their place. For example, Pai et al describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os.</p>

<p>UNIX SIGNALS</p>

<p>A huge and fascinating infrastructure known as <strong>signals</strong> is present in all mod ern UNIX variants. At its simplest, signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a <strong>signal handler</strong>, i.e., some code in the application to handle that signal. When finished, the process just resumes its previous behaviour. A program can be configured to catch that signal. Or when a signal is sent to a process not config- ured to handle that signal, some default behavior is enacted; for SEGV, the process is killed.</p>

<p><strong>Issue: State Management</strong></p>

<p>When an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread. Adya et al. call this work <strong>manual stack management</strong>, and it is fundamental to event-based programming.</p>

<p>Solution: Continuation</p>

<p>Use an old programming language construct known as a <strong>continuation</strong>. Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_state_management.png" alt="os-event_state_management.png" /></p>

<p>Record the socket descriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (fd). When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller.</p>

<p><strong>What Is Still Difficult With Events</strong></p>

<p>Multiple CPUS. When systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared. Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel; when doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed. Thus, on modern multicore systems, simple event handling without locks is no longer possible.</p>

<p>Implicit blocking. It does not integrate well with certain kinds of systems activity, such as paging. For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes. Even though the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.</p>

<p>API changes all the time. That event-based code can be hard to manage over time, as the exact semantics of various routines changes]. For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces. Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.</p>

<p>Async network I/O. Though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there, and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think. For example, while one would simply like to use the select() interface to manage all outstanding I/Os, usually some combination of select() for networking and the AIO calls for disk I/O are required.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Virtualization - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces/"/>
    <updated>2015-11-22T13:44:38+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#cpu-virtualisation">CPU Virtualisation</a>    <ul>
      <li><a href="#process">Process</a>        <ul>
          <li><a href="#chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</a></li>
          <li><a href="#chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</a></li>
        </ul>
      </li>
      <li><a href="#mechanism">Mechanism</a>        <ul>
          <li><a href="#chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</a></li>
        </ul>
      </li>
      <li><a href="#scheduling">Scheduling</a>        <ul>
          <li><a href="#chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</a></li>
          <li><a href="#chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</a></li>
          <li><a href="#chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</a></li>
          <li><a href="#chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#memory-virtualisation">Memory Virtualisation</a>    <ul>
      <li><a href="#address-space">Address Space</a>        <ul>
          <li><a href="#chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</a></li>
          <li><a href="#chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</a></li>
        </ul>
      </li>
      <li><a href="#dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</a>        <ul>
          <li><a href="#chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</a></li>
          <li><a href="#chapter-16-segmentation">Chapter 16 Segmentation</a></li>
          <li><a href="#chapter-17---free-space-management">Chapter 17 - Free-Space Management</a></li>
        </ul>
      </li>
      <li><a href="#paging">Paging</a>        <ul>
          <li><a href="#chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</a></li>
          <li><a href="#chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</a></li>
          <li><a href="#note-on-cache-management">Note on Cache Management</a></li>
          <li><a href="#chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</a></li>
        </ul>
      </li>
      <li><a href="#beyond-physical-memory">Beyond Physical Memory</a>        <ul>
          <li><a href="#chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</a></li>
          <li><a href="#chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</a></li>
          <li><a href="#chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>

<p><strong>The Crux of the whole book</strong></p>

<p>How does the operating system virtualize resources?
What mechanisms and policies are implemented by the OS to attain virtualization?
How does the OS do so efficiently?</p>

<p><strong>The Von Neumann model of computing</strong></p>

<p>Many millions (and these days, even billions) of times every second, the processor <strong>fetches</strong> an instruction from memory, <strong>decodes</strong> it (i.e., figures out which instruction this is), and <strong>executes</strong> it.</p>

<p><strong>The OS is sometimes known as a resource manager</strong></p>

<p>The primary way the OS does this is through a general technique that we call virtualization. That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more general, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a <strong>virtual machine</strong>.</p>

<p><strong>Virtualizing the CPU</strong></p>

<p>Turning a single CPU (or small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to seemingly run at once is what we call virtualizing the CPU.</p>

<p><strong>Virtualizing the Memory</strong></p>

<p>Memory is just an array of bytes; to <strong>read</strong> memory, one must specify an <strong>address</strong> to be able to access the data stored there; to <strong>write</strong> (or update) memory, one must also specify the data to be written to the given address.</p>

<p>The OS is virtualizing memory. Each process accesses its own private <strong>virtual address space</strong> (sometimes just called its address space)</p>

<p><strong>Concurrency</strong></p>

<p>Three instructions: one to <strong>load</strong> the value of the counter from memory into a register, one to <strong>increment</strong> it, and one to <strong>store</strong> it back into memory. Because these three instructions do not execute atomically (all at once), strange things can happen.</p>

<p><strong>Persistence</strong></p>

<p>The software in the operating system that usually manages the disk is called the <strong>file system</strong>; it is thus responsible for storing any files the user creates in a reliable and efficient manner on the disks of the system.</p>

<p>For performance reasos, most file systems first <strong>delay</strong> such writes for a while, hoping to batch them into larger groups. To handle the problems of system crashes during writes, most file systems incorporate some kind of intricate write protocol, such as <strong>journaling</strong> or <strong>copy-on-write</strong>, carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards.</p>

<p><strong>Design Goals</strong></p>

<p>What an OS actually does: it takes physical <strong>resources</strong>, such as a CPU, memory, or disk, and <strong>virtualizes</strong> them. It handles tough and tricky issues related to <strong>concurrency</strong>. And it stores files <strong>persistently</strong>, thus making them safe over the long-term.</p>

<ol>
  <li>To build up some <strong>abstractions</strong> in order to make the system convenient and easy to use.</li>
  <li>To provide high <strong>performance</strong>, another way to say this is our goal is to minimize the overheads of the OS.</li>
  <li>To provide <strong>protection</strong> between applications, as well as between the OS and applications. Protection is at nthe heart of one of the main principles underlying an operating system, which is that of <strong>isolation</strong>; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.</li>
</ol>

<p><strong>Some History</strong></p>

<ol>
  <li>Early Operating Systems: Just Libraries.  This mode of computing was known as <strong>batch</strong> processing.</li>
  <li>Beyond Libraries: Protection. The idea of a system call was invented. The key difference between a <strong>system call</strong> and a <strong>procedure call</strong> is that a system call transfers control (i.e., jumps) into the OS while simultaneously raising the hardware privilege level. User applications run in what is referred to as user mode which means the hardware restricts what applications can do; When a system call is initiated (usually through a special hardware instruction called a trap), the hardware transfers control to a pre-specified trap handler (that the OS set up previously) and simultaneously raises the privilege level to kernel mode.</li>
  <li>The Era of Multiprogramming by minicomputer. In particular, multiprogramming became commonplace due to the desire to make better use of machine resources. One of the major practical advances of the time was the introduction of the <strong>UNIX</strong> operating system, primarily thanks to <strong>Ken Thompson</strong> (and <strong>Dennis Ritchie</strong>) at Bell Labs (yes, the phone company). <strong>Bill Joy</strong>, made a wonderful distribution (the Berkeley Systems Distribution, or <strong>BSD</strong>) which had some advanced virtual memory, file system, and networking subsystems. Joy later co-founded Sun Microsystems.</li>
  <li>The Modern Era by PC with DOS, Mac OS.</li>
</ol>

<h1 id="cpu-virtualisation">CPU Virtualisation</h1>

<h2 id="process">Process</h2>

<h3 id="chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</h3>

<p><strong>Process</strong></p>

<p>The definition of a process, informally, is quite simple: it is a running program.</p>

<p><strong>How to provide the illusion of many CPUs?</strong></p>

<p>This basic technique, known as <strong>time sharing</strong> of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.</p>

<p><strong>Mechanisms</strong></p>

<p>Mechanisms are low-level methods or protocols that implement a needed piece of functionality.</p>

<p><strong>Policies</strong></p>

<p>On top of these mechanisms resides some of the intelligence in the OS, in the form of policies.</p>

<p><strong>Tip: Separate policy and mechanism</strong></p>

<p>In many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms. You can think of the mechanism as providing the answer to a <strong>how</strong> question about a system; for example, how does an operating system perform a context switch? The policy provides the answer
 to a <strong>which</strong> question; for example, which process should the operating system run right now?</p>

<p><strong>Machine State</strong></p>

<p>To understand what constitutes a process, we thus have to understand its <strong>machine state</strong>: what a program can read or update when it is running. At any given time, what parts of the machine are important to the execution of this program?</p>

<ol>
  <li>Memory. The memory that the process can address (called its <strong>address space</strong>) is part of the process.</li>
  <li>Registry. There are some particularly special registers that form part of this machine state. For example, the <strong>program counter</strong> (PC) (sometimes called the instruction pointer or IP). similarly a stack pointer and associated <strong>frame pointer</strong> are used to manage the stack for function parameters, local variables, and return addresses.</li>
  <li>I/O information. Programs often access persistent storage devices too. Such I/O information might include a list of the files the process currently has open.</li>
</ol>

<p><strong>Process API</strong></p>

<ol>
  <li>Create</li>
  <li>Destroy</li>
  <li>Wait</li>
  <li>Miscellaneous Control (suspend, resume)</li>
  <li>Status</li>
</ol>

<p><strong>How does the OS get a program up and running?</strong></p>

<ol>
  <li>To <strong>load</strong> its code and any static data (e.g., initialized variables) into memory, into the <strong>address space</strong> of the process. In early (or simple) operating systems, the loading process is done <strong>eagerly</strong>; modern OSes perform the process <strong>lazily</strong>, i.e., by loading pieces of code or data only as they are needed during program execution. To truly understand how lazy loading of pieces of code and data works, you’ll have to understand more about the machinery of <strong>paging</strong> and <strong>swapping</strong>.</li>
  <li>Once the code and static data are loaded into memory, there are a few other things the OS needs to do before running the process. Some memory must be allocated for the program’s <strong>run-time stack</strong> (or just stack). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.</li>
  <li>The OS may also allocate some memory for the program’s <strong>heap</strong>. In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling malloc() and free it explicitly by calling free(). The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures.</li>
  <li>The OS will also do some other initialization tasks, particularly as related to input/output (I/O). For example, in UNIX systems, each process by default has three open <strong>file descriptors</strong>.</li>
  <li>To start the program running at the entry point, namely main(), the OS transfers control of the CPU to the newly-created process, and thus the program begins its execution.</li>
</ol>

<p><strong>Process States</strong></p>

<ol>
  <li>Running</li>
  <li>Ready</li>
  <li>Blocked</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-process_state_transitions.png" alt="os-process_state_transitions.png" /></p>

<p><strong>Data Structures</strong></p>

<p>To track the state of each process, for example, the OS likely will keep some kind of <strong>process list</strong> for all processes that are ready, as well as some additional information to track which process is currently running.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-the_xv6_proc_structure.png" alt="os-the_xv6_proc_structure.png" /></p>

<p>The <strong>register context</strong> will hold, for a stopped process, the contents of its registers. When a process is stopped, its registers will be saved to this memory location; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running the process.</p>

<p>Sometimes people refer to the individual structure that stores information about a process as a <strong>Process Control Block (PCB)</strong>.</p>

<h3 id="chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</h3>

<p>UNIX presents one of the most intriguing ways to create a new process with a pair of system calls:</p>

<p><strong>fork()</strong></p>

<p>The newly-created process (called the <strong>child</strong>, in contrast to the creating <strong>parent</strong>) desn’t start running at main(), like you might expect (note, the “hello, world” message only got printed out once); rather, it just comes into life as if it had called fork() itself. You might have noticed: the child isn’t an exact copy. Specifically, al- though it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of fork() is different.</p>

<p>The output is <strong>not deterministic</strong>. When the child process is created, there are now two active processes in the system that we care about: the parent and the child.</p>

<p><strong>wait()</strong></p>

<p>Adding a wait() call to the code above makes the output <strong>deterministic</strong>.</p>

<p><strong>exec()</strong></p>

<p>It does not create a new process; rather, it transforms the currently running program (formerly p3) into a different running program (wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns.</p>

<p><strong>Why? Motivating The API</strong></p>

<p>Why would we build sucho an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of fork() and exec() is essential in building a UNIX shell, because it lets the shell run code after the call to fork() but before the call to exec(); this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.</p>

<p><strong>How Does Shell Utilise The API?</strong></p>

<p>The shell is just a user program.</p>

<ol>
  <li>It shows you a prompt and then waits for you to type something into it.</li>
  <li>You then type a command (i.e., the name of an executable program, plus any arguments) into it;</li>
  <li>In most cases, the shell then figures out where in the file system the executable resides</li>
  <li>calls fork() to create a new child process to run the command</li>
  <li>calls some variant of exec() to run the command</li>
  <li>waits for the command to complete by calling wait().</li>
  <li>When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command.</li>
</ol>

<p>eg. prompt&gt; wc p3.c &gt; newfile.txt</p>

<p>When the child is created, before calling exec(), the shell closes standard output and opens the file newfile.txt.</p>

<h2 id="mechanism">Mechanism</h2>

<h3 id="chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</h3>

<p><strong>The Crux</strong></p>

<ul>
  <li>performance: how can we implement virtualization without adding excessive overhead to the system?</li>
  <li>control: how can we run processes efficiently while retaining control over the CPU?</li>
</ul>

<p>Attaining performance while maintaining control is thus one of the central challenges in building an operating system.</p>

<p><strong>Basic Technique: Limited Direct Execution</strong></p>

<p>The basic idea is straightforward: just run the program you want to run on the CPU, but first make sure to set up the hardware so as to limit what the process can do without OS assistance.</p>

<p>In an analogous manner, the OS “baby proofs” the CPU, by first (during boot time) setting up the <strong>trap handlers</strong> and starting an <strong>interrupt timer</strong>, and then by only running processes in a restricted mode. By doing so, the OS can feel quite assured that processes can run efficiently, only requir- ing OS intervention to perform privileged operations or when they have monopolized the CPU for too long and thus need to be switched out.</p>

<p><strong>Problem #1: Restricted Operations</strong></p>

<p>Use Protected Control Transfer</p>

<p>The hardware assists the OS by providing different modes of execution. In <strong>user mode</strong>, applications do not have full access to hardware resources. In <strong>kernel mode</strong>, the OS has access to the full resources of the machine. When the user process wants to perform some kinds of privileged operation, it can perform a <strong>system call</strong>.</p>

<p><strong>System Call</strong></p>

<p>To execute a system call, a program must execute a special <strong>trap</strong> instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now per- form whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When finished, the OS calls a special <strong>return-from-trap</strong> instruction</p>

<p><strong>Why System Calls Look Like Procedure Calls?</strong></p>

<p>It is a procedure call, but hidden inside that procedure call is the famous trap instruction. More specifically, when you call open() (for example), you are executing a procedure call into the C library. The parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and return values correctly, as well as execute the hardware-specific trap instruction. And now you know why you personally don’t have to write assembly code to trap into an OS; somebody has already written that assembly for you.</p>

<p><strong>How does the trap know which code to run inside the OS?</strong></p>

<p>The kernel does so by setting up a <strong>trap table</strong> at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be. The OS informs the hardware of the locations of these <strong>trap handlers</strong>.</p>

<p><strong>Limited Direct Execution Protocol</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-limited_directed_execution_protocol.png" alt="os-limited_directed_execution_protocol.png" /></p>

<p>There are two phases in the LDE protocol:</p>

<p>In the first (at boot time), the kernel initializes the <strong>trap table</strong>, and the CPU remembers its location for subsequent use.</p>

<p>In the second (when running a process), the kernel sets up a few things (e.g., allocating a node on the process list, allocating memory) before using a <strong>return-from-trap</strong> instruction to start the execution of the process; this switches the CPU to user mode and begins running the process.</p>

<p>Normal flow:</p>

<p>When the process wishes to issue a system call, it traps back into the OS, which handles it and once again returns control via a return-from-trap to the process. The process then completes its work, and returns from main(); this usually will return into some stub code which will properly exit the program (say, by calling the exit() system call, which traps into the OS).</p>

<p><strong>Problem #2: Switching Between Processes</strong></p>

<p>How can the operating system regain control of the CPU so that it can switch between processes?</p>

<p>In a <strong>cooperative</strong> scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some kind to take place.</p>

<p>How can the OS gain control of the CPU even if processes are not being cooperative? What can the OS do to ensure a rogue process does not take over the machine?</p>

<p><strong>Timer Interrupt</strong></p>

<p>A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.</p>

<p>The OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is of course a privileged operation.</p>

<p><strong>Scheduler</strong></p>

<p>Whether to continue running the currently-running process, or switch to a different one. This decision is made by a part of the operating system known as the scheduler.</p>

<p>If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a <strong>context switch</strong>. A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its kernel stack).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-timer_interrupt.png" alt="os-timer_interrupt.png" /></p>

<h2 id="scheduling">Scheduling</h2>

<h3 id="chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</h3>

<p><strong>Scheduling Metrics</strong></p>

<ul>
  <li>performance
    <ul>
      <li>turnaround = T(completion) - T(arrival)</li>
      <li>responsive time = T(first run) - T(arrival)</li>
    </ul>
  </li>
  <li>fairness</li>
</ul>

<p>Performance and fairness are often at odds in scheduling.</p>

<p>The introduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born: response time.</p>

<p><strong>Assumption</strong></p>

<ol>
  <li>Each job runs for the same amount of time.</li>
  <li>All jobs arrive at the same time.</li>
  <li>Once started, each job runs to completion.</li>
  <li>All jobs only use the CPU (i.e., they perform no I/O)</li>
  <li>The run-time (length) of each job is known.</li>
</ol>

<p><strong>Policy 1-1 FIFO</strong></p>

<p>under assumption: 1,2,3,4,5</p>

<p>Given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm.</p>

<p><strong>Policy 1-2 SJF (Shortest Job First)</strong></p>

<p>under assumption: <del>1,</del>2,3,4,5</p>

<p>Why is FIFO not good?</p>

<p>If Assumption(1) is false, there will be the <strong>convoy effect</strong>, where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.</p>

<p>Is SJF preemptive?</p>

<p>No, it’s <strong>non-preemptive</strong>. In the old days of batch computing, a number of non-preemptive scheulers were developed; such systems would run each job to completi before considering whether to run a new job. Virtually all modern schedulers are <strong>preemptive</strong>, and quite willing to stop one process from running in order to run another.</p>

<p><strong>Policy 1-3 STCF (Shortest Time-to-Completion First) or PSJF (Preemptive Shortest Job First)</strong></p>

<p>under assumption: <del>1,2,3,</del>4,5</p>

<p>Notice that there a significant difference between SJF and STCF. As SJF is non-preemptive, system would run each job to completion before running other jobs. But STCF prefers the shortest time-to-completion jobs, which should preempt CPU to make sense. That’s why STCF also has another name, PSFJ, Preemptive Shortest Job First.</p>

<p><strong>Policy 2 RR (Round-Robin)</strong></p>

<p>The basic idea is simple: instead of running jobs to completion, RR runs a job for a <strong>time slice</strong> (sometimes called a scheduling quantum) and then switches to the next job in the run queue.</p>

<p>The length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance. Thus, de- ciding on the length of the time slice presents a trade-off to a system de- signer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive.</p>

<p>RR, with a reonasonable time slice, is thus an excellent scheduler if response time is our only metric. It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric.</p>

<p><strong>Policy 1 vs. Policy 2</strong></p>

<p>There is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to com- pletion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time. This type of trade-off is common in systems</p>

<p><strong>Incorporate I/O by overlap</strong></p>

<p>under assumption: 4</p>

<p>We see how a scheduler might incorporate I/O. By treating each CPU burst as a job, the scheduler makes sure processes that are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.</p>

<h3 id="chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</h3>

<p><strong>MLFQ</strong></p>

<p>it has <strong>multiple levels of queues</strong>, and <strong>uses feedback to determine the priority</strong> of a given job.</p>

<p>Instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads.</p>

<p><em>Multi-Level</em></p>

<p>The MLFQ has a number of distinct queues, each assigned a different <strong>priority level</strong>. At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run. Of course, more than one job may be on a given queue, and thus have the same priority. In this case, we will just use round-robin scheduling among those jobs.</p>

<p><em>Feedback</em></p>

<p>Thus, the key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.</p>

<p><strong>How To Change Priority</strong></p>

<p>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).
Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).
Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.</p>

<p><em>Problems</em></p>

<ol>
  <li>Starvation</li>
  <li>Smart user could rewrite their program to game the scheduler.</li>
  <li>A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.</li>
</ol>

<p><strong>How to prevent gaming of our scheduler?</strong></p>

<p>Rules 4a and 4b, let a job retain its priority by relinquishing the CPU before the time slice expires. The solution here is to perform better <strong>accounting</strong> of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue.</p>

<p>Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</p>

<p><strong>Priority Boost</strong></p>

<p>The simple idea here is to periodically boost the priority of all the jobs in system.</p>

<p>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</p>

<p><strong>Tuning MLFQ</strong></p>

<p>One big question is how to <strong>parameterize</strong> such a scheduler.</p>

<ul>
  <li>How many queues should there be?</li>
  <li>How big should the time slice be per queue?</li>
  <li>How often should priority be boosted in order to avoid starvation and account for changes in behavior?</li>
</ul>

<p><em>Some Variants</em></p>

<p>Most MLFQ variants allow for <strong>varying time-slice length</strong> across different queues. The high-priority queues are usually given short time slices; the low-priority queues, in contrast, contain long-running jobs that are CPU-bound; hence, longer time slices work well.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lower_priority_longer_quanta.png" alt="os-lower_priority_longer_quanta.png" /></p>

<p>The FreeBSD scheduler (version 4.3) uses a formula to calculate the current priority level of a job, basing it on how much CPU the process has used.</p>

<p>Some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow some user advice to help set priorities; for example, by using the command-line utility nice.</p>

<p><strong>Refined Rules</strong></p>

<ul>
  <li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t).</li>
  <li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li>
  <li>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).</li>
  <li>Rule 4: Once a job uses up its time allotment at a given level (re- gardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</li>
  <li>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</li>
</ul>

<h3 id="chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</h3>

<p><strong>0. Basic Idea</strong></p>

<p><strong>Proportional-share scheduler</strong>, also sometimes referred to as a <strong>fair-share scheduler</strong>. Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.</p>

<p><strong>Implementations</strong></p>

<ul>
  <li><strong>lottery</strong> scheduling, lottery uses randomness in a clever way to achieve proportional share</li>
  <li><strong>stride</strong> scheduling, stride does so deterministically</li>
</ul>

<p><strong>Application</strong></p>

<p>One is that such approaches do not particularly mesh well with I/O [AC97]; another is that they leave open the hard problem of ticket assignment, i.e., how do you know how many tickets your browser should be allocated?</p>

<p>As a result, proportional-share schedulers are more useful in domains where some of these problems (such as assignment of shares) are rela- tively easy to solve. For example, in a virtualized data centre.</p>

<p><strong>1. Lottery Scheduling</strong></p>

<p>The basic idea is quite simple: every so often, hold a lottery to determine which process should get to run next; processes that should run more often should be given more chances to win the lottery. One of the most beautiful aspects of lottery scheduling is its use of randomness.</p>

<p><strong>Advantage</strong></p>

<ul>
  <li>randomness
    <ul>
      <li>First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling.</li>
      <li>Second, random also is lightweight, requiring little state to track alternatives.</li>
      <li>Finally, random can be quite fast.</li>
    </ul>
  </li>
  <li>simplicity of implementation</li>
  <li>no global state</li>
</ul>

<p><strong>Disadvantage</strong></p>

<ul>
  <li>Hard to assign tickets to jobs</li>
  <li>Not deterministic. Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired outcome.</li>
</ul>

<p><strong>Ticket</strong></p>

<p>Tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question.</p>

<p><strong>Ticket Mechanisms</strong></p>

<p>Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways.</p>

<ul>
  <li>ticket currency</li>
  <li>ticket transfer</li>
  <li>ticket inflation</li>
</ul>

<p><strong>Implementation</strong></p>

<p>Probably the most amazing thing about lottery scheduling is the simplicity of its implementation.</p>

<ul>
  <li>a good random number generator to pick the winning ticket</li>
  <li>a data structure to track the processes of the system (e.g., a list)</li>
  <li>the total number of tickets.</li>
</ul>

<p><strong>2. Stride Scheduling</strong></p>

<p>a <strong>deterministic</strong> fair-share scheduler.</p>

<p>Respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. We call this value the <strong>stride</strong> of each process.</p>

<p>Jobs A, B, and C, with 100, 50, and 250 tickets. if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40.</p>

<p>Every time a process runs, we will increment a counter for it (called its <strong>pass</strong> value) by its stride to track its global progress. The scheduler then uses the stride and pass to determine which process should run next.</p>

<p>The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride.</p>

<p><strong>Advantage</strong></p>

<p>Lottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right at the end of each scheduling cycle.</p>

<p><strong>Disadvantage</strong></p>

<p>Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there. In this way, lottery makes it much easier to incorporate new processes in a sensible manner.</p>

<h3 id="chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</h3>

<p><em>TODO after reading Concurrency</em></p>

<h1 id="memory-virtualisation">Memory Virtualisation</h1>

<h2 id="address-space">Address Space</h2>

<h3 id="chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</h3>

<p><strong>Multiprogramming</strong> (多道程序), in which multiple processes were ready to run at a given time, and the OS would switch between them.</p>

<p><strong>Time sharing</strong>, One way to implement time sharing would be to run one process for a short while, giving it full access to all memory, then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process’s state, run it for a while, and thus implement some kind of crude sharing of the machine. Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows.</p>

<p><strong>Address space</strong></p>

<p>Address space, easy to use abstraction of physical memory, and it is the running program’s view of memory in the system. Understanding this fundamental OS ab- straction of memory is key to understanding how memory is virtualized.</p>

<p>When the OS does this, we say the OS is <strong>virtualizing memory</strong>.</p>

<p><strong>Goals</strong></p>

<p>The VM system is responsible for providing the illusion of a large, sparse, private address space to programs, which hold all of their instructions and data therein.</p>

<ul>
  <li>transparency</li>
  <li>efficiency</li>
  <li>protection (isolation)</li>
</ul>

<p><strong>EVERY ADDRESS YOU SEE IS VIRTUAL</strong></p>

<p>Any address you can see as a programmer of a user-level program is a virtual address, if you print out an address in a program, it’s a virtual one.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-every_address_you_see_is_virtual.png" alt="os-every_address_you_see_is_virtual.png" /></p>

<h3 id="chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</h3>

<p><strong>Types of Memory</strong></p>

<ul>
  <li><strong>stack memory</strong>, allocations and deallocations of it are managed implicitly by the compiler for you, the programmer.</li>
  <li><strong>heap memory</strong>, it is this need for long-lived memory, where all allocations and deallocations are explicitly handled by you, the programmer.</li>
</ul>

<p>Example</p>

<p><code>c
void func() {     int *x = (int *) malloc(sizeof(int));     ... }
</code></p>

<p>First, you might no- tice that both stack and heap allocation occur on this line: first the com- piler knows to make room for a pointer to an integer when it sees your declaration of said pointer (int *x); subsequently, when the program calls malloc(), it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program.</p>

<p><strong>API</strong></p>

<ul>
  <li><strong>malloc()</strong></li>
  <li><strong>free()</strong></li>
</ul>

<p>There are really two levels of memory management in the system. The first is level of memory management is performed by the OS, which hands out memory to processes when they run, and takes them back when processes exit (or otherwise die). The second level of management is within each process, for example within the heap when you call malloc() and free().</p>

<p>They are not system calls, but rather library calls. Thus the malloc library manages space within your virtual address space, but itself is built on top of some system calls.</p>

<ul>
  <li><strong>mmap()</strong></li>
</ul>

<p>You can also obtain memory from the operating system via the <code>mmap()</code> call. By passing in the correct arguments, mmap() can create an anonymous memory region within your program — a region which is not associated with any particular file but rather with swap space. This memory can then also be treated like a heap and managed as such.</p>

<ul>
  <li><strong>calloc()</strong></li>
</ul>

<p>Allocates memory and also zeroes it before returning; this prevents some errors where you assume that memory is zeroed and forget to initialize it yourself.</p>

<ul>
  <li><strong>realloc()</strong></li>
</ul>

<p>when you’ve allocated space for something (say, an array), and then need to add something to it: realloc() makes a new larger region of memory, copies the old region into it, and returns the pointer to the new region.</p>

<p><strong>Common Errors</strong></p>

<ul>
  <li>Forgetting To Allocate Memory - <strong>segmentation fault</strong>, which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY. Forget to allocate memory.</li>
  <li>Not Allocating Enough Memory - <strong>buffer overflow</strong></li>
  <li>Forgetting to Initialize Allocated Memory - <strong>uninitialized read</strong></li>
  <li>Forgetting To Free Memory - <strong>memory leak</strong></li>
  <li>Freeing Memory Before You Are Done With It - <strong>dangling pointer</strong></li>
  <li>Freeing Memory Repeatedly - <strong>double free</strong></li>
</ul>

<p><strong>Tools</strong></p>

<ul>
  <li><strong>gdb</strong>, add -g flag to gcc, then run it with gdb. eg. gcc -g null.c -o null -Wall &amp;&amp; gdb null</li>
  <li><strong>valgrind</strong>, eg. valgrind —leak-check=yes null</li>
</ul>

<h2 id="dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</h2>

<h3 id="chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</h3>

<p><strong>hardware-based address translation</strong></p>

<p>With address translation, the hardware transforms each memory access (e.g., an instruction fetch, load, or store), changing the <strong>virtual</strong> address provided by the instruction to a <strong>physical</strong> address where the desired information is actually located.</p>

<p>Transforming a virtual address into a physical address is exactly the technique we refer to as address translation.</p>

<p>Key to the efficiency of this technique is hardware support, which performs the translation quickly for each access, turning virtual addresses (the process’s view of memory) into physical ones (the actual view).</p>

<p><strong>Static (Software-based) Relocation</strong></p>

<p>A piece of software known as the loader takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memory.</p>

<p><strong>Dynamic (Hardware-based) Relocation</strong></p>

<p>The <strong>base and bounds</strong> technique is also referred to as dynamic relocation. With dynamic relocation, a little hardware goes a long way. Namely, a <strong>base</strong> register is used to transform virtual addresses (generated by the program) into physical addresses. A <strong>bounds</strong> (or <strong>limit</strong>) register ensures that such addresses are within the confines of the address space. Together they provide a simple and efficient virtualization of memory.</p>

<p>Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as dynamic relocation.</p>

<p>We should note that the base and bounds registers are hardware stru tures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the <strong>memory management unit (MMU)</strong>.</p>

<p><strong>Disadvantage</strong></p>

<p>The simple approach of using a base and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t fit into memory; thus, base and bounds is not as flexible as we would like.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-base_and_bounds.png" alt="os-base_and_bounds.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>The hardware should provide special instructions to modify the base and bounds registers, allowing the OS to change them when different processes run. These instructions are privileged; only in kernel (or privileged) mode can the registers be modified.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynaimic_relocation_hardware_requirement.png" alt="os-dynaimic_relocation_hardware_requirement.png" /></p>

<p><strong>Operating System Support</strong></p>

<p>The combination of hardware support and OS management leads to the implementation of a simple virtual memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_os_responsibility.png" alt="os-dynamic_relocation_os_responsibility.png" /></p>

<p><strong>Limited Direct Execution Protocol (Dynamic Relocation)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_LDE.png" alt="os-dynamic_relocation_LDE.png" /></p>

<h3 id="chapter-16-segmentation">Chapter 16 Segmentation</h3>

<p><strong>Segmentation: Generalized Base/Bounds</strong></p>

<p>Considering the disadvantage of the simple base and bounds, instead of having just one base and bounds pair in our <strong>MMU</strong>, why not <strong>have a base and bounds pair per logical segment of the address space</strong>? A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different segments: code, stack, and heap.</p>

<p>The hardware structure in our <strong>MMU</strong> required to support segmenta- tion is just what you’d expect: in this case, a set of three base and bounds register pairs.</p>

<p><strong>Advantage</strong></p>

<p>Remove the Inner Fragmentation.</p>

<p>What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation.png" alt="os-segmentation.png" /></p>

<p><strong>THE SEGMENTATION FAULT</strong></p>

<p>The term segmentation fault or violation arises from a memory access on a segmented machine to an illegal address. Humorously, the term persists, even on machines with no support for segmentation at all. Or not so humorously, if you can’t figure why your code keeps faulting</p>

<p><strong>Implementation</strong></p>

<p>One common approach, sometimes referred to as an explicit approach, is to chop up the address space into segments based on the top few bits of the virtual address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_implementation.png" alt="os-segmentation_implementation.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>Negative growth for stack, and protection bits for code sharing. (to save memory, sometimes it is useful to share certain memory segments between address spaces. In particular, <strong>code sharing</strong> is common and still in use in systems today.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_register_with_protection.png" alt="os-segmentation_register_with_protection.png" /></p>

<p><strong>Fine-grained vs. Coarse-grained Segmentation</strong></p>

<ul>
  <li>Coarse-grained, with just a few segments (i.e., code, stack, heap).</li>
  <li>Fine-grained, to consist of a large number smaller segments, with (further hardware support) a <strong>segment table</strong> of some kind stored in memory.</li>
</ul>

<p><strong>Disadvantage</strong></p>

<p>The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones. We call this problem <strong>external fragmentation</strong>.</p>

<p>Because segments are variablesized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be difficult. One can try to use smart algorithms or periodically compact memory, but the problem is fundamental and hard to avoid. (compact physical memory by rearranging the existing segments, is memory-intensive and generally uses a fair amount of processor time.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_compact_memory.png" alt="os-segmentation_compact_memory.png" /></p>

<p>Segmentation still isn’t flexible enough to support our fully generalized, sparse address space.</p>

<h3 id="chapter-17---free-space-management">Chapter 17 - Free-Space Management</h3>

<p>Managing free space can certainly be easy, as we will see when we discuss the concept of paging. It is easy when the space you are managing is divided into fixed-sized units; in such a case, you just keep a list of these fixed-sized units; when a client requests one of them, return the first entry.</p>

<p>Where free-space management becomes more difficult (and interesting) is when the free space you are managing consists of variable-sized units; this arises in a user-level memory-allocation library (as in malloc() and free()) and in an OS managing physical memory when using segmentation to implement virtual memory. In either case, the problem that exists is known as <strong>external fragmentation</strong>: the free space gets chopped into little pieces of different sizes and is thus fragmented; subsequent requests may fail because there is no single contiguous space that can satisfy the request, even though the total amount of free space exceeds the size of the request.</p>

<p><strong>Target</strong></p>

<p>The more you know about the exact workload presented to an <strong>allocator</strong>, the more you could do to tune it to work better for that workload.</p>

<p><strong>Assumptions</strong></p>

<p>Focus on the great history of allocators found in user-level memory-allocation libraries. The space that this library manages is known historically as the heap, and the geeric data structure used to manage free space in the heap is some kind of <strong>free list</strong>. This structure contains references to all of the free chunks of space in the managed region of memory.</p>

<p>Example</p>

<p>void free(void *ptr) takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when freeing the space, does not inform the library of its size; thus, the library must be able to figure out how big a chunk of memory is when handed just a pointer to it.</p>

<p><strong>Splitting and Coalescing</strong></p>

<ul>
  <li>The split is commonly used in allocators when requests are smaller than the size of any particular free chunk.</li>
  <li>Coalesce free space when a chunk of memory is freed.</li>
</ul>

<p><strong>Tracking The Size Of Allocated Regions</strong></p>

<p>To accomplish this task, most allocators store a little bit of extra information in a <strong>header</strong> block which is kept in memory, usually just before the handed-out chunk of memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewenndy.github.io/raw/source/image-repo/os-free_space_management_non_coalesced_free_list.png" alt="os-free_space_management_non_coalesced_free_list.png" /></p>

<h2 id="paging">Paging</h2>

<h3 id="chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</h3>

<p><strong>Background</strong></p>

<p>The operating system takes one of two approaches when solving most any space-management problem.</p>

<ol>
  <li>The first approach is to chop things up into <strong>variable-sized</strong> pieces, as we saw with segmenta- tion in virtual memory.</li>
  <li>To chop up space into <strong>fixed-sized</strong> pieces. In virtual memory, we call this idea paging.</li>
</ol>

<p><strong>Page vs. Page Frame</strong></p>

<ul>
  <li>From perspective of address space, the fixed-sized unit is called page.</li>
  <li>From perspective of physical space, the fixed-sized unit is called page frame.</li>
</ul>

<p>So, the address translation is to translate page to relevant page frame.</p>

<p><strong>32 bits vs. 64 bits</strong></p>

<p>Sometimes we say the OS is 32 bits or 64 bits, we may infer that</p>

<ul>
  <li>32 bits OS has 4GB address space</li>
  <li>64 bits OS has 10mGB address space</li>
</ul>

<p><strong>Advantage</strong></p>

<ul>
  <li>First, it does not lead to external fragmentation, as paging (by design) divides memory into fixed-sized units.</li>
  <li>Second, it is quite flexible, enabling the sparse use of virtual address spaces.</li>
</ul>

<p><strong>Translation</strong></p>

<p>To translate this virtual address that the process generated, we have to first split it into two components: the <strong>virtual page number (VPN)</strong>, and the <strong>offset</strong> within the page.</p>

<p>With our virtual page number, we can now index our page table, to get the <strong>physical frame number (PFN)</strong> (also sometimes called the <strong>physical page number or PPN</strong>).</p>

<p>Note the offset stays the same (i.e., it is not translated), because the offset just tells us which byte within the page we want.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_address_translation_process.png" alt="os-paging_address_translation_process.png" /></p>

<p><strong>Page Table</strong></p>

<p>The operating system usually keeps a per-process data structure known as a page table.</p>

<p>One of the most important data structures in the memory management subsystem of a modern OS is the page table. In general, a page table stores virtual-to-physical address translations</p>

<p>The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical addresses (physical frame numbers). The OS indexes the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_page_table.png" alt="os-paging_page_table.png" /></p>

<p><strong>Storage</strong></p>

<p>Because page tables are so big, we don’t keep any special on-chip hard- ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in memory somewhere.</p>

<p><strong>Page Table Entry (PTE)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_x86_pte_example.png" alt="os-paging_x86_pte_example.png" /></p>

<p><strong>Page Table Base Register (PTBR)</strong></p>

<p>PTBR contains the physical address of the starting location of the page table.</p>

<p>Code Example</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_access_memory_code_demo.png" alt="os-paging_access_memory_code_demo.png" /></p>

<h3 id="chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</h3>

<p><strong>Background</strong></p>

<p>Using paging as the core mechanism to support virtual memory can lead to high performance overheads. By chopping the address space into small, fixed-sized units (i.e., pages), paging requires a large amount of mapping information. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow.</p>

<p><strong>Translation Lookaside Buffer (TLB)</strong></p>

<p>To speed address translation, we are going to add what is called (for historical reasons) a <strong>translation-lookaside buffer</strong>, or <strong>TLB</strong>. A TLB is part of the chip’s <strong>memory-management unit (MMU)</strong>, and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an <strong>address-translation cache</strong>.</p>

<p><strong>Advantage</strong></p>

<p>By providing a small, dedicated on-chip TLB as an address-translation cache, most memory references will hopefully be handled without having to access the page table in main memory.</p>

<p><strong>Algorithm</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow.png" alt="os-paging_tlb_control_flow.png" /></p>

<p>Goal is to improve the TLB <strong>hit rate</strong>.</p>

<p><strong>TLB Content</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_content.png" alt="os-paging_tlb_content.png" /></p>

<p>TLB contains both VPN and PFN in each entry, in hardware terms, the TLB is known as a <strong>fully-associative</strong> cache.</p>

<p><strong>TLB Miss Handling</strong></p>

<p>Two answers are possible: the hardware, or the software (OS).</p>

<p>A modern system that uses <strong>software-managed TLBs</strong>. On a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler. As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow_os_handled.png" alt="os-paging_tlb_control_flow_os_handled.png" /></p>

<p><strong>Performance Matters</strong></p>

<p>Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties. The idea behind hardware caches is to take advantage of <strong>locality</strong> in instruction and data references. Hardware caches, whether for instructions, data, or address translations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory.</p>

<ol>
  <li><strong>spatial locality</strong>, the idea is that if a program accesses memory at address x, it will likely soon access memory near x.</li>
  <li><strong>temporal locality</strong>, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future.</li>
  <li>page size, why don’t we just make bigger caches and keep all of our data in them? Because any large cache by definition is slow, and thus defeats the purpose.</li>
</ol>

<p><strong>Issue 1: Context Switch</strong></p>

<p>Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.</p>

<ol>
  <li><strong>flush</strong> the TLB on context switches, thus emptying it before running the next process. But there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high.</li>
  <li><strong>address space identifier (ASID)</strong>, which you can think of the ASID as a process identifier (PID), to enable sharing of the TLB across context switches.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_with_asid.png" alt="os-paging_tlb_with_asid.png" /></p>

<p><strong>Issue 2: Replacement Policy</strong></p>

<p>When we are installing a new entry in the TLB, we have to replace an old one, which one to replace?</p>

<ul>
  <li><strong>least-recently-used (LRU)</strong></li>
  <li><strong>random policy</strong></li>
</ul>

<p>LRU tries to take advantage of locality in the memory-reference stream, and what the random policy exists for?</p>

<p>Random policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over n + 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better.</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>
    <p>Exceeding the TLB coverage, and it can be quite a problem for certain programs. Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed.</p>

    <p><strong>RAM isn’t always RAM</strong>. Sometimes randomly accessing your address space, particular if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties. Because one of our advisors, David Culler, used to always point to the TLB as the source of many performance problems, we name this law in his honor: <strong>Culler’s Law</strong>.</p>
  </li>
  <li>
    <p>TLB access can easily become a bottleneck in the CPU pipeline, in particular with what is called a <strong>physically-indexed cache</strong>. With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. A <strong>virtually-indexed cach</strong>e solves some performance problems, but introduces new issues into hardware design as well.</p>
  </li>
</ol>

<h3 id="note-on-cache-management">Note on Cache Management</h3>

<p>Define cache miss and hit, and goal is to improve the cache rate. Normally, better <strong>replacement policy</strong> lead to higher cache rate.</p>

<p><strong>Find the best replacement policy</strong></p>

<ul>
  <li>Find the optimal</li>
  <li>Find the easiest</li>
  <li>Improve toward optimal, considering Principle of Locality</li>
  <li>Think about corner case</li>
</ul>

<p><strong>Reference: Optimal Replacement Policy</strong></p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Reference: Easiest Replacement Policy</strong></p>

<p>Random policy, with an extraordinary advantage, can avoid corner case.</p>

<p><strong>Reference: Principle of Locality</strong></p>

<p>Programs tend to access certain code sequences (e.g., in a loop) and data structures (e.g., an array accessed by the loop) quite frequently.</p>

<ul>
  <li>spatial locality</li>
  <li>temporal locality, e.g., LRU</li>
  <li>operation expense, e.g., When swapping out pages, dirty pages are much more expensive</li>
</ul>

<p><strong>Reference: Types of Cache Misses</strong></p>

<p>In the computer architecture world, architects sometimes find it useful to characterize misses by type, into one of three categories, sometimes called the Three C’s.</p>

<ul>
  <li><strong>Compulsory miss</strong> (cold-start miss) occurs because the cache is empty to begin with and this is the first reference to the item.</li>
  <li><strong>Capacity miss</strong> occurs because the cache ran out of space and had to evict an item to bring a new item into the cache.</li>
  <li><strong>Conflict miss</strong> arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as set-associativity; it does not arise in the OS page cache because such caches are always fully-associative, i.e., there are no restrictions on where in memory a page can be placed.</li>
</ul>

<h3 id="chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</h3>

<p><strong>Crux</strong></p>

<p>How to get rid of all those invalid regions in the page table instead of keeping them all in memory?</p>

<p><strong>Background</strong></p>

<p>Page tables are t big and thus consume too much memory.</p>

<p>Assume again a 32-bit address space (2^32 bytes), with 4KB (2^12 byte) pages and a 4-byte page-table entry. An address space thus has roughly one million virtual pages in it ( 2^20 ); multiply by the page-table entry size and you see that our page table is 4MB in size. Recall also: we usually have one page table for every process in the system! With a hundred active processes (not uncommon on a modern system), we will be allocating hundreds of megabytes of memory just for page tables!</p>

<p><strong>Solution 1 - Bigger Pages</strong></p>

<p>Big pages lead to waste within each page, a problem known as internal fragmentation. Thus, most systems use relatively small page sizes in the common case: 4KB (as in x86).</p>

<p><strong>Solution 2 - Hybrid Approach: Paging and Segments</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_hybrid_approach.png" alt="os-paging_tlb_hybrid_approach.png" /></p>

<p><strong>Algorithm</strong></p>

<p>Instead of having a single page table for the entire address soopace of the process, have one per logical segment. In this example, we might thus have three page tables.</p>

<p>Remember with segmentation, we had a <strong>base</strong> register that told us where each segment lived in physical memory, and a <strong>bound</strong> or limit register that told us the size of said segment.</p>

<ol>
  <li>Each logical segment (code, stack, and heap) has one page table.</li>
  <li>Each segment has one pair of base and bounds resisters.</li>
  <li>Base register points to the page table of the segment, and bounds is used to indicate the end of the page table.</li>
</ol>

<p><strong>Advantage</strong></p>

<p>In this manner, our hybrid approach realizes a significant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>It still requires us to use segmentation, as it assumes a certain usage pattern of the address space; if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste.</li>
  <li>This hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, finding free space for them in memory is more complicated.</li>
</ol>

<p><strong>Solution 3 - Multi-level Page Tables</strong></p>

<p>It turns the linear page table into something like a tree (<strong>page directory</strong>). This approach is so effective that many modern systems employ it (e.g., x86).</p>

<p><strong>Algorithm</strong></p>

<p>First, chop up the page table into page-sized units; if an entire page of page-table entries (PTEs) is invalid, don’t allocate that page of the page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory. The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.</p>

<p>The page directory, in a simple two-level table, contains one entry per page of the page table. It consists of a number of <strong>page directory entries (PDE)</strong>. A PDE (minimally) has a <strong>valid bit</strong> <strong>and a page frame number (PFN)</strong>, similar to a PTE.</p>

<p>VA contains VPN and offset, and VPN can be splitted into <strong>page directory index</strong> and <strong>page table index</strong>.</p>

<ol>
  <li>Use <strong>page directory index</strong> to search page directory, to get <strong>page directory entry</strong>, to get <strong>page frame number</strong>, to get the specific <strong>page table</strong>.</li>
  <li>Use <strong>page table index</strong> to search the page table, to get <strong>page table entry</strong>, to get the real <strong>physical frame number</strong>.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo.png" alt="os-paging_multi_level_page_table_demo.png" /></p>

<p>Demo code</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo_code.png" alt="os-paging_multi_level_page_table_demo_code.png" /></p>

<p><strong>Advantage</strong></p>

<ol>
  <li>The multi-level table only allocates page-table space in proportion to the amount of address space you are usig; thus it is generally compact and supports sparse address spaces.</li>
  <li>
    <p>If carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page table.</p>

    <p>Contrast this to a simple (non-paged) linear page table, for a large page table (say 4MB), finding such a large chunk of unused contiguous free physical memory can be quite a challenge. With a multi-level structure, the indirection allows us to place page-table pages wherever we would like in physical memory.</p>
  </li>
</ol>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>Time-space trade-off. It should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself).</li>
  <li>Another obvious negative is complexity. Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubt- nedly more involved than a simple linear page-table lookup.</li>
</ol>

<p><strong>Example</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example.png" alt="os-paging_multi_level_page_table_example.png" /></p>

<p>Virtual Address format</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_va.png" alt="os-paging_multi_level_page_table_example_va.png" /></p>

<p>Explanation</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_explanation.png" alt="os-paging_multi_level_page_table_example_explanation.png" /></p>

<p><strong>Issues</strong></p>

<p><strong><em>What if the page directory gets too big?</em></strong></p>

<p>Make it more than two levels, add index to page directory index.</p>

<p><strong><em>How to make it extreme space savings?</em></strong></p>

<p>Inverted page tables. Instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each physical page of the system. The entry tells us which process is using this page, and which virtual page of that process maps to this physical page.</p>

<p>A hash table is often built over the base structure to speed lookups.</p>

<p><strong><em>How to choose page table size?</em></strong></p>

<p>In a memory-constrained system (like many older systems), small structures make sense; in a system with a reasonable amount of memory and with workloads that actively use a large number of pages, a bigger table that speeds up TLB misses might be the right choice.</p>

<p><strong><em>What if the page tables are too big to fit into memory all at once?</em></strong></p>

<p>Thus far, we have assumed that page tables reside in kernel-owned physical memory. Some systems place such page tables in <strong>kernel virtual memory</strong>, thereby allowing the system to swap some of these page tables to disk when memory pressure gets a little tight.</p>

<h2 id="beyond-physical-memory">Beyond Physical Memory</h2>

<h3 id="chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</h3>

<p><strong>Background</strong></p>

<p>In fact, we’ve been assuming that every address space of every running process fits into memory. We will now relax these big assumptions, and assume that we wish to support many concurrently-running large address spaces.</p>

<p>To support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren’t in great demand. In modern systems, this role is usually served by a hard disk drive.</p>

<p><strong>Mechanism</strong></p>

<p>To do so requires more complexity in page-table structures, as a <strong>present bit</strong> (of some kind) must be included to tell us whether the page is present in memory or not. When not, the operating system <strong>page-fault handler</strong> runs to service the <strong>page fault</strong>, and thus arranges for the transfer of the desired page from disk to memory, perhaps first replacing some pages in memory to make room for those soon to be swapped in.</p>

<p><strong>Swap Space</strong></p>

<p>To reserve some space on the disk for moving pages back and forth. We will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to remember the <strong>disk address</strong> of a given page (PTE).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_example.png" alt="os-swap_example.png" /></p>

<p>The size of the swap space is important, as ultimately it determines the <strong>maximum number of memory pages</strong> that can be in use by a system at a given time.</p>

<p>We should note that swap space is not the only on-disk location for swapping traffic.</p>

<blockquote>
  <p>For example, assume you are running a program binary (e.g., ls, or your own compiled main program). The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution, or, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re-use the memry space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system.</p>
</blockquote>

<p><strong>Present Bit</strong></p>

<p>OS use this piece of information in each page-table entry to flag if the page is in physical memory or swap space.</p>

<p>If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above; if it is set to zero, the page is not in memory but rather on disk somewhere.</p>

<p><strong>Page Faut</strong></p>

<p>The act of accessing a page that is not in physical memory is commonly referred to as a <strong>page fault</strong> (it should be called a <strong>page miss</strong>. But when something the hardware doesn’t know how to handle occurs, the hardware simply transfers control to the OS. In perspective of the hardware it is a page fault).</p>

<p><strong>Page Fault Handler</strong></p>

<p>Upon a page fault, the OS is invoked to service the page fault. A particular piece of code, known as a <strong>page-fault handler</strong>, runs, and must service the page fault.</p>

<p>The appropriately-named <strong>OS page-fault handler</strong> runso to determine what to do. Virtually all systems handle page faults in software; even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.</p>

<p><strong>Page Fault Control Flow</strong></p>

<p>Hardware</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow.png" alt="os-swap_page_fault_control_flow.png" /></p>

<p>Software</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow_software.png" alt="os-swap_page_fault_control_flow_software.png" /></p>

<p>How to handle or how will the OS know where to find the desired page?</p>

<ol>
  <li>The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.</li>
  <li>When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN field of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction.</li>
  <li>Then generate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB when servicing the page fault to avoid this step)</li>
  <li>Finally, a last restart would find the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address.</li>
</ol>

<p>Note that while the I/O is in flight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes while the page fault is being serviced.</p>

<p><strong><em>What If Memory Is Full?</em></strong></p>

<p>OS might like to first page out one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or replace is known as the <strong>page-replacement policy</strong>.</p>

<p><strong><em>When Replacements Really Occur?</em></strong></p>

<p>There are many reasons for the OS to keep a small portion of memory free more proactively. To keep a small amount of memory free, most operating systems thus have some kind of <strong>high watermark (HW)</strong> and <strong>low watermark (LW)</strong> to help decide when to start evicting pages from memory.</p>

<p>When the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The background thread, sometimes called the <strong>swap daemon</strong> or <strong>page daemon</strong>, then goes to sleep, happy that it has freed some memory for running processes and the OS to use.</p>

<p>So, instead of performing a replacement directly, the algorithm would instead simply check if there are any free pages available. If not, it would inform the <strong>page daemon</strong> that free pages are needed; when the thread frees up some pages, it would re-awaken the original thread, which could then page in the desired page and go about its work.</p>

<p><strong><em>How To Make Replacement Efficient?</em></strong></p>

<p>Many systems will cluster or group a number of pages and write them out at once to the swap partition, thus increasing the efficiency of the disk.</p>

<h3 id="chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</h3>

<p><strong>Background</strong></p>

<p>In such a case, this memory pressure forces the OS to start <strong>paging out</strong> pages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the <strong>replacement policy</strong> of the OS.</p>

<p><strong>Cache Management</strong></p>

<p>Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. And our goal as maximizing the number of <strong>cache hits</strong>.</p>

<p>Knowing the number of cache hits and misses let us calculate the <strong>average memory access time (AMAT)</strong> for a program.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_amat.png" alt="os-replacement_amat.png" /></p>

<p>Example</p>

<p>Suppose T(M) = 100ns (10^-7), T(D) = 10ms (10^-2)</p>

<ul>
  <li>P(Hit) = 90%, P(Miss) = 10%, AMAT = 1ms + 90ns</li>
  <li>P(Hit) = 99.9%, P(Miss) = 0.1%, AMAT = 0.01ms + 99.9ns</li>
</ul>

<p>The cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs.</p>

<p><strong>Polices</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_summary.png" alt="os-replacement_summary.png" /></p>

<p><strong>Policy 1. Optimal Replacement Policy</strong></p>

<p>Replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses.</p>

<p>In the development of scheduling policies, the future is not generally known; you can’t build the optimal policy for a general-purpose operating system.</p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Policy 2. FIFO</strong></p>

<p>Normal efficiency, easy to implement, and has corner case.</p>

<p>In some cases, when increasing the cache size, hit rate may get lower. This odd behavior is generally referred to as <strong>Belady’s Anomaly</strong>.</p>

<p><strong>Policy 3. Random</strong></p>

<p>Normal efficiency, easy to implement, but remember, it can avoid corner case.</p>

<p><strong>Policy 4. LRU</strong></p>

<p>LRU has what is known as a stack property. When increasing the cache size, hit rate will either stay the same or improve.</p>

<p><strong>Comparison with Workload</strong></p>

<p>No locality workload</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_no_locality_workload.png" alt="os-replacement_no_locality_workload.png" /></p>

<p>The 80-20 Workload, 80% of the references are made to 20% of the pages (the “hot” pages).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload.png" alt="os-replacement_80_20_workload.png" /></p>

<p>The Looping-Sequential Workload</p>

<p>Looping sequential workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, …, up to page 49, and then we lp, repeating those accesses.</p>

<p>It represents a worst-case for both LRU and FIFO, but no influence on Random. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_looping_sequential_workload.png" alt="os-replacement_looping_sequential_workload.png" /></p>

<p><strong>Implementation - Approximating LRU</strong></p>

<p>To keep track of which pages have been least- and most-recently used, the system has to do some accounting work on every memory reference. Unfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive.</p>

<p>Idea</p>

<p>Approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a <strong>use bit</strong> (sometimes called the <strong>reference bit</strong>).</p>

<ul>
  <li>Whenever a page is referenced (i.ooe., read or written), the use bit is set by hardware to 1.</li>
  <li>The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS.</li>
</ul>

<p>Implementation by Clock Algorithm</p>

<ul>
  <li>Imagine all the pages of the system arranged in a circular list. A clock hand points to some particular page to begin with.</li>
  <li>When a replacement must occur, the OS iterating the circular list checking on use bit.
    <ul>
      <li>If 1, clear use bit to 0, and find next</li>
      <li>If 0, use it</li>
    </ul>
  </li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload_with_clock.png" alt="os-replacement_80_20_workload_with_clock.png" /></p>

<p><strong>Considering Dirty Pages</strong></p>

<p>Consider the locality by the expense on swapping out pages.</p>

<ul>
  <li>If a page has been <strong>modified</strong> and is thus <strong>dirty</strong>, it must be written back to disk to evict it, which is expensive.</li>
  <li>If it has not been modified (and is thus clean), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O.
Idea</li>
</ul>

<p>To support this behavior, the hardware should include a <strong>modified bit</strong> (a.k.a. <strong>dirty bit</strong>).</p>

<p>Implementation by Clock Algorithm</p>

<p>The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; failing to find those, then for unused pages that are dirty, and so forth.</p>

<p><strong>Other VM Policies</strong></p>

<p><strong><em>When the OS bring a page into memory?</em></strong></p>

<p>Page selection policy. The OS simply uses <strong>demand paging</strong>, which means the OS brings the page into memory when it is accessed, “on demand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as <strong>prefetching</strong>.</p>

<p><strong><em>How the OS writes pages out to disk?</em></strong></p>

<p>Any systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write. This behavior is usually called <strong>clustering</strong> or simply <strong>grouping</strong> of writes, and is effective because of the nature of disk drives.</p>

<p><strong><em>What about
 the memory demands of the set of running processes simply exceeds the available physical memory? (condition sometimes referred to as thrashing)</em></strong></p>

<p>Given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes working sets (the pages that they are using actively) fit in memory and thus can make progress. This approach, generally known as <strong>admission control</strong>, states that it is sometimes better to do less work well than to try to do everything at once poorly.</p>

<p>Some versions of Linux run an <strong>out-of-memory killer</strong> when memory is oversubscribed; this daemon chooses a memory- intensive process and kills it, thus reducing memory in a none-too-subtle manner.</p>

<h3 id="chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</h3>

<p><strong>Background</strong></p>

<p>The VAX-11 minicomputer architecture was introduced in the late 1970’s by Digital Equipment Corporation (DEC).</p>

<p>As an additional issue, VMS is an excellent example of software innovations used to hide some of the inheret flaws of the architecture.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-vax_vms_address_space.png" alt="os-vax_vms_address_space.png" /></p>

<p><strong>Reduce Page Table Pressure</strong></p>

<p>First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process; thus, no page-table space is needed for the unused portion of the address space between the stack and the heap.</p>

<p>Second, the OS reduces memory pressure even further by placing user page tables (for P0 and P1, thus two per process) in kernel virtual memory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S. If memory comes undersevere pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.</p>

<p><strong>Replacement policy: Segmented FIFO with Page Clustering</strong></p>

<p>Each process has a maximum number of pages it can keep in memory, known as its <strong>residentn set size (RSS)</strong>. Each of these pages is kept on a FIFO list; when a process exceeds its RSS, the “first-in” page is evicted. FIFO clearly does not need any support from the hardware (no use bit), and is thus easy to implement.</p>

<p>To improve FIFO’s performance, VMS introduced two <strong>second-chance lists</strong> where pages are placed before getting evicted from memory, specifically a global clean-page free list and dirty-page list. The bigger these global second-chance lists are, the closer the segmented FIFO algorithm performs to LRU.</p>

<p>Clustering is used in most modern systems, as the freedom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve performance.</p>

<p><strong>Optimisation: Be Lazy</strong></p>

<p>Laziness can put off work until later, which is beneficial within an OS for a number of reasons.</p>

<ul>
  <li>First, putting off work might reduce the latency of the current operation, thus improving responsiveness; for example, operating systems often report that writes to a file succeeded immediately, and only write them to disk later in the background.</li>
  <li>Second, and more importantly, laziness sometimes obviates the need to do the work at all; for example, delaying a write until the file is deleted removes the need to do the write at all.</li>
</ul>

<p><strong>Lazy Optimisation: Demanding Zero</strong></p>

<p>With demand zeroing, the OS instead does very little work when the page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or writes the page, a trap into the OS takes place. When handling the trap, the OS notices that this is actually a demand-zero page; at this point, the OS then does the needed work of finding a physical page, zeroing it, and mapping it into the process’s address space. If the process never accesses the page, all of this work is avoided, and thus the virtue of demand zeroing.</p>

<p><strong>Lazy Optimisation: Copy-on-write</strong></p>

<p>When the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces.</p>

<ul>
  <li>If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.</li>
  <li>If, however, one of the address spaces does indeed try to write to the page, it will trap into the OS. The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process. The process then continues and now has its own private copy of the page.</li>
</ul>

<p>In UNIX systems, COW is even more critical, due to the semantics of <code>fork()</code> and <code>exec()</code>. <code>fork()</code> creates an exact copy of the address space of the caller; with a large address space, making such a copy is slow and data intensive. Even worse, most of the address space is immediately over-written by a subsequent call to <code>exec()</code>, which overlays the calling process’s address space with that of the soon-to-be-exec’d program. By instead performing a copy-on-write <code>fork()</code>, the OS avoids much of the needless copying and thus retains the correct semantics while improving performance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Guidance from POODR]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/01/29/guidance-from-poodr/"/>
    <updated>2015-01-29T15:20:13+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/01/29/guidance-from-poodr</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Practical Object Oriented Design in Ruby</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td>Sandi Metz</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://www.poodr.com/">www.poodr.com</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#object-oriented-design">Object-Oriented Design</a>    <ul>
      <li><a href="#the-tools-of-design">The Tools of Design</a>        <ul>
          <li><a href="#design-principles">Design Principles</a></li>
          <li><a href="#design-patterns">Design Patterns</a></li>
        </ul>
      </li>
      <li><a href="#the-act-of-design">The Act of Design</a>        <ul>
          <li><a href="#how-design-fails">How Design Fails</a></li>
          <li><a href="#when-to-design">When to Design</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#guidance">Guidance</a>    <ul>
      <li><a href="#designing-classes-with-a-single-responsibility">Designing Classes with a Single Responsibility</a>        <ul>
          <li><a href="#depend-on-behavior-not-data">Depend on Behavior, Not Data</a></li>
          <li><a href="#enforce-single-responsibility-everywhere">Enforce Single Responsibility Everywhere</a></li>
        </ul>
      </li>
      <li><a href="#manage-dependencies">Manage Dependencies</a>        <ul>
          <li><a href="#inject-dependencies">Inject Dependencies</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#after">after</a>    <ul>
      <li><a href="#isolate-dependencies">Isolate Dependencies</a></li>
      <li><a href="#remove-argument-order-dependencies">Remove Argument-Order Dependencies</a></li>
      <li><a href="#managing-dependency-direction">Managing Dependency Direction</a></li>
      <li><a href="#creating-flexible-interfaces">Creating Flexible Interfaces</a>        <ul>
          <li><a href="#finding-the-public-interface">Finding the Public Interface</a>            <ul>
              <li><a href="#focus-messages-between-domain-objects">Focus Messages between Domain Objects</a></li>
              <li><a href="#use-sequence-diagrams">Use Sequence Diagrams</a></li>
              <li><a href="#asking-for-what-instead-of-telling-how">Asking for “What” Instead of Telling “How”</a></li>
              <li><a href="#seeking-contect-independence">Seeking Contect Independence</a></li>
            </ul>
          </li>
          <li><a href="#the-law-of-demeter">The Law of Demeter</a></li>
        </ul>
      </li>
      <li><a href="#reductin-costs-with-duck-typing">Reductin Costs with Duck Typing</a>        <ul>
          <li><a href="#polymorphism">Polymorphism</a></li>
          <li><a href="#recognizing-hidden-ducks">Recognizing Hidden Ducks</a></li>
          <li><a href="#guidance-1">Guidance</a></li>
        </ul>
      </li>
      <li><a href="#acquiring-behavior-through-inheritance">Acquiring Behavior Through Inheritance</a>        <ul>
          <li><a href="#inheritance">Inheritance</a></li>
          <li><a href="#recognizing-where-to-use-inheritance">Recognizing Where to Use Inheritance</a>            <ul>
              <li><a href="#finding-the-abstraction">Finding the Abstraction</a></li>
            </ul>
          </li>
          <li><a href="#using-templage-methods">Using Templage Methods</a>            <ul>
              <li><a href="#template-method">Template Method</a></li>
              <li><a href="#implementing-every-template-method">Implementing Every Template Method</a></li>
            </ul>
          </li>
          <li><a href="#manging-coupling">Manging Coupling</a>            <ul>
              <li><a href="#decoupling-subclasses-using-hook-messages">Decoupling Subclasses Using Hook Messages</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#sharing-role-behavior-with-modules">Sharing Role Behavior with Modules</a>        <ul>
          <li><a href="#understanding-roles">Understanding Roles</a></li>
          <li><a href="#writing-inheritable-code">Writing Inheritable Code</a>            <ul>
              <li><a href="#recognize-the-antipatterns">Recognize the Antipatterns</a></li>
              <li><a href="#insist-on-the-abstraction">Insist on the Abstraction</a></li>
              <li><a href="#honor-the-contract">Honor the Contract</a></li>
              <li><a href="#use-the-template-method-pattern">Use the Template Method Pattern</a></li>
              <li><a href="#preemptively-decouple-classes">Preemptively Decouple Classes</a></li>
              <li><a href="#create-shallow-hierarchies">Create Shallow Hierarchies</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#combining-objects-with-composition">Combining Objects with Composition</a>        <ul>
          <li><a href="#aggregation-a-special-kind-of-composition">Aggregation: A Special Kind of Composition</a></li>
          <li><a href="#deciding-between-inheritance-and-composition">Deciding Between Inheritance and Composition</a>            <ul>
              <li><a href="#inheritance-1">Inheritance</a></li>
              <li><a href="#composition">Composition</a></li>
            </ul>
          </li>
          <li><a href="#guidance-2">Guidance</a></li>
        </ul>
      </li>
      <li><a href="#designing-cost-effective-tests">Designing Cost-Effective Tests</a>        <ul>
          <li><a href="#intentional-testing">Intentional Testing</a>            <ul>
              <li><a href="#knowing-your-intentions">Knowing Your Intentions</a></li>
              <li><a href="#knowing-what-to-test">Knowing What to Test</a>                <ul>
                  <li><a href="#remove-the-duplicate">Remove the Duplicate</a></li>
                  <li><a href="#message-model">Message Model</a></li>
                </ul>
              </li>
              <li><a href="#knowing-when-to-test">Knowing When to Test</a></li>
              <li><a href="#knowing-how-to-test">Knowing How to Test</a></li>
            </ul>
          </li>
          <li><a href="#testing-incoming-messages">Testing Incoming Messages</a></li>
          <li><a href="#testing-private-methods">Testing Private Methods</a></li>
          <li><a href="#testing-outgoing-messages">Testing Outgoing Messages</a></li>
          <li><a href="#testing-duck-types">Testing Duck Types</a></li>
          <li><a href="#testing-inherited-code">Testing Inherited Code</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="object-oriented-design">Object-Oriented Design</h1>

<p>Object-oriented design (OOD) requires that you shift from thinking of the world as a collection of predefined procedures to modeling the world as a series of messages that pass between objects.</p>

<p>Object-oriented applications are made up of parts that interact to produce the behavior of the whole. The parts are <em>objects</em>; interactions are embodied in the <em>messages</em> that pass between them.</p>

<p>Object-oriented design is about managing dependencies. In the absence of design, unmanaged dependencies wreak havoc because objects know too much about one another.</p>

<p>Design is thus an art, the art of arranging code, and design is more the art of preserving changeability than it is the act of achieving perfection. You must not only write code for the feature you plan to deliver today, you must also create code that is amenable to being changed later. It doesn’t guess the future; it preserves your options for accommodating the future. It doesn’t choose; it leaves you room to move.</p>

<p>The trick to getting the most bang for your design buck is to acquire an understanding of the theories of design and to apply these theories appropriately, at the right time, and in the right amounts. </p>

<p>Well-designed applications are constructed of reusable code. Small, trustworthy self- contained objects with minimal context, clear interfaces, and injected dependencies are inherently reusable.</p>

<h2 id="the-tools-of-design">The Tools of Design</h2>

<h3 id="design-principles">Design Principles</h3>

<ul>
  <li>
    <p><strong>SOLID</strong></p>

    <ul>
      <li>Single Responsibility</li>
      <li>Open-Closed</li>
      <li>Liskov Substitution</li>
      <li>Interface Segregation</li>
      <li>Dependency Inversion</li>
    </ul>
  </li>
  <li><strong>DRY</strong>, Don’t Repeat Yourself</li>
  <li><strong>LoD</strong>, Law of Demeter</li>
</ul>

<h3 id="design-patterns">Design Patterns</h3>

<p>by Gof</p>

<h2 id="the-act-of-design">The Act of Design</h2>

<h3 id="how-design-fails">How Design Fails</h3>

<ul>
  <li>Lack of it. Successful but undesigned applications carry the seeds of their own destruction; they are easy to write but gradually become impossible to change. “Yes, I can add that feature, but it will break everything.”</li>
  <li>Overdesign. Aware of OO design techniques but do not yet understand how to apply them. “No, I can’t add that feature; it wasn’t designed to do that.”</li>
  <li>Seperated from the act of programming. Design is a process of progressive discovery that relies on a feedback loop. The iterative techniques of the Agile software movement are thus perfectly suited to the creation of well-designed OO applications. The iterative nature of Agile development allows design to adjust regularly and to evolve naturally. </li>
</ul>

<h3 id="when-to-design">When to Design</h3>

<blockquote>
  <p>Agile believes that your customers can’t define the software they want before seeing it, so it’s best to show them sooner rather than later. If this premise is true, then it logically follows that you should build software in tiny increments, gradually iterating your way into an application that meets the customer’s true need. The Agile experience is that this collaboration produces software that differs from what was initially imagined; the resulting software could not have been anticipated by any other means. </p>
</blockquote>

<p>If Agile is correct, then</p>

<ol>
  <li>there is absolutely no point in doing a Big Up Front Design (BUFD) (because it cannot possibly be correct)</li>
  <li>no one can predict when the application will be done (because you don’t know in advance what it will eventually do)</li>
</ol>

<p>Agile processes guarantee change and your ability to make these changes depends on your application’s design. If you cannot write well-designed code you’ll have to rewrite your application during every iteration.</p>

<h1 id="guidance">Guidance</h1>

<p>Focus on object,</p>

<ul>
  <li>Single <strong>Responsibility</strong></li>
  <li>Manage <strong>Dependencies</strong></li>
</ul>

<p>Focus on message,</p>

<ul>
  <li><strong>Interface</strong>, creating flexible interfaces</li>
  <li><strong>Duck Typing</strong>, reducing costs with Duck Typing</li>
  <li><strong>Inheritance</strong>, acquiring behavior through inheritance</li>
  <li><strong>Module</strong>, sharing <strong>role</strong> behavior with modules</li>
  <li><strong>Composition</strong>, combining objects with composition</li>
  <li><strong>Tests</strong>, designing cost-effective tests</li>
</ul>

<h2 id="designing-classes-with-a-single-responsibility">Designing Classes with a Single Responsibility</h2>

<p>SRP requires that a class be <strong>cohesive</strong>, that everything in a class is related to its central purpose, the class is said to be highly cohesive or to have a single responsibility.</p>

<h3 id="depend-on-behavior-not-data">Depend on Behavior, Not Data</h3>

<p>“Don’t Repeat Yourself” (DRY) is a shortcut for this idea.</p>

<ul>
  <li>Hide instance variables</li>
  <li>Hide data structures</li>
</ul>

<h3 id="enforce-single-responsibility-everywhere">Enforce Single Responsibility Everywhere</h3>

<ul>
  <li>
    <p>Extract extra responsibilities from methods</p>

    <p>Methods, like classes, should have a single responsibility. All of the same reasons apply; having just one responsibility makes them easy to change and easy to reuse.</p>
  </li>
  <li>
    <p>Isolate extra responsibilities in classes</p>

    <p>Postponing decisions until you are absolutely forced to make them. Any decision you make in advance of an explicit requirement is just a guess. Don’t decide; preserve your ability to make a decision later.</p>
  </li>
</ul>

<h2 id="manage-dependencies">Manage Dependencies</h2>

<p>To collaborate, an object must know something know about others. <em>Knowing</em> creates a dependency, or <em>coupling</em> creates a dependency.</p>

<p>Dependency management is core to creating future-proof applications.</p>

<p>An object has a dependency when it knows</p>

<ul>
  <li>The name of another class.</li>
  <li>The name of a message that it intends to send to someone other than self.</li>
  <li>The arguments that a message requires.</li>
  <li>The order of those arguments.</li>
  <li>Knowing the name of a message you plan to send to someone other than self.</li>
  <li>Tests on code.</li>
</ul>

<h3 id="inject-dependencies">Inject Dependencies</h3>

<p>```ruby
# before
class Gear
  …</p>

<p>def gear_inches
    ratio * Wheel.new(rim, tire).diameter
  end
end</p>

<h1 id="after">after</h1>
<p>class Gear
  attr_reader :chainring, :cog, :wheel
  def initialize(chainring, cog, wheel)
    @chainring = chainring
    @cog       = cog
    @wheel     = wheel
  end</p>

<p>def gear_inches
    ratio * wheel.diameter
  end
end
``` </p>

<p>Gear previously had explicit dependencies on the Wheel class and on the type and order of its initialization arguments, but through injection these dependencies have been reduced to a single dependency on the diameter method.</p>

<h3 id="isolate-dependencies">Isolate Dependencies</h3>

<p><strong>Isolate Instance Creation</strong></p>

<p>If you are so constrained that you cannot change the code to inject a Wheel into a Gear, you should isolate the creation of a new Wheel inside the Gear class.</p>

<p>```ruby
class Gear
  …</p>

<p>def gear_inches
    ratio * wheel.diameter
  end</p>

<p>def wheel
    @wheel ||= Wheel.new(rim, tire)
  end
```</p>

<ul>
  <li>Isolate Vulnerable External Messages</li>
</ul>

<p>External messages, that is, messages that are “sent to someone other than self.”</p>

<p>```ruby
class Gear
  …</p>

<p>def gear_inches
    ratio * diameter
  end</p>

<p>def diameter
    wheel.diameter
  end
```</p>

<h3 id="remove-argument-order-dependencies">Remove Argument-Order Dependencies</h3>

<ul>
  <li>Use Hashes for Initialization Arguments</li>
  <li>Explicitly Define Defaults</li>
  <li>Isolate Multiparameter Initialization, use a wrapper.</li>
</ul>

<h3 id="managing-dependency-direction">Managing Dependency Direction</h3>

<p>Depend on things that change less often than you do.</p>

<ul>
  <li>Some classes are more likely than others to have changes in requirements.</li>
  <li>Concrete classes are more likely to change than abstract classes.</li>
  <li>Changing a class that has many dependents will result in widespread consequences.</li>
</ul>

<p>Depend on abstractions.</p>

<h2 id="creating-flexible-interfaces">Creating Flexible Interfaces</h2>

<blockquote>
  <p>Interface within a class, make up its public interface.</p>
</blockquote>

<p>Public Interfaces</p>

<ul>
  <li>Reveal its primary responsibility</li>
  <li>Are expected to be invoked by others</li>
  <li>Will not change on a whim</li>
  <li>Are safe for others to depend on</li>
  <li>Are thoroughly documented in the tests</li>
</ul>

<p>Private Interfaces</p>

<ul>
  <li>Handle implementation details</li>
  <li>Are not expected to be sent by other objects</li>
  <li>Can change for any reason whatsoever</li>
  <li>Are unsafe for others to depend on</li>
  <li>May not even be referenced in the tests</li>
</ul>

<p>Well-defined public interfaces consist of stable methods that expose the responsibilities of their underlying classes (public methods should read like a description of responsibilities).</p>

<h3 id="finding-the-public-interface">Finding the Public Interface</h3>

<h4 id="focus-messages-between-domain-objects">Focus Messages between Domain Objects</h4>

<p>Nouns in the application that have both data and behavior are called domain objects. Domain objects are easy to find but they are not at the design center of your application. Design experts notice domain objects without concentrating on them; they focus not on these objects but on the messages that pass between them.</p>

<h4 id="use-sequence-diagrams">Use Sequence Diagrams</h4>

<p>They explicitly specify the messages that pass between objects, and because objects should only communicate using public interfaces, sequence diagrams are a vehicle for exposing, experimenting with, and ultimately defining those interfaces.</p>

<h4 id="asking-for-what-instead-of-telling-how">Asking for “What” Instead of Telling “How”</h4>

<h4 id="seeking-contect-independence">Seeking Contect Independence</h4>

<p>The best possible situation is for an object to be completely independent of its context. An object that could collaborate with others without knowing who they are or what they do could be reused in novel and unanticipated ways.</p>

<p>The technique for collaborating with others without knowing who they are—dependency injection. </p>

<h3 id="the-law-of-demeter">The Law of Demeter</h3>

<p>It prohibits routing a message to a third object via a second object of a different type. “Only talk to your immediate neighbors” or “use only one dot.”</p>

<p>Delegation is tempting as a solution to the Demeter problem because it removes the visible evidence of violations.</p>

<p>Listening to Demeter means paying attention to your point of view. If you shift to a message-based perspective, the messages you find will become public interfaces in the objects they lead you to discover. However, if you are bound by the shackles of existing domain objects, you’ll end up assembling their existing public interfaces into long message chains and thus will miss the opportunity to find and construct flexible public interfaces.</p>

<h2 id="reductin-costs-with-duck-typing">Reductin Costs with Duck Typing</h2>

<blockquote>
  <p>Interface, across classes and is independent of any single class. The interface represents a set of messages where the messages themselves define the interface. It’s almost as if the interface defines a virtual class; that is, any class that implements the required methods can act like the interface kind of thing.</p>
</blockquote>

<p><strong>Duck types</strong> are public interfaces that are not tied to any specific class. These across-class interfaces add enormous flexibility to your application by replacing costly dependencies on class with more forgiving dependencies on messages.</p>

<h3 id="polymorphism">Polymorphism</h3>

<p><strong>Polymorphism</strong> in OOP refers to the ability of many different objects to respond to the same message. Senders of the message need not care about the class of the receiver; receivers supply their own specific version of the behavior. Polymorphic methods honor an implicit bargain; they agree to be inter- changeable from the sender’s point of view.</p>

<p>A single message thus has many (poly) forms (morphs).</p>

<p>There are a number of ways to achieve polymorphism:</p>

<ul>
  <li>Duck Typing</li>
  <li>Inheritance</li>
  <li>Behavior Sharing (module)</li>
</ul>

<h3 id="recognizing-hidden-ducks">Recognizing Hidden Ducks</h3>

<ul>
  <li>Case statements that switch on class</li>
  <li><code>kind_of?</code> and <code>is_a?</code></li>
  <li><code>responds_to?</code></li>
</ul>

<h3 id="guidance-1">Guidance</h3>

<p>When you create duck types you must both document and test their public inter- faces. Fortunately, good tests are the best documentation.</p>

<p>The decision to create a new duck type relies on judgment. The purpose of design is to lower costs; bring this measuring stick to every situation. If creating a duck type would reduce unstable dependencies, do so. Use your best judgment.</p>

<h2 id="acquiring-behavior-through-inheritance">Acquiring Behavior Through Inheritance</h2>

<h3 id="inheritance">Inheritance</h3>

<p>Inheritance is, at its core, a mechanism for <strong>automatic message delegation</strong>. It defines a forwarding path for not-understood messages. It creates relationships such that, if one object cannot respond to a received message, it delegates that message to another. You don’t have to write code to explicitly delegate the message, instead you define an inheritance relationship between two objects and the forwarding happens automatically.</p>

<p>When your problem is one of needing numerous specializations of a stable, common abstraction, inheritance can be an extremely low-cost solution.</p>

<h3 id="recognizing-where-to-use-inheritance">Recognizing Where to Use Inheritance</h3>

<p>The inheritance exactly solves: that of highly related types that share common behavior but differ along some dimension.  </p>

<p>Inheritance provides a way to define two objects as having a relationship such that when the first receives a message that it does not understand, it automatically forwards, or delegates, the message to the second. It’s as simple as that.  </p>

<p>Duck types cut across classes, they do not use classical inheritance to share common behavior. Duck types share code via Ruby modules.</p>

<h4 id="finding-the-abstraction">Finding the Abstraction</h4>

<p>It almost never makes sense to create an abstract superclass with only one sub-class.  </p>

<p>Creating a hierarchy has costs; the best way to minimize these costs is to maximize your chance of getting the abstraction right before allowing subclasses to depend on it. While the two bikes you know about supply a fair amount of information about the common abstraction, three bikes would supply a great deal more. If you could put this decision off until FastFeet asked for a third kind of bike, your odds of finding the right abstraction would improve dramatically.</p>

<p>When deciding between refactoring strategies, indeed, when deciding between design strategies in general, it’s useful to ask the question: “What will happen if I’m wrong?”</p>

<h3 id="using-templage-methods">Using Templage Methods</h3>

<h4 id="template-method">Template Method</h4>

<p>This technique of defining a basic structure in the superclass and sending messages to acquire subclass-specific contributions is known as the template method pattern.</p>

<h4 id="implementing-every-template-method">Implementing Every Template Method</h4>

<p>Any class that uses the template method pattern must supply an implementation for every message it sends, and creating code that fails with reasonable error messages takes minor effort in the present but provides value forever.</p>

<p><code>ruby
class Bicycle
  #...
  def default_tire_size
    raise NotImplementedError, "This #{self.class} cannot respond to:"
  end 
end
</code></p>

<h3 id="manging-coupling">Manging Coupling</h3>

<p>When a subclass sends <code>super</code> it’s effectively declaring that it knows the algorithm; it depends on this knowledge. If the algorithm changes, then the subclasses may break even if their own specializations are not otherwise affected.</p>

<h4 id="decoupling-subclasses-using-hook-messages">Decoupling Subclasses Using Hook Messages</h4>

<p>Instead of allowing subclasses to know the algorithm and requiring that they send <code>super</code>, superclasses can instead send <code>hook</code> messages, ones that exist solely to provide subclasses a place to contribute information by implementing matching methods. This strategy removes knowledge of the algorithm from the subclass and returns control to the superclass.</p>

<p>```ruby
class Bicycle
  def initialize(args={})
    @size = args[:size]
    @chain = args[:chain] || default_chain
    @tire_size = args[:tire_size] || default_tire_size</p>

<pre><code>post_initialize(args)   # Bicycle both sends   end
</code></pre>

<p>def post_initialize(args) # and implements this 
    nil
  end
  # …
end</p>

<p>class RoadBike &lt; Bicycle</p>

<p>def post_initialize(args)         # RoadBike can 
    @tape_color = args[:tape_color] # optionally
  end                               # override it
  # …
end
```</p>

<p>This change allows RoadBike to know less about Bicycle, reducing the coupling between them and making each more flexible in the face of an uncertain future. New subclasses need only implement the <code>hook</code> methods.</p>

<h2 id="sharing-role-behavior-with-modules">Sharing Role Behavior with Modules</h2>

<h3 id="understanding-roles">Understanding Roles</h3>

<p>Modules thus provide a perfect way to allow objects of different classes to play a common role using a single set of code.</p>

<p>The rules for modules are the same as for classical inheritance. If a module sends a message it must provide an implementation, even if that implementation merely raises an error indicating that users of the module must implement the method.</p>

<p>This is-a versus behaves-like-a difference definitely matters, each choice has distinct consequences.</p>

<h3 id="writing-inheritable-code">Writing Inheritable Code</h3>

<p>The usefulness and maintainability of inheritance hierarchies and modules is in direct proportion to the quality of the code. </p>

<h4 id="recognize-the-antipatterns">Recognize the Antipatterns</h4>

<p>There are two antipatterns that indicate that your code might benefit from inheritance.</p>

<ul>
  <li>An object that uses a variable with a name like <code>type</code> or <code>category</code> to determine what message to send to <code>self</code> contains two highly related but slightly different types.</li>
  <li>When a sending object checks the class of a receiving object to determine what message to send, you have overlooked a duck type. In addition to sharing an interface, duck types might also share behavior. When they do, place the shared code in a module and include that module in each class or object that plays the role.</li>
</ul>

<h4 id="insist-on-the-abstraction">Insist on the Abstraction</h4>

<p>Superclasses should not contain code that applies to some, but not all, subclasses. This restriction also applies to modules: the code in a module must apply to all who use it.</p>

<p>Subclasses that override a method to raise an exception like “does not implement” are a symptom of this problem. When subclasses override a method to declare that they <em>do not do that thing</em> they come perilously close to declaring that they <em>are not that thing</em>.</p>

<h4 id="honor-the-contract">Honor the Contract</h4>

<p>Subclasses agree to a contract; they promise to be substitutable for their superclasses.</p>

<p>Subclasses that fail to honor their contract are difficult to use. They’re “special” and cannot be freely substituted for their superclasses. These subclasses are declaring that they are not really a kind-of their superclass </p>

<p><strong>Liskov Substitution Principle (LSP)</strong>, which in mathematical terms says that a subtype should be substitutable for its supertype. Named after Barbara Liskov.</p>

<h4 id="use-the-template-method-pattern">Use the Template Method Pattern</h4>

<p>The abstract code defines the algorithms and the concrete inheritors of that abstraction contribute specializations by overriding these template methods.</p>

<p>Modules, therefore, should use the template method pattern to invite those that include them to supply specializations, and should implement hook methods to avoid forcing includers to send <code>super</code>.</p>

<h4 id="preemptively-decouple-classes">Preemptively Decouple Classes</h4>

<p>Avoid writing code that requires its inheritors to send <code>super</code>; instead use hook messages to allow subclasses to participate while absolving them of responsibility for knowing the abstract algorithm. Writing code that requires subclasses to send <code>super</code> adds an additional dependency; avoid this if you can.</p>

<p>Hook methods solve the problem of sending <code>super</code>, but, unfortunately, only for adjacent levels of the hierarchy.</p>

<h4 id="create-shallow-hierarchies">Create Shallow Hierarchies</h4>

<p>The limitations of hook methods are just one of the many reasons to create shallow hierarchies.</p>

<p>Because objects depend on everything above them, a deep hierarchy has a large set of built-in dependencies, each of which might someday change.</p>

<p>Another problem with deep hierarchies is that programmers tend to be familiar with just the classes at their tops and bottoms; that is, they tend to understand only the behavior implemented at the boundaries of the search path.</p>

<h2 id="combining-objects-with-composition">Combining Objects with Composition</h2>

<p>Composition is the act of combining distinct parts into a complex whole such that the whole becomes more than the sum of its parts.</p>

<h3 id="aggregation-a-special-kind-of-composition">Aggregation: A Special Kind of Composition</h3>

<p>Delegation creates dependencies; the receiving object must recognize the message and know where to send it. Composition often involves delegation but the term means something more. A composed object is made up of parts with which it expects to interact via well-defined interfaces.</p>

<p>Composition indicates a <em>has-a</em> relationship where the contained object has no life inde- pendent of its container.</p>

<p>Aggregation is exactly like composition except that the contained object has an independent life.</p>

<h3 id="deciding-between-inheritance-and-composition">Deciding Between Inheritance and Composition</h3>

<ul>
  <li>Remember that classical inheritance is a code arrangement technique. For the cost of arranging objects in a hierarchy, you get message delegation for free. </li>
  <li>Composition is an alternative that reverses these costs and benefits. Composition allows objects to have structural independence, but at the cost of explicit message delegation.</li>
</ul>

<p>The general rule is that, faced with a problem that composition can solve, you should be biased towards doing so. If you cannot explicitly defend inheritance as a better solution, use composition.</p>

<h4 id="inheritance-1">Inheritance</h4>

<p><strong>Benefits</strong></p>

<p>Inheritance is a better solution when its use provides high rewards for low risk.</p>

<p>Use of inheritance results in code that can be described as open–closed; hierarchies are open for extension while remaining closed for modification. </p>

<p>You need look no farther than the source of object-oriented languages themselves to see the value of organizing code using inheritance.</p>

<p><strong>Costs</strong></p>

<p>You might be fooled into choosing inheritance to solve the wrong kind of problem. If you make this mistake a day will come when you need to add behavior but find there’s no easy way do so.</p>

<p>Even when inheritance makes sense for the problem, you might be writing code that will be used by others for purposes you did not anticipate.</p>

<p>The very high cost of making changes near the top of an incorrectly modeled hierarchy. In this case, the leveraging effect works to your disadvantage; small changes break everything.</p>

<p>The impossibility of adding behavior when new subclasses represent a mixture of types.</p>

<p>Inheritance, therefore, is a place where the question “<em>What will happen when I’m wrong?</em>” assumes special importance. Inheritance by definition comes with a deeply embedded set of dependencies. Subclasses depend on the methods defined in their superclasses and on the automatic delegation of messages to those superclasses. This is classical inheritance’s greatest strength and biggest weakness.</p>

<p><strong>Guidance</strong></p>

<p>Your consideration of the use of inheritance should be tempered by your <em>expectations about the population who will use your code</em>. If you are writing code for an in-house application in a domain with which you are intimately familiar, you may be able to predict the future well enough to be confident that your design problem is one for which inheritance is a cost-effective solution.</p>

<p>Avoid writing frameworks that require users of your code to subclass your objects in order to gain your behavior. Their application’s objects may already be arranged in a hierarchy; inheriting from your framework may not be possible.</p>

<h4 id="composition">Composition</h4>

<p>Composed objects do not depend on the structure of the class hierarchy, and they delegate their own messages.</p>

<p><strong>Benefits</strong></p>

<p>When using composition, the natural tendency is to create many small objects that con- tain straightforward responsibilities that are accessible through clearly defined interfaces. These small objects have a single responsibility and specify their own behavior. They are transparent.</p>

<p>By their very nature, objects that participate in composition are small, structurally independent, and have well-defined interfaces. This allows their seamless transition into pluggable, interchangeable components.</p>

<p><strong>Costs</strong></p>

<p>The composed object must explicitly know which messages to delegate and to whom. Identical delegation code may be needed by many different objects. Composition provides no way to share this code.</p>

<p>Composition is excellent at prescribing rules for assembling an object made of parts but doesn’t provide as much help for the problem of arranging code for a collection of parts that are very nearly identical.</p>

<h3 id="guidance-2">Guidance</h3>

<p>Composition, classical inheritance, and behavior sharing via modules are competing techniques for arranging code.</p>

<ul>
  <li>Use inheritance for <em>is-a</em> Relationships.</li>
  <li>Use Duck Types for <em>behaves-like-a</em> Relationships</li>
  <li>Use Composition for <em>has-a</em> Relationships</li>
</ul>

<h2 id="designing-cost-effective-tests">Designing Cost-Effective Tests</h2>

<p>An understanding of object-oriented design, good refactoring skills, and the ability to write efficient tests form a <strong>three-legged stool</strong> upon which changeable code rests.</p>

<p>Your overall goal is to create well-designed applications that have acceptable test coverage. </p>

<h3 id="intentional-testing">Intentional Testing</h3>

<h4 id="knowing-your-intentions">Knowing Your Intentions</h4>

<p>The true purpose of testing, just like the true purpose of design, is to reduce costs.</p>

<p>It is common for programmers who are new to testing to find themselves in the unhappy state where the tests they write do cost more than the value those tests provide, and who therefore want to argue about the worth of tests. The solution to the problem of costly tests, however, is not to stop testing but instead to get better at it.</p>

<ol>
  <li>Finding Bugs</li>
  <li>Supplying Documentation</li>
  <li>Deferring Design Decisions</li>
  <li>Supporting Abstractions</li>
  <li>Exposing Design Flaws. When the design is bad, testing is hard. The best way to achieve this goal is to write loosely coupled tests about only the things that matter.</li>
</ol>

<h4 id="knowing-what-to-test">Knowing What to Test</h4>

<h5 id="remove-the-duplicate">Remove the Duplicate</h5>

<p>One simple way to get better value from tests is to write fewer of them. The safest way to accomplish this is to test everything just once and in the proper place.</p>

<p>Removing duplication from testing lowers the cost of changing tests in reaction to application changes, and putting tests in the right place guarantees they’ll be forced to change only when absolutely necessary.</p>

<h5 id="message-model">Message Model</h5>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/origins_of_messages.png" alt="origins_of_messages" /></p>

<p>Think of an object-oriented application as a series of messages passing between a set of black boxes. Tests should concentrate on the incoming or outgoing messages that cross an object’s boundaries.</p>

<ul>
  <li>
    <p>Incoming Message</p>

    <p>Objects should make assertions about <em>state</em> only for messages in their own public interfaces.</p>
  </li>
  <li>
    <p>Outgoing Message</p>

    <ul>
      <li><em>query</em>, outgoing messages have no side effects and thus matter only to their senders.</li>
      <li><em>command</em>, outgoing messages do have side effects (a file gets written, a database record is saved, an action is taken by an observer). It is the responsibility of the sending object to prove that they are properly sent. Proving that a message gets sent is a test of behavior, not state.</li>
    </ul>
  </li>
</ul>

<p><strong>Conclusion</strong></p>

<p>Incoming messages should be tested for the state they return. Outgoing command messages should be tested to ensure they get sent. Outgoing query messages should not be tested.</p>

<h4 id="knowing-when-to-test">Knowing When to Test</h4>

<p>You should write tests first, whenever it makes sense to do so.</p>

<p>Done at the correct time and in the right amounts, testing, and writing code test-first, will lower your overall costs. Gaining these benefits requires applying object-oriented design principles everywhere, both to the code of your application and to the code in your tests.</p>

<p><em>What novices do?</em></p>

<p>Novices often write code that is far too coupled; they combine unrelated responsibilities and bind many dependencies into every object. </p>

<p>It is an unfortunate truth that the most complex code is usually written by the least qualified person.</p>

<p>Novice programmers don’t yet have the skills to write simple code.</p>

<h4 id="knowing-how-to-test">Knowing How to Test</h4>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/bdd_and_tdd.png" alt="bdd_and_tdd" /></p>

<ul>
  <li><strong>BDD</strong> takes an outside-in approach, creating objects at the boundary of an application and working its way inward, mock-ing as necessary to supply as-yet-unwritten objects.</li>
  <li><strong>TDD</strong> takes an inside-out approach, usually starting with tests of domain objects and then reusing these newly created domain objects in the tests of adjacent layers of code.</li>
</ul>

<p><strong>Testing point-of-view</strong></p>

<p>Your tests could stand completely inside of the object under test, with effective access to all of its internals. This is a bad idea.</p>

<p>It’s better for tests to assume a viewpoint that sights along the edges of the object under test, where they can know only about messages that come and go.</p>

<h3 id="testing-incoming-messages">Testing Incoming Messages</h3>

<ul>
  <li>
    <p>Deleting Unused Interfaces</p>

    <p>Do not test an incoming message that has no dependents; delete it. </p>
  </li>
  <li>
    <p>Proving the Public Interface</p>
  </li>
  <li>
    <p>Isolating the Object Under Test</p>
  </li>
  <li>
    <p>Injecting Dependencies as Roles</p>

    <p>Object-oriented design tells you to inject dependencies because it believes that specific concrete classes will vary more than these roles, or conversely, roles will be more stable than the classes from which they were abstracted.</p>

    <ul>
      <li>Creating Test Doubles</li>
      <li>Using Tests to Document Roles</li>
    </ul>
  </li>
</ul>

<h3 id="testing-private-methods">Testing Private Methods</h3>

<p>Dealing with private methods requires judgment and flexibility.</p>

<p>The rules-of-thumb for testing private methods are thus: Never write them, and if you do, never ever test them, unless of course it makes sense to do so.</p>

<h3 id="testing-outgoing-messages">Testing Outgoing Messages</h3>

<ul>
  <li>Ignoring Query Messages</li>
  <li>
    <p>Proving Command Messages</p>

    <p>The responsibility for testing a message’s return value lies with its receiver. <strong>Mocks</strong> are tests of behavior, as opposed to tests of state. Instead of making assertions about what a message returns, mocks define an expectation that a message will get sent. </p>
  </li>
</ul>

<h3 id="testing-duck-types">Testing Duck Types</h3>

<p>The desire to test duck types creates a need for shareable tests for roles, and once you acquire this role-based perspective you can use it to your advantage in many situations. From the point of view of the object under test, every other object is a role and dealing with objects as if they are representatives of the roles they play loosens coupling and increases flexibility, both in your application and in your tests.</p>

<ul>
  <li>Testing Roles. Extract a module, test it and include in every role.</li>
  <li>Using Role Tests to Validate Doubles.</li>
</ul>

<h3 id="testing-inherited-code">Testing Inherited Code</h3>

<ul>
  <li>
    <p>Specifying the Inherited Interface</p>

    <p>Write a shared test for the common contract and include this test in every object.</p>
  </li>
  <li>
    <p>Specifying Subclass Responsibilities</p>

    <ul>
      <li>Confirming Subclass Behavior. The <em>BicycleInterfaceTest</em> and the <em>BicycleSubclassTest</em>, combined, take all of the pain out of testing the common behavior of subclasses. These tests give you confidence that subclasses aren’t drifting away from the standard.</li>
      <li>Confirming Superclass Enforcement. Test the template method.</li>
    </ul>
  </li>
  <li>
    <p>Testing Unique Behavior</p>

    <ul>
      <li>Testing Concrete Subclass Behavior. It’s important to test these specializations without embedding knowledge of the superclass into the test.</li>
      <li>Testing Abstract Superclass Behavior. Because Bicycle used tem- plate methods to acquire concrete specializations you can stub the behavior that would normally be supplied by subclasses. Even better, because you understand the Liskov Substitution Principle, you can easily manufacture a testable instance of Bicycle by creating a new subclass for use solely by this test.</li>
    </ul>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Practicing Rails]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2014/11/26/practicing-rails/"/>
    <updated>2014-11-26T11:32:14+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2014/11/26/practicing-rails</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Practicing Rails</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td>Justin Weiss</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="https://www.justinweiss.com/book/">www.justinweiss.com/book</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#meta-principles">Meta Principles</a></li>
  <li><a href="#tiny-apps">Tiny Apps</a>    <ul>
      <li><a href="#build-a-tiny-app">Build a tiny App</a></li>
      <li><a href="#owning-the-things-you-learn">Owning the things you learn</a></li>
    </ul>
  </li>
  <li><a href="#build-your-own-app">Build Your Own App</a>    <ul>
      <li><a href="#where-to-start">Where to start?</a>        <ul>
          <li><a href="#build-from-ui-down">Build from UI Down</a></li>
          <li><a href="#thinking-in-resources">Thinking in resources</a></li>
          <li><a href="#t-shaped-development">T-Shaped development</a></li>
          <li><a href="#just-in-time-learning">Just-in-time learning</a></li>
        </ul>
      </li>
      <li><a href="#which-feature-do-you-build-next">Which feature do you build next?</a></li>
    </ul>
  </li>
  <li><a href="#test-your-code-efficiently">Test Your Code Efficiently</a>    <ul>
      <li><a href="#feature-development-process-with-testing">Feature development process with testing</a></li>
      <li><a href="#organizing-and-structuring-your-tests">Organizing and structuring your tests</a>        <ul>
          <li><a href="#use-object-oriented-design-to-make-your-tests-better">Use object-oriented design to make your tests better</a></li>
          <li><a href="#refacoring">Refacoring</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#learning-skill">Learning Skill</a>    <ul>
      <li><a href="#learning-stage">Learning Stage</a></li>
      <li><a href="#a-to-dont-list">A to-don’t list</a></li>
      <li><a href="#guidance">Guidance</a></li>
      <li><a href="#google-and-stackoverflow-may-not-be-the-answer">Google and StackOverflow may not be the answer</a></li>
      <li><a href="#dig-into-code-to-understand-it-better">Dig into code to understand it better</a></li>
      <li><a href="#avoiding-the-temptation-of-the-new">Avoiding the temptation of the new</a></li>
      <li><a href="#building-a-good-mental-filter">Building a good mental filter</a></li>
      <li><a href="#push-vs-pull">Push vs. Pull</a></li>
      <li><a href="#system-learning">System learning</a></li>
      <li><a href="#when-to-give-new-tech-a-chance">When to give new tech a chance</a></li>
      <li><a href="#catch-up-with-changes">Catch up with changes</a></li>
    </ul>
  </li>
  <li><a href="#form-a-habit">Form a habit</a>    <ul>
      <li><a href="#why">Why?</a></li>
      <li><a href="#how-to-keep-consistent">How to keep consistent?</a>        <ul>
          <li><a href="#morning-or-evening">Morning or Evening?</a></li>
        </ul>
      </li>
      <li><a href="#how-to-keep-motivated">How to keep motivated?</a>        <ul>
          <li><a href="#pre-prepare">Pre-prepare</a></li>
          <li><a href="#processes">Processes</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="meta-principles">Meta Principles</h2>

<ul>
  <li>
    <p><strong>As soon as you want to learn something, try it out.</strong></p>
  </li>
  <li>
    <p><strong>When you feel yourself procrastinating or stressed about something, break it apart.</strong></p>

    <p>Large, fuzzy tasks are killer. If the next thing you want to do is tiny, and you can start on it in the next five minutes, you probably will. If it’s big and vague, you’ll put it off until you know how to start it. Which will probably be never.</p>
  </li>
  <li>
    <p><strong>Start backwards.</strong></p>

    <p>When you write an app, start from the UI down. Allow your vision of the feature to guide your development. It’s easy to know if you’re building the right thing when you start from the end and trace back to the beginning. And UI sketches and HTML views are a lot easier to think about than abstract data models.</p>
  </li>
  <li>
    <p><strong>Keep it simple, and add complexity later.</strong></p>

    <p>The most frustrating struggles come from running into problems you don’t know how to solve. You can skip these problems by avoiding new things until you understand the old things. This goes for everything from gems and libraries to patterns and object-oriented design principles.</p>
  </li>
  <li>
    <p><strong>Systems, not motivation.</strong></p>

    <p>You can’t rely on motivation every day. Instead, set up systems and habits, so you don’t have to motivate yourself to work.</p>
  </li>
  <li>
    <p><strong>Struggling should tell you that you’re on the brink of learning something really valuable. Keep it up.</strong></p>
  </li>
</ul>

<h2 id="tiny-apps">Tiny Apps</h2>

<p>When you read something interesting, tweak it with a tiny app.</p>

<ul>
  <li>UI related, with a <code>rails server</code></li>
  <li>Functional, with a <code>rails console</code>. If setting it up in the console gets annoying, write a test case for it.</li>
</ul>

<h3 id="build-a-tiny-app">Build a tiny App</h3>

<blockquote>
  <p>I care about getting the most knowledge in the least amount of time, and scaffolds and other Rails code generators are a great way to do just that.</p>
</blockquote>

<p><code>ruby
rails new test_polymorphic_association
cd test_polymorphic_association
bin/rails generate scaffold bug title:string description:text
bin/rake db:migrate
</code></p>

<p><strong>Some tips for investigating ideas through the Rails console</strong></p>

<p>You can use a lot of Rails features through the <code>app</code> object. app is a special object that has some useful methods for experimenting with your Rails app.</p>

<p><code>ruby
irb(main):002:0&gt; app.bug_path bug
irb(main):003:0&gt; app.get "/bugs/1"
irb(main):003:0&gt; puts app.response.body.first(200)
</code></p>

<p>The <code>helper</code> object provides all of your app’s view and helper methods in the Rails console:</p>

<p><code>ruby
irb(main):005:0&gt; helper.content_tag :h1, "Hey there" =&gt; "&lt;h1&gt;Hey there&lt;/h1&gt;"
</code></p>

<h3 id="owning-the-things-you-learn">Owning the things you learn</h3>

<p>Explore the boundaries of that concept until you feel like you really get it.</p>

<ol>
  <li>test the boundaries</li>
  <li>break and dig it, brainstorm some questions about the idea you’re exploring.</li>
</ol>

<h2 id="build-your-own-app">Build Your Own App</h2>

<ul>
  <li><strong>Core Paths</strong>, focus on building an important path through your app as your first feature.</li>
  <li><strong>Build from UI down</strong>.</li>
  <li><strong>Avoid large, fuzzy tasks</strong>. At every stage, you should be trying to break large tasks apart into smaller ones.</li>
</ul>

<h3 id="where-to-start">Where to start?</h3>

<p>Take a few minutes and think what you’re trying to build. Write down every feature that comes to mind. Think of the different paths a user could take through your application, the different things they could do. Describe them in a single sentence.</p>

<p>Then, narrow them down to paths where, if you didn’t have them, your app couldn’t exist. <strong>Core paths</strong>.</p>

<p>In general, less code is better code. And <strong>starting from the view and building toward the model</strong> from there is the best way I know to consistently write less code.</p>

<ul>
  <li>Finding Core Path</li>
  <li>View toward Model</li>
</ul>

<h4 id="build-from-ui-down">Build from UI Down</h4>

<blockquote>
  <p>Feature development process</p>
</blockquote>

<ol>
  <li>Take the small feature from earlier.</li>
  <li>Think of one simple thing someone could do with that feature.</li>
  <li>Draw just enough screens for that user to be able to do that thing.</li>
  <li>Describe the path through that action, as if you were telling someone what you were going to do.</li>
  <li>As you describe that path, write out the objects, properties of those objects, and other actions you think you need to develop that path.</li>
</ol>

<h4 id="thinking-in-resources">Thinking in resources</h4>

<p>Rails works well with “resources.” (You can think of a resource as an ActiveRecord model and a controller that has seven actions: <code>index</code>, <code>new</code>, <code>create</code>, <code>show</code>, <code>edit</code>, <code>update</code>, and <code>destroy</code>).</p>

<h4 id="t-shaped-development">T-Shaped development</h4>

<p>When you build something, try to get something rough up as quickly as possible. As long as the core of the feature you want to build is there, it’s fine.</p>

<h4 id="just-in-time-learning">Just-in-time learning</h4>

<p>It means you’re not trying to learn everything at the beginning. That’s the most common way to procrastinate starting something that you’re too nervous to do.</p>

<h3 id="which-feature-do-you-build-next">Which feature do you build next?</h3>

<p>Keeping these tasks as small as possible is the key. And try to follow the questions below.</p>

<ol>
  <li>Ask some questions: What is the app missing? What did you postpone to get that first feature done?</li>
  <li>Try using what you have so far. What would make your life easier if it was built? What annoys you while you’re using your app?</li>
  <li>Ifyou’rebuildingthisappforsomeoneelse,watchhowtheyusetheproject. Where are they struggling? What do they complain about?</li>
</ol>

<h2 id="test-your-code-efficiently">Test Your Code Efficiently</h2>

<h3 id="feature-development-process-with-testing">Feature development process with testing</h3>

<ol>
  <li>Take your small feature.</li>
  <li>Think of one simple thing someone could do with that feature.</li>
  <li>Draw just enough screens for that user to be able to do that thing.</li>
  <li>Translate that sketch into a failing integration test.</li>
  <li>Get the first part of that test to pass:</li>
  <li>Write a failing controller test.</li>
  <li>Write some failing unit tests.</li>
  <li>Write enough code to get the unit and controller tests to pass.</li>
  <li>Repeat until the next part of your integration test passes.</li>
</ol>

<p><strong><em>What do you test?</em></strong></p>

<ul>
  <li>Happy path tests.</li>
  <li>Sad path tests.</li>
  <li>What-if tests, must be documented.</li>
</ul>

<p><strong><em>How do you test?</em></strong></p>

<ol>
  <li>Arrange</li>
  <li>Act</li>
  <li>Assert</li>
</ol>

<p><strong><em>When aren’t you testing enough?</em></strong></p>

<p>Bugs imply that you missed a test somewhere. If you run into a bug in your app that your tests didn’t catch, you’re probably missing a test.</p>

<ol>
  <li>Write a test that fails while the bug exists.</li>
  <li>Fix the bug.</li>
  <li>Make sure the test passes now.</li>
  <li>Check in both your test and fix, so you don’t run into the problem again.</li>
</ol>

<p><strong><em>How to keep TDD?</em></strong></p>

<p>Translate that sketch into a failing <strong>integration test</strong>(Capybara).</p>

<ol>
  <li>Write a failing controller test.</li>
  <li>Write some failing unit tests.</li>
  <li>Write enough code to get the unit and controller tests to pass.</li>
  <li>Repeat until the next part of your integration test passes.</li>
</ol>

<h3 id="organizing-and-structuring-your-tests">Organizing and structuring your tests</h3>

<h4 id="use-object-oriented-design-to-make-your-tests-better">Use object-oriented design to make your tests better</h4>

<p>With minitest, all of your test suites are just classes, tests are methods. That means you can use your object oriented design skills to reorganize your tests.</p>

<p>Guides to follow:</p>

<ol>
  <li><strong>For tests, clarity is better than cleverness.</strong> You don’t have anything testing your tests, so you have to be careful not to make things too abstract. Hard-coding values, copy and pasted code, all that kind of stuff is usually OK in tests, if they make the test easier to understand.</li>
  <li>Organizing your tests is easiest if your test organization matches your code organization. </li>
  <li>Wait until you feel real pain before refactoring your tests.</li>
</ol>

<h4 id="refacoring">Refacoring</h4>

<ul>
  <li>Using Extract Method to write custom assertions.</li>
  <li>Using Extract Method to make mocks easier to write.</li>
  <li>Using modules to share tests between test suites.</li>
</ul>

<h2 id="learning-skill">Learning Skill</h2>

<blockquote>
  <p>The skill of learning.</p>
</blockquote>

<h3 id="learning-stage">Learning Stage</h3>

<ol>
  <li>First stage. Baseline knowledge, bootstrapping your learning process.</li>
  <li>Second stage. You’re past the basics but not an expert, still require conscious thought.</li>
  <li>Thrid stage. Skills in the third stage are mastered. They don’t require thought, you use instinct and intuition when you use them.</li>
</ol>

<p>If all of their skills were at an intermediate level, they’d not only have to think about how to use each of those skills, but how they interacted in this one specific situation, and what kinds of tradeoffs they’ll have to make.</p>

<p>1 + 3 &gt; 2 + 2</p>

<p>So you’ll be much more productive if you have 5 skills in the third stage and 5 skills in the first stage than if you have ten skills in the second stage.</p>

<h3 id="a-to-dont-list">A to-don’t list</h3>

<p>It might seem like you need to master JavaScript before you can write a Rails app. You will, someday. But you won’t get anywhere without starting an app you can get excited about, and you don’t need JavaScript for that first stage. So cross JavaScript off the list. For now.</p>

<p><strong>For now</strong> is powerful. You’re giving yourself permission to set other things aside, so you can focus on something else.</p>

<h3 id="guidance">Guidance</h3>

<p>Search for the things that really resonate with you, the things you get lost in, the things you just want to do for hours.</p>

<p>Take a look at what you know and what you don’t. What you want to learn, and what your app needs you to learn. Set aside some things that seem less important, and turn them into a “Not Right Now” list. Eventually you’ll have a few things you just can’t set aside – learn those thoroughly. And keep moving forward.</p>

<p>Intermediate Rails isn’t about learning all the stuff you learned as a beginner in a little more depth. It’s about the stem of the “T” in T-Shaped Learning. It’s about gaining deep knowledge in a few different areas, one thing at a time. And it’s about using that knowledge to build your own apps, the way you imagined.</p>

<h3 id="google-and-stackoverflow-may-not-be-the-answer">Google and StackOverflow may not be the answer</h3>

<p>This can be the fastest way to get your problem solved, and is much easier than investigating it yourself. But you lose the opportunity to go through the investigation, and you miss the chance to build experience debugging and solving your own problems. This robs you of a chance to get to know your code, the language, and the framework better.</p>

<p>Every problem you run into is an opportunity to learn.</p>

<p>But if you use them to solve a problem, make sure you read the answer, follow references, and immerse yourself in the knowledge the solution brings you. </p>

<p>When you use someone else’s solution to solve your problems, your app will become a mess of inconsistent code that probably only works coincidentally.</p>

<h3 id="dig-into-code-to-understand-it-better">Dig into code to understand it better</h3>

<p>Great developers know how to read and understand code. When you read code, you’ll understand much more than what the documentation tells you. Sometimes, after spending time with the code, you’ll know as much about it as the author does!</p>

<p><strong>Reading code is a skill you’ll have to build like any other, and it’s not like reading a book.</strong></p>

<p>Practice reading code. Learning to read and explore code will teach you things you won’t learn anywhere else. And when you get good enough at reading code, you’ll be able to solve problems you might have thought were impossible before.</p>

<p>Remember, reading code isn’t like reading anything else. It’s about debugging and exploration. <strong>You have to run it.</strong></p>

<h3 id="avoiding-the-temptation-of-the-new">Avoiding the temptation of the new</h3>

<p>You have to separate the things that are interesting because they’re new, from the things that are interesting because they’ll help you get work done.</p>

<h3 id="building-a-good-mental-filter">Building a good mental filter</h3>

<p>Two questions to ask.</p>

<ol>
  <li>Is this something I need to know right now?</li>
  <li>If I knew this a year ago, would it have made my life easier today?</li>
</ol>

<h3 id="push-vs-pull">Push vs. Pull</h3>

<p>Hitting these sites is the “pull” model of receiving tech news. You’re the one digging it up. But these days, I’ve been using the “push” model more and more. I’ve been getting email newsletters, podcasts, things that get delivered to you instead of you looking for them.</p>

<h3 id="system-learning">System learning</h3>

<p>If you find a few good sources, keep a steady learning and practice schedule, and learn things as they become important to you, you’ll make much more progress than those who constantly chase the news sites.</p>

<h3 id="when-to-give-new-tech-a-chance">When to give new tech a chance</h3>

<p>Try it out on a new small project. Then, take the technique as far as it’ll go.</p>

<p>But unfortunately, a lot of the techniques won’t have as much of a benefit in smaller projects. So create a new branch for your experiments, so you don’t wreck your code if you don’t like the technique. And</p>

<ol>
  <li>Make the change.</li>
  <li>Look at the old code next to the new.</li>
  <li>Ask yourself, which code do you prefer?</li>
  <li>Ask a few other people, which code do they prefer?</li>
  <li>If the new way is an improvement, go forward with that.</li>
</ol>

<p><strong>A technique only has value if it improves your code, so a direct comparison is the best way to judge.</strong></p>

<h3 id="catch-up-with-changes">Catch up with changes</h3>

<p>The best way to keep up with changes to your gems is to track down the project’s <em>CHANGELOG</em> file.</p>

<p>These will help you catch up on the big changes from version to version. Usually, they’re just a short summary of each major change.</p>

<h2 id="form-a-habit">Form a habit</h2>

<blockquote>
  <p>Keep your schedule consistent</p>
</blockquote>

<h3 id="why">Why?</h3>

<p><strong>Whenever you try to build a new skill, consistency is much more important than the amount of time you spend.</strong></p>

<p>When you first try to keep a schedule, it’ll feel weird. To me, it feels like I’m just pretending, like I’m just copying what someone else told me to do, instead of actually getting anything done. That feeling starts to hit me around the fourth or fifth day in a row, and it goes away after about three weeks. It’s totally normal, but it can be dangerous.</p>

<p>Anytime you change a routine, you’ll face some internal resistance. Our daily routines seem normal to us, that’s what makes them routines. So building the momentum to change those routines takes a lot of effort, since you’re breaking habits that have taken years to form, in just a few weeks.</p>

<p>You might be tempted to spend 6 hours learning one day and take the rest of the week off. But that doesn’t solve the core problem.</p>

<p><strong>Habits are built off of repetition, not total time.</strong> If you try to jam everything all into a single day, you’ll have less repetition. It’ll be easier to skip. And if your schedule is really so packed that you can’t find time every day to work on this stuff, how will you find a big chunk of time one day a week?</p>

<p><strong>Once you’re consistent enough to form a habit, motivation won’t be as much of an issue. It’s just become something I do, so I do it.</strong></p>

<h3 id="how-to-keep-consistent">How to keep consistent?</h3>

<p>By “consistency,” I don’t mean that you have to spend hours every single night on this stuff. When you first build your schedule, go shorter rather than longer. <strong>Aim for 40 minutes a day to start.</strong></p>

<p>If you can’t find the time anywhere else, staying up 40 minutes later at night usually won’t be too painful. Same thing with waking up a little earlier each day.</p>

<p>But 40 minutes is also short enough that it’ll surprise you when it’s over. You’ll start the next task and leave it unfinished.</p>

<p>When you <strong>leave something unfinished</strong>, it’ll stay in the back of your mind. When this happens, you’ll unconsciously be looking for closure, so you’ll be more receptive to related ideas that just pop into your head. You’ll really want to finish it.</p>

<h4 id="morning-or-evening">Morning or Evening?</h4>

<p>Cons on <strong>evening</strong>:</p>

<ol>
  <li>You don’t really have a set deadline (except sleep), so you can let your motivation carry you beyond the time you set aside.</li>
  <li>You might also be drained. If you’re tired, it’s easy to convince yourself to skip it, “just this once.” And after a frustrating day at work, you’ll start to tell yourself that you’ve had a rough day, you deserve to just get a good night’s sleep. You’ll catch up tomorrow, right?</li>
  <li>It’s also easy to push until later. “If I watch one more episode, I’ll start as soon as it’s done. It’ll only be 10 minutes late.” But before you know it, you’re an hour late, you’ve destroyed your sleeping schedule and you’ll pay for it tomorrow, when you’re drained again and you’ll skip again.</li>
</ol>

<p>While <strong>morning</strong>:</p>

<blockquote>
  <p>You might feel brain-dead and uncreative in the morning, which can be killer if you’re learning and practicing creative work like learning Rails. And it’s hard to wake up early until you get used to it.  </p>

  <p>I heard a lot of people I trust and respect suggest trying waking up a little earlier for a week or so. I did, and it was hard, and I felt totally unproductive.  </p>

  <p>But somehow, once I measured my actual productivity, I found out I was twice as pro- ductive in the mornings as the evenings. This is crazy, because it felt like the exact opposite!</p>
</blockquote>

<p>I really need to give it a shot.</p>

<h3 id="how-to-keep-motivated">How to keep motivated?</h3>

<p>Of course, to form a habit is the best way. But before that, try these.</p>

<h4 id="pre-prepare">Pre-prepare</h4>

<ol>
  <li>Separating the decision about where to start from the decision about what to do.</li>
  <li>It creates an unfinished loop in your mind, they’re just as powerful here.</li>
</ol>

<h4 id="processes">Processes</h4>

<p>Seinfeld method:</p>

<blockquote>
  <p>[Jerry Seinfeld] told me to get a big wall calendar that has a whole year on one page and hang it on a prominent wall. The next step was to get a big red magic marker.</p>

  <p>He said for each day that I do my task of writing, I get to put a big red X over that day. “After a few days you’ll have a chain. Just keep at it and the chain will grow longer every day. You’ll like seeing that chain, especially when you get a few weeks under your belt. Your only job next is to not break the chain.”</p>

  <p>“Don’t break the chain,” he said again for emphasis.</p>
</blockquote>

<p>When you miss a habit and break a chain, you lose all your motivation to keep the streak going. And you lose it at the exact time you need extra motivation to build that streak back up.</p>

<p>So keep consistent with a habit tracker, calendar, or Beeminder.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Design Patterns in Ruby]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2014/10/17/design-patterns-in-ruby/"/>
    <updated>2014-10-17T17:40:01+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2014/10/17/design-patterns-in-ruby</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Design Patterns in Ruby</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td>Russ Olsen</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://designpatternsinruby.com/">designpatternsinruby.com</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#meta-design-pattern">Meta Design Pattern</a></li>
  <li><a href="#about-design-pattern">About Design Pattern</a></li>
  <li><a href="#design-pattern-classification">Design Pattern Classification</a></li>
  <li><a href="#the-template-method">The Template Method</a>    <ul>
      <li><a href="#description">Description</a></li>
      <li><a href="#using-and-abusing">Using and Abusing</a></li>
      <li><a href="#in-the-wild">In the Wild</a></li>
    </ul>
  </li>
  <li><a href="#the-strategy">The Strategy</a>    <ul>
      <li><a href="#description-1">Description</a></li>
      <li><a href="#comparing-to-the-template-method">Comparing to the Template Method</a></li>
      <li><a href="#sharing-data-between-the-context-and-strategy">Sharing Data between the Context and Strategy</a></li>
      <li><a href="#using-and-abusing-1">Using and Abusing</a></li>
      <li><a href="#in-the-wild-1">In the Wild</a></li>
    </ul>
  </li>
  <li><a href="#the-observer">The Observer</a>    <ul>
      <li><a href="#description-2">Description</a></li>
      <li><a href="#code-usage">Code Usage</a></li>
      <li><a href="#interfaces-pull-or-push">Interfaces. Pull or Push?</a></li>
      <li><a href="#using-and-abusing-2">Using and Abusing</a></li>
    </ul>
  </li>
  <li><a href="#dont-inform-the-observers-just-yet">Don’t inform the observers just yet</a></li>
  <li><a href="#now-inform-the-observers">Now inform the observers!</a>    <ul>
      <li><a href="#in-the-wild-2">In the Wild</a></li>
      <li><a href="#composite">Composite</a>        <ul>
          <li><a href="#description-3">Description</a></li>
          <li><a href="#code-usage-1">Code Usage</a></li>
          <li><a href="#concerns">Concerns</a></li>
          <li><a href="#using-and-abusing-3">Using and Abusing</a></li>
          <li><a href="#in-the-wild-3">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#the-iterator">The Iterator</a>        <ul>
          <li><a href="#description-4">Description</a></li>
          <li><a href="#internal-iterators-vs-external-iterators">Internal Iterators vs. External Iterators</a></li>
          <li><a href="#the-inimitable-enumerable">The Inimitable Enumerable</a></li>
          <li><a href="#using-and-abusing-4">Using and Abusing</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#red">red</a>    <ul>
      <li><a href="#in-the-wild-4">In the Wild</a></li>
    </ul>
  </li>
  <li><a href="#if-you-supply-the-argument-eachobject-will-iterate-over-only">If you supply the argument, each_object will iterate over only</a>    <ul>
      <li><a href="#the-command">The Command</a>        <ul>
          <li><a href="#description-5">Description</a></li>
          <li><a href="#keep-track-of-what-you-have-done">Keep Track of What You Have Done</a></li>
          <li><a href="#undo-or-redo">Undo or Redo</a></li>
          <li><a href="#queuing-up-commands">Queuing Up Commands</a></li>
          <li><a href="#using-and-abusing-5">Using and Abusing</a></li>
          <li><a href="#in-the-wild-5">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#the-adapter">The Adapter</a>        <ul>
          <li><a href="#description-6">Description</a></li>
          <li><a href="#adapt-or-modify">Adapt or Modify?</a></li>
          <li><a href="#using-and-abusing-6">Using and Abusing</a></li>
          <li><a href="#in-the-wild-6">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#the-proxy">The Proxy</a>        <ul>
          <li><a href="#description-7">Description</a></li>
          <li><a href="#the-protection-proxy">The Protection Proxy</a></li>
          <li><a href="#the-remove-proxy">The Remove Proxy</a></li>
          <li><a href="#the-virtual-proxy">The Virtual Proxy</a></li>
          <li><a href="#using-and-abusing-7">Using and Abusing</a></li>
          <li><a href="#in-the-wild-7">In the Wild</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#client">client</a></li>
  <li><a href="#the-client-side-mathservice-is-actually-a-remote-proxy-to-the-real">the client-side math_service is actually a remote proxy to the real</a>    <ul>
      <li><a href="#the-decorator">The Decorator</a>        <ul>
          <li><a href="#description-8">Description</a></li>
          <li><a href="#why-not-the-template-method">Why Not The Template Method?</a></li>
          <li><a href="#code-usage-2">Code Usage</a></li>
          <li><a href="#using-and-abusing-8">Using and Abusing</a></li>
          <li><a href="#in-the-wild-8">In the Wild</a></li>
          <li><a href="#adapter-proxy-or-decorator">Adapter, Proxy or Decorator</a></li>
        </ul>
      </li>
      <li><a href="#singleton">Singleton</a>        <ul>
          <li><a href="#code-usage-3">Code Usage</a></li>
          <li><a href="#alternatives">Alternatives</a></li>
          <li><a href="#using-and-abusing-9">Using and Abusing</a></li>
          <li><a href="#in-the-wild-9">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#factory">Factory</a>        <ul>
          <li><a href="#description-9">Description</a></li>
          <li><a href="#abstract-factory">Abstract Factory</a></li>
          <li><a href="#factory--abstract-factory">Factory &amp;&amp; Abstract Factory</a></li>
          <li><a href="#using-and-abusing-10">Using and Abusing</a></li>
          <li><a href="#in-the-wild-10">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#builder">Builder</a>        <ul>
          <li><a href="#description-10">Description</a></li>
          <li><a href="#code-usage-4">Code Usage</a></li>
          <li><a href="#builders-can-ensure-sane-objects">Builders Can Ensure Sane Objects</a></li>
          <li><a href="#resuable-buidlers">Resuable Buidlers</a></li>
          <li><a href="#better-builders-with-magic-methods">Better Builders with Magic Methods</a></li>
          <li><a href="#using-and-abusing-11">Using and Abusing</a></li>
          <li><a href="#in-the-wild-11">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#interpreter">Interpreter</a>        <ul>
          <li><a href="#description-11">Description</a></li>
          <li><a href="#with-a-parser">With a Parser</a></li>
          <li><a href="#without-a-parser">Without a Parser</a></li>
          <li><a href="#using-and-abusing-12">Using and Abusing</a></li>
          <li><a href="#in-the-wild-12">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#domain-specific-languages">Domain-Specific Languages</a>        <ul>
          <li><a href="#description-12">Description</a></li>
          <li><a href="#using-and-abusing-13">Using and Abusing</a></li>
          <li><a href="#in-the-wild-13">In the Wild</a></li>
        </ul>
      </li>
      <li><a href="#custom-objects">Custom Objects</a>        <ul>
          <li><a href="#custom-tailoring-technique">Custom-Tailoring Technique</a></li>
          <li><a href="#reflections">Reflections</a></li>
          <li><a href="#using-and-abusing-14">Using and Abusing</a></li>
        </ul>
      </li>
      <li><a href="#convention-over-configuration">Convention Over Configuration</a>        <ul>
          <li><a href="#description-13">Description</a></li>
          <li><a href="#using-and-abusing-15">Using and Abusing</a></li>
        </ul>
      </li>
      <li><a href="#reference">Reference</a></li>
    </ul>
  </li>
</ul>

<h2 id="meta-design-pattern">Meta Design Pattern</h2>

<ul>
  <li>Seperate out the things that change from thos that stay the same.</li>
  <li>Program to an interface, not an implementation.</li>
  <li>Prefer composition over inheritance.</li>
  <li>Delegate, delegate, delegate.</li>
</ul>

<p>Others:</p>

<ul>
  <li>YAGNI, You ain’t gonna need it.</li>
  <li>A pattern is not just about code: Intent is critical.</li>
</ul>

<p><strong>Seperate out the things that change from thos that stay the same.</strong></p>

<p>A key goal of software engineering is to build systems that allow us to contain the damage. In an ideal system, all changes are local.</p>

<p>You get there by separating the things that are likely to change from the things that are likely to stay the same. If you can identify which aspects of your system design are likely to change, you can isolate those bits from the more stable parts.</p>

<p>But how do you keep the changing parts from infecting the stable parts? <em>Program to an interface, not an implementation.</em></p>

<p><strong>Program to an interface, not an implementation.</strong></p>

<p>A good start is to write code that is less tightly coupled to itself in the first place.</p>

<p>The idea here is to program to the most general type you can.</p>

<p><strong>Prefer composition over inheritance.</strong></p>

<p>The trouble is that inheritance comes with some unhappy strings attached. When you create a subclass of an existing class, you are not really creating two separate entities: Instead, you are making two classes that are bound together by a common implementation core. Inheritance, by its very nature, tends to marry the subclass to the superclass.</p>

<p>If our goal is to build systems that are not tightly coupled together, to build systems where a single change does not ripple through the code like a sonic boom, breaking the glassware as it goes, then probably we should not rely on inheritance as much as we do.</p>

<p>We can assemble the behaviors we need through composition. In short, we try to avoid saying that an object is <em>a kind of</em> something and instead say that it <em>has</em> something.</p>

<p><strong>Delegate, delegate, delegate.</strong></p>

<p>The combination of composition and delegation is a powerful and flexible alternative to inheritance. We get most of the benefits of inheritance, much more flexibility, and none of the unpleasant side effects. Of course, nothing comes for free. Delegation requires an extra method call, as the delegating object passes the buck along. This extra method call will have some performance cost—but in most cases, it will be very minor.</p>

<p>Another cost of delegation is the boilerplate code you need to write.</p>

<p><strong>YAGNI, You ain’t gonna need it.</strong></p>

<p>You Ain’t Gonna Need It (YAGNI for short). The YAGNI principle says simply that you should not implement features, or design in flexibility, that you don’t need right now.</p>

<p>A well-designed system is one that will flex gracefully in the face of bug fixes, changing requirements, the ongoing march of technology, and inevitable redesigns. The YAGNI principle says that you should focus on the things that you need right now, building in only the flexibility that you know you need.</p>

<p>The use of design patterns has somehow become associated with a particularly virulent strain of over-engineering, with code that tries to be infinitely flexible at the cost of being understandable, and maybe even at the cost of just plain working. The proper use of design patterns is the art of making your system just flexible enough to deal with the problems you have today, but no more.</p>

<p>Your system will not work better because you used all 23 of the GoF design patterns in every possible combination. Your code will work better only if it focuses on the job it needs to do right now.</p>

<h2 id="about-design-pattern">About Design Pattern</h2>

<p>Background</p>

<blockquote>
  <p>In 1995, Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides set out to redirect all the effort going into building redundant software wheels into some- thing more useful. That year, building on the work of Christopher Alexander, Kent Beck, and others, they published Design Patterns: Elements of Reusable Object-Oriented Software. The book was an instant hit, with the authors rapidly becoming famous (at least in software engineering circles) as the Gang of Four (GoF).</p>
</blockquote>

<p>Focus on some key questions:</p>

<ul>
  <li>How do objects like the ones you tend to find in most systems relate to one another?</li>
  <li>How should they be coupled together?</li>
  <li>What should they know about each other?</li>
  <li>How can we swap out parts that are likely to change frequently?</li>
</ul>

<p>It’s commonly agreed that the most useful thing about patterns is the way in which they form a vocabulary for articulating design decisions during the normal course of development conversations among programmers.</p>

<p>Design patterns are little spring-loaded solutions to common programming problems. And a reckless use of every design pattern on the menu to solve nonexistent problems gives design patterns a bad name in some circles.</p>

<p><strong>With Ruby</strong></p>

<p>With Ruby, we no longer need to pull out relatively heavyweight design patterns to solve tiny problems. Instead, Ruby allows you to do simple things simply. </p>

<ul>
  <li>
    <p>Like a Command object in the GoF sense is essentially a wrapper around some code that knows how to do one specific thing, to run a particular bit of code at some time. Of course, that is also a fairly accurate description of a Ruby code block object or a <code>Proc</code>.</p>
  </li>
  <li>
    <p>Internal Domain Specific Languages. I believe that his treatment of the subject, as an evolution of the Interpreter pattern, is the first significant reference work in publication on the topic.</p>
  </li>
</ul>

<p>The Ruby programming language makes implementing patterns so easy that sometimes they fade into the background.</p>

<ul>
  <li>Ruby is dynamically typed.</li>
  <li>Ruby has code closures.</li>
  <li>Ruby classes are real objects.</li>
  <li>Ruby has an elegant system of code reuse.</li>
</ul>

<p>The traditional implementations of many design patterns work, but they make you work, too. Ruby allows you to concentrate on the real problems that you are trying to solve instead of the plumbing.</p>

<p>The increasing industry recognition of the value of dynamic and flexible languages such as Ruby has plunged us into yet another wisdom gap.</p>

<blockquote>
  <p>Design Patterns was published is the need for wisdom. </p>

  <p>Bruce Tate is fond of pointing out1 that when a new programming technique or language pops up, there is frequently a wisdom gap. The industry needs time to come to grips with the new technique, to figure out the best way to apply it. How many years had to elapse between the first realization that object- oriented programming was the way to go and the time when we really began to use object-oriented technology effectively? Those years were the object-oriented wisdom gap.</p>
</blockquote>

<h2 id="design-pattern-classification">Design Pattern Classification</h2>

<p><strong>Creational</strong> (5)</p>

<ul>
  <li>Factory Method</li>
  <li>Abstract Factory</li>
  <li>Builder</li>
  <li>Prototype</li>
  <li>Singleton</li>
</ul>

<p><strong>Structural</strong> (7)</p>

<ul>
  <li>Facade</li>
  <li>Adapter</li>
  <li>Proxy</li>
  <li>Decorator</li>
  <li>Bridge</li>
  <li>Composite</li>
  <li>Flyweight</li>
</ul>

<p><strong>Behavioural</strong> (11)</p>

<ul>
  <li>Template Method</li>
  <li>Observer</li>
  <li>State</li>
  <li>Strategy</li>
  <li>Chain of Responsibility</li>
  <li>Command</li>
  <li>Visitor</li>
  <li>Mediator</li>
  <li>Memento</li>
  <li>Iterator</li>
  <li>Interpreter</li>
</ul>

<h2 id="the-template-method">The Template Method</h2>

<p>Basic idea is <em>Seperate out the things that change from thos that stay the same.</em> </p>

<h3 id="description">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/template-method.png" alt="template-method" /></p>

<ol>
  <li>Extract the common part into an abstract base class</li>
  <li>Create some hook methods as the interface</li>
  <li>Let the subclass to implement it</li>
</ol>

<p>The Template Method pattern is simply a fancy way of saying that if you want to vary an algorithm, one way to do so is to code the invariant part in a base class and to encapsulate the variable parts in methods that are defined by a number of subclasses.</p>

<p>The abstract base class controls the higher-level processing through the template method; the sub-classes simply fill in the details.</p>

<p>Non-abstract methods that can be overridden in the concrete classes of the Template Method pattern are called <strong>hook methods</strong>.</p>

<p>Duck typing is a trade-off: You give up the compile-time safety of static typing, and in return you get back a lot of code clarity and programming flexibility.</p>

<h3 id="using-and-abusing">Using and Abusing</h3>

<p>The Template Method pattern is at its best when it is at its leanest—that is, when every abstract method and hook is there for a reason. Try to avoid creating a template class that requires each subclass to override a huge number of obscure methods just to cover every conceivable possibility. You also do not want to create a template class that is encrusted with a multitude of hook methods that no one will ever override.</p>

<h3 id="in-the-wild">In the Wild</h3>

<p>There is another very common example of the Template Method pattern that is perhaps so pervasive that it is hard to see. Think about the <code>initialize</code> method that we use to set up our objects. All we know about <code>initialize</code> is that it is called sometime toward the end of the process of creating a new object instance and that it is a method that we can override in our class to do any specific initialization. Sounds like a hook method to me.</p>

<p><code>Class#new</code> calls <code>allocate</code> first, then <code>initialise</code>. Every class inherits the <code>new</code> method, and defines its own concrete <code>initialise</code> method. So, we can treat <code>Class#new</code> as a template method, and <code>initialise</code> as a hook method.</p>

<h2 id="the-strategy">The Strategy</h2>

<p>Basic idea is <em>Delegate, delegate, delegate</em> and <em>Prefer composition over inheritance</em>.</p>

<h3 id="description-1">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/strategy.png" alt="strategy" /></p>

<ol>
  <li>Pull the algorithm out into a seperate “strategy” object.</li>
  <li>All of the startegy objects support the same interface. </li>
  <li>Let the context choose.</li>
</ol>

<p>Given that all of the strategy objects look alike from the outside, the user of the strategy—called the <strong>context</strong> class by the GoF—can treat the strategies like interchangeable parts.</p>

<h3 id="comparing-to-the-template-method">Comparing to the Template Method</h3>

<p>The Template Method pattern is built around inheritance.</p>

<p>No matter how carefully you design your code, your subclasses are tangled up with their superclass: It is in the nature of the relationship. On top of this, inheritance-based techniques such as the Template Method pattern limit our runtime flexibility. Once we have selected a particular variation of the algorithm—in our example, once we have created an instance of HTMLReport—changing our mind is hard.</p>

<p><code>ruby
# The Template Method
report = HTMLReport.new
report.output_report
report = PlainTextReport.new
report.output_report
</code></p>

<p>Because the Strategy pattern is based on composition and delegation, rather than on inheritance, it is easy to switch strategies at runtime.</p>

<p><code>ruby
# The Strategy
report = Report.new(HTMLFormatter.new)
report.output_report
report.formatter = PlainTextFormatter.new
report.output_report
</code></p>

<h3 id="sharing-data-between-the-context-and-strategy">Sharing Data between the Context and Strategy</h3>

<ol>
  <li><em>Pass in everything that the strategy needs as arguments when the context calls the methods on the strategy object.</em> The downside of doing things this way is that if there is a lot of complex data to pass between the context and the strategy, then, well, you are going to be passing a lot of complex data around without any guarantee that it will get used.</li>
  <li>Having the context object pass a reference to itself to the strategy object.</li>
</ol>

<h3 id="using-and-abusing-1">Using and Abusing</h3>

<p>Particular attention to the details of the interface between the context and the strategy as well as to the coupling between them. Remember, the Strategy pattern will do you little good if you couple the con- text and your first strategy so tightly together that you cannot wedge a second or a third strategy into the design.</p>

<h3 id="in-the-wild-1">In the Wild</h3>

<p>Ruby code blocks, which are essentially code wrapped up in an instant object (the Proc object), are wonderfully useful for creating quick, albeit simple, strategy objects.  </p>

<p>Use Proc as the lightweight strategy object. </p>

<p>```ruby
class Report
  attr_reader :title, :text
  attr_accessor :formatter</p>

<p>def initialize(&amp;formatter)
    @title = ‘Monthly Report’
    @text = [ ‘Things are going’, ‘really, really well.’ ]
    @formatter = formatter
  end</p>

<p>def output_report
    @formatter.call( self )
  end
end
```</p>

<h2 id="the-observer">The Observer</h2>

<h3 id="description-2">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/observer.png" alt="observer" /></p>

<p>The aiming is to build a system that is highly integrated—that is, a system where every part is aware of the state of the whole.</p>

<p>The GoF called this idea of building a clean interface between the source of the news that some object has changed and the consumers of that news the Observer pattern. The class with the news is the <strong>subject</strong>, and the objects which are interested in getting the news are the <strong>observor</strong>.</p>

<blockquote>
  <p>It has always seemed to me that the Observer pattern is somewhat misnamed. While the observer object gets top billing—in fact, the only billing—it is actually the subject that does most of the work. It is the subject that is responsible for keeping track of the observers. It is also the subject that needs to inform the observers that a change has come down the pike.</p>
</blockquote>

<h3 id="code-usage">Code Usage</h3>

<p>```ruby
module Subject
  def initialize
    @observers=[]
  end</p>

<p>def add_observer(observer)
    @observers « observer
  end</p>

<p>def delete_observer(observer)
    @observers.delete(observer)
  end</p>

<p>def notify_observers
    @observers.each do |observer|
      observer.update(self)
    end
  end
end</p>

<p>class Employee
  include Subject</p>

<p>attr_reader :name, :address
  attr_reader :salary</p>

<p>def initialize( name, title, salary)
   super()
   @name = name
   @title = title
   @salary = salary
  end</p>

<p>def salary=(new_salary)
    @salary = new_salary
    notify_observers
  end
end
```</p>

<p>The Ruby standard library comes with a fine, prebuilt <a href="http://ruby-doc.org/stdlib-1.9.3/libdoc/observer/rdoc/Observable.html">Observable</a> module that provides all of the support you need to make your object, well, observable.</p>

<p>With the observable module, the observable object must:</p>

<ol>
  <li>assert that it has <code>#changed</code></li>
  <li>call <code>#notify_observers</code></li>
</ol>

<p>```ruby
require ‘observer’
class Employee
  include Observable</p>

<p>attr_reader :name, :address
  attr_reader :salary</p>

<p>def initialize( name, title, salary)
   @name = name
   @title = title
   @salary = salary
  end</p>

<p>def salary=(new_salary)
    @salary = new_salary
    changed
    notify_observers(self)
  end
end
```</p>

<h3 id="interfaces-pull-or-push">Interfaces. Pull or Push?</h3>

<p>The key decisions that you need to make when implementing the Observer pattern all center on the interface between the subject and the observer.</p>

<p>Just have a single method in the observer whose only argument is the subject. The GoF term for this strategy is the <strong>pull</strong> method, because the observers have to pull whatever details about the change that they need out of the subject.</p>

<p>The other possibility—logically enough termed the <strong>push</strong> method—has the subject send the observers a lot of details about the change:</p>

<p><code>ruby
observer.update_salary(self, old_salary, new_salary)
observer.update_title(self, old_title, new_title)
</code></p>

<p>The advantage in providing more details is that the observers do not have to work quite as hard to keep track of what is going on. The disadvantage of the push model is that if all of the observers are not interested in all of the details, then the work of passing the data around goes for naught.</p>

<h3 id="using-and-abusing-2">Using and Abusing</h3>

<p><em>The frequency and timing of the updates.</em> The subject class can help with all of this by avoiding broadcasting redundant updates. Just because some- one updates an object, it does not mean that anything really changed.</p>

<p><code>ruby
def salary=(new_salary)
  old_salary = @salary
  @salary = new_salary
  if old_salary != new_salary
    changed
    notify_observers(self)
  end
end
</code></p>

<p><em>The consistency of the subject as it informs its observers of changes.</em></p>

<p>```ruby
fred = Employee.new(“Fred”, “Crane Operator”, 30000)</p>

<p>fred.salary = 1000000
# Warning! Inconsistent state here!
fred.title = ‘Vice President of Sales’</p>

<h1 id="dont-inform-the-observers-just-yet">Don’t inform the observers just yet</h1>
<p>fred.salary = 1000000
fred.title = ‘Vice President of Sales’</p>

<h1 id="now-inform-the-observers">Now inform the observers!</h1>
<p>fred.changes_complete
```</p>

<p><em>Badly behaved observers.</em> Like responds by raising an exception?</p>

<h3 id="in-the-wild-2">In the Wild</h3>

<p>Use Proc as Observers. Just use <code>call</code> as the interface when notifying observers.</p>

<p><code>ruby
module Subject
  def notify_observers
    @observers.each do |observer|
      observer.call(self)
    end
  end
end
</code></p>

<p>ActiveRecord::Observer has been deprecated from Rails 4.0, but we can still get the feature by the extracted gem. <a href="https://github.com/rails/rails-observers">rails-observers</a></p>

<h2 id="composite">Composite</h2>

<h3 id="description-3">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/composite.png" alt="composite" /></p>

<ol>
  <li><strong>component</strong>, a common interface or base class for all of your objects.</li>
  <li><strong>leaf</strong>, the class doing simple, indivisible building blocks of process.</li>
  <li><strong>composite</strong>, a component, also a higher-level object that is build from subcomponents.</li>
</ol>

<p>The GoF called the design pattern for our “<em>the sum acts like one of the parts</em>” situa- tion the Composite pattern. You will know that you need to use the Composite pattern when you are trying to build a hierarchy or tree of objects, and you do not want the code that uses the tree to constantly have to worry about whether it is dealing with a single object or a whole bushy branch of the tree. Once you grasp its <em>recursive nature</em>, the Composite pattern is really quite simple.</p>

<h3 id="code-usage-1">Code Usage</h3>

<p>```ruby
class CompositeTask &lt; Task
  def initialize(name)
    super(name)
    @sub_tasks = []
  end</p>

<p>def add_sub_task(task)
    @sub_tasks « task
  end</p>

<p>def remove_sub_task(task)
    @sub_tasks.delete(task)
  end</p>

<p>def get_time_required
    time=0.0
    @sub_tasks.each {|task| time += task.get_time_required}
    time
  end
end</p>

<p>class MakeBatterTask &lt; CompositeTask
  def initialize
    super(‘Make batter’)
    add_sub_task( AddDryIngredientsTask.new )
    add_sub_task( AddLiquidsTask.new )
    add_sub_task( MixTask.new )
  end
end
```</p>

<h3 id="concerns">Concerns</h3>

<p><strong><em>How to handle the difference between a composite and a leaf?</em></strong></p>

<p>The goal of the Composite pattern is to make the leaf objects <em>more or less</em> indistinguishable from the composite objects. But there is one unavoidable difference between a composite and a leaf: The composite has to manage its children, which probably means that it needs to have a method to get at the children and possibly methods to add and remove child objects. The leaf classes, of course, really do not have any children to manage; that is the nature of leafyness.</p>

<p>As I say, how you handle this decision is mostly a matter of taste: Make the leaf and composite classes different, or burden the leaf classes with embarrassing methods that they do not know how to handle. My own instinct is to leave the methods off of the leaf classes. Leaf objects cannot handle child objects, and we may as well admit it.</p>

<p><strong><em>How to traverse the tree structrue which the composite pattern make?</em></strong></p>

<p>Each composite object holds references to its subcomponents but the child compo- nents do not know a thing about their parents, it is easy to traverse the tree from the root to the leaves but hard to go the other way.</p>

<p>Add a parent reference in the component class.</p>

<p>```ruby
class Task
  attr_accessor :name, :parent</p>

<p>def initialize(name)
    @name = name
    @parent = nil
  end
end</p>

<p>class CompositeTask &lt; Task
  def initialize(name)
    super(name)
    @sub_tasks = []
  end</p>

<p>def add_sub_task(task)
    @sub_tasks « task
    task.parent = self
  end</p>

<p>def remove_sub_task(task)
    @sub_tasks.delete(task)
    task.parent = nil
  end
end
```</p>

<h3 id="using-and-abusing-3">Using and Abusing</h3>

<p>The error that crops up so frequently with the Composite pattern is assuming that the tree is only one level deep—that is, assuming that all of the child components of a composite object are, in fact, leaf objects and not other composites.</p>

<p>Remember, the power of the Composite pattern is that it allows us to build arbi- trarily deep trees.</p>

<h3 id="in-the-wild-3">In the Wild</h3>

<h2 id="the-iterator">The Iterator</h2>

<h3 id="description-4">Description</h3>

<p>Provide a way to access the elements of an aggregate object sequentially without exposing its underlying representation.</p>

<p>Iterators in Ruby are a great example of what is right with the language. Instead of providing special-purpose external iterator objects for each aggregate class, Ruby relies on the very flexible idea of Proc objects and code blocks to build internal iterators.</p>

<p><strong>external iterator</strong>, the iterator is a separate object from the aggregate.</p>

<p><code>java
# in java
ArrayList list = new ArrayList();
list.add("red");
list.add("green");
list.add("blue");
for( Iterator i = list.iterator(); i.hasNext();) {
        System.out.println( "item: " + i.next());
}
</code></p>

<p><strong>internal iterator</strong>, the code block-based iterators, all of the iterating action occurs inside the aggregate object.</p>

<p>```ruby
# in ruby
def for_each_element(array)
  i= 0
  while i &lt; array.length
    yield(array[i])
    i += 1
  end
end</p>

<p>a = [10, 20, 30]
for_each_element(a) {|element| puts(“The element is #{element}”)}
```</p>

<h3 id="internal-iterators-vs-external-iterators">Internal Iterators vs. External Iterators</h3>

<p>With Internal Iterator, the main advantage is simplicity and code clarity.</p>

<p>With External Iterator</p>

<ol>
  <li>
    <p>You have more flexibility on iteration control. With an external iterator, you won’t call <code>next</code> until you are good and ready for the next element. With an internal iterator, by contrast, the aggregate relentlessly pushes the code block to accept item after item.</p>

    <p><em>If you are trying to merge the contents of two sorted arrays into a single array that was itself sorte?</em></p>

    <p>the merge is actually fairly easy with an external iterator, simply create an iterator for the two input arrays and then merge the arrays by repeatedly pushing the smallest value from either of the iterators onto the output array.</p>
  </li>
  <li>
    <p>A second advantage of external iterators is that, because they are external, you can share them—you can pass them around to other methods and objects. Of course, this is a bit of a double-edged sword: You get the flexibility but you also have to know what you are doing. In particular, beware of multiple threads getting hold of a non-thread-safe external iterator.</p>
  </li>
</ol>

<h3 id="the-inimitable-enumerable">The Inimitable Enumerable</h3>

<p>To mix in <code>Enumerable</code>, you need only make sure that your internal iterator method is named <code>each</code> and that the individual elements that you are going to iterate over have a reasonable implementation of the <code>&lt;=&gt;</code> comparison operator. </p>

<p>```ruby
class Account
  attr_accessor :name, :balance</p>

<p>def initialize(name, balance)
    @name = name
    @balance = balance
  end</p>

<p>def &lt;=&gt;(other)
    balance &lt;=&gt; other.balance
  end
end</p>

<p>class Portfolio
  include Enumerable</p>

<p>def initialize
    @accounts = []
  end</p>

<p>def each(&amp;block)
    @accounts.each(&amp;block)
  end</p>

<p>def add_account(account)
    @accounts « account
  end
end</p>

<p>my_portfolio.any? {|account| account.balance &gt; 2000}
my_portfolio.all? {|account| account.balance &gt; = 10}
```</p>

<h3 id="using-and-abusing-4">Using and Abusing</h3>

<p>The main danger is this: What happens if the aggregate object changes while you are iterating through it?</p>

<p>You may use a shallow copy when initializing.</p>

<p><code>ruby
class ChangeResistantArrayIterator
  def initialize(array)
    @array = Array.new(array)
    @index = 0
  end
  ...
</code></p>

<p>A Ruby trick example.</p>

<p>```ruby
array=[‘red’, ‘green’, ‘blue’, ‘purple’]</p>

<p>array.each do | color |
  puts(color)
  if color == ‘green’
    array.delete(color)
  end
end</p>

<h1 id="red">red</h1>
<p># green
# purple
```</p>

<p><em>Finally, a multithreaded program is a particularly dangerous home for iterators.</em> You need to take all of the usual care to ensure that one thread does not rip the aggregate rug out from under your iterator.</p>

<h3 id="in-the-wild-4">In the Wild</h3>

<p><strong>IO</strong></p>

<p>The neat thing about the IO object is that it is amphibious—it does both internal and external iterators.</p>

<p>```ruby
f = File.open(‘names.txt’)
while not f.eof?
  puts(f.readline)
end
f.close</p>

<p>f = File.open(‘names.txt’)
f.each {|line| puts(line)}
f.close
```</p>

<p><strong>Pathname</strong> <a href="http://ruby-doc.org/stdlib-2.1.0/libdoc/pathname/rdoc/Pathname.html">API</a></p>

<p>Pathname tries to offer one-stop shopping for all your directory and path manipulation needs.</p>

<p>```ruby
pn.each_filename {|file| puts(“File: #{file}”)}
# File: usr
# File: local
# File: lib
# File: ruby
# File: 1.8</p>

<p>pn.each_entry {|entry| puts(“Entry: #{entry}”)}
# Entry: .
# Entry: ..
# Entry: i686-linux
# Entry: shellwords.rb
# Entry: mailread.rb
# …
```</p>

<p><strong>ObjectSpace</strong> <a href="http://www.ruby-doc.org/core-2.1.3/ObjectSpace.html">API</a></p>

<p>ObjectSpace provides a window into the complete universe of objects that exist within your Ruby interpreter. The fundamental iterator supplied by ObjectSpace is the <code>each_object</code> method. It iterates across all of the Ruby objects—everything that is loaded into your Ruby interpreter:</p>

<p>```ruby
ObjectSpace.each_object {|object| puts(“Object: #{object}”)}</p>

<h1 id="if-you-supply-the-argument-eachobject-will-iterate-over-only">If you supply the argument, each_object will iterate over only</h1>
<p># the instances of that class or module.
ObjectSpace.each_object(Numeric) {|n| puts(“The number is #{n}”)}
```</p>

<p>Try this execellent <code>subclasses_of</code> method:</p>

<p>```ruby
def subclasses_of(superclass)
  subclasses = []</p>

<p>ObjectSpace.each_object(Class) do |k|
    next if !k.ancestors.include?(superclass) || superclass == k || k.to_s.include?(‘::’) || subclasses.include?(k.to_s)
    subclasses « k.to_s
  end</p>

<p>subclasses
end</p>

<p>subclasses_of(Numeric)
# =&gt; [“Complex”, “Rational”, “Bignum”, “Float”, “Fixnum”, “Integer”]
```</p>

<h2 id="the-command">The Command</h2>

<h3 id="description-5">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/command.png" alt="command" /></p>

<p>The idea of factoring out the action code into its own object is the essence of the Command pattern.</p>

<p>The key thing about the Command pattern is that it separates the thought from the deed. When you use this pattern, you are no longer simply saying, “Do this”; instead, you are saying, “Remember how to do this,” and, sometime later, “Do that thing that I told you to remember.”</p>

<p>Command pattern can be useful in</p>

<ol>
  <li>Keeping track of what you need to do, or what you have already done</li>
  <li>Undo or redo</li>
  <li>Queuing up comands</li>
</ol>

<h3 id="keep-track-of-what-you-have-done">Keep Track of What You Have Done</h3>

<p>```ruby
class Command
  attr_reader :description</p>

<p>def initialize(description)
    @description = description
  end</p>

<p>def execute
  end
end</p>

<p>class CreateFile &lt; Command
  def initialize(path, contents)
    super(“Create file: #{path}”)
    @path = path
    @contents = contents
  end</p>

<p>def execute
    f = File.open(@path, “w”)
    f.write(@contents)
    f.close
  end
end
```</p>

<p><strong>Use Composite</strong></p>

<p>When we are trying to keep track of what we are about to do—or have done—we will need a class to collect all of our commands. Hmm, a class that acts like a command, but really is just a front for a number of subcommands. Sounds like a composite:</p>

<p>```ruby
class CompositeCommand &lt; Command
  def initialize
    @commands = []
  end</p>

<p>def add_command(cmd)
    @commands « cmd
  end</p>

<p>def execute
    @commands.each {|cmd| cmd.execute}
  end</p>

<p>def description
    description = ‘’
    @commands.each {|cmd| description += cmd.description + “\n”}
    description
  end
end</p>

<p>cmds = CompositeCommand.new
cmds.add_command(CreateFile.new(‘file1.txt’, “hello world\n”))
cmds.add_command(CopyFile.new(‘file1.txt’, ‘file2.txt’))
cmds.add_command(DeleteFile.new(‘file1.txt’))</p>

<p>cmds.execute
```</p>

<h3 id="undo-or-redo">Undo or Redo</h3>

<p>Every undoable command that we create has two methods. Along with the usual <code>execute</code> method, which does the thing, we add an <code>unexecute</code> method, which undoes the same thing.</p>

<p>As delete a file maybe destructive, so we need to save the contents of the original file.</p>

<p>```ruby
class DeleteFile &lt; Command
  def initialize(path)
    super “Delete file: #{path}”
    @path = path
  end</p>

<p>def execute
    if File.exists?(@path)
      @contents = File.read(@path)
    end
    f = File.delete(@path)
  end</p>

<p>def unexecute
    if @contents
      f = File.open(@path,”w”)
      f.write(@contents)
      f.close
    end
  end
end<br />
```</p>

<p>Creating a file with CreateFile could be destructive, too: The file that we are trying to create might already exist and be overwritten as we create the new file. In a real system, we would need to deal with this possibility as well as with a host of issues related to file permissions and ownership. </p>

<p>```ruby
class CreateFile &lt; Command
  def initialize(path, contents)
    super “Create file: #{path}”
    @path = path
    @contents = contents
  end</p>

<p>def execute
    f = File.open(@path, “w”)
    f.write(@contents)
    f.close
  end
  def unexecute
    File.delete(@path)
  end
end
```</p>

<p>Finnaly, add an <code>unexecute</code> method to the <code>CompositeCommad</code> class.</p>

<p>```ruby
class CompositeCommand &lt; Command
  # …</p>

<p>def unexecute
    @commands.reverse.each { |cmd| cmd.unexecute }
  end</p>

<p># …
end
```</p>

<h3 id="queuing-up-commands">Queuing Up Commands</h3>

<blockquote>
  <p>For example, it frequently takes a minor computer-time eternity to connect to a database. If you need to perform a number of database operations over time, you sometimes face the unpleasant choice of (1) leaving the connection open for the whole time, thereby wasting a scarce resource, or (2) wasting the time it takes to open and close the connection for each operation.</p>
</blockquote>

<p>The Command pattern offers one way out of this kind of bind. Instead of performing each operation as a stand-alone task, you accumulate all of these commands in a list. Periodically, you can open a connection to the database, execute all of your commands, and flush out this list.</p>

<h3 id="using-and-abusing-5">Using and Abusing</h3>

<p>The key thing about the Command pattern is that it separates the thought from the deed. When you use this pattern, you are no longer simply saying, “Do this”; instead, you are saying, “Remember how to do this,” and, sometime later, “Do that thing that I told you to remember.” Make sure that you really need that complexity before you pull the Command pattern out of your bag of tricks.</p>

<p><strong>Creation Time versus Execution Time</strong></p>

<p>Assuming you really do need the Command pattern, to make it work you have to be sure that the initial thought is complete. You have to carefully think through the circumstances in which the command object will find itself when it is executed versus when it was created. Yes, this key file was open, and that vital object was initialized when I cre- ated the command. Will it all still be there for me when the command is executed?</p>

<h3 id="in-the-wild-5">In the Wild</h3>

<p><strong>ActiveRecord::Migration</strong></p>

<p>```ruby
class CreateBookTable &lt; ActiveRecord::Migration
  # execute
  def self.up
    create_table :books do |t|
      t.column :title, :string
      t.column :author, :string
    end
  end</p>

<p># unexecute
  def self.down
    drop_table :books
  end
end
```</p>

<p><strong>Madeleine</strong></p>

<p><a href="https://github.com/ghostganz/madeleine">repo</a></p>

<blockquote>
  <p>Imagine how slow your system would be if you had to write out a whole airport’s worth of seat assignments every time someone changed his or her mind and wanted that aisle seat after all.</p>
</blockquote>

<p>Madeleine is a transactional, high-performance, object persistence framework that does not need any object relational mapping for the simple reason that it does not use a relational database—or any other kind of database, for that matter. Instead, Madeleine relies on the Ruby Marshal package, a facility for converting live Ruby objects into bytes and for turning those bytes back into objects. Unfortunately, being able to marshal your objects to a file is not by itself a complete solution to application persistence.</p>

<p><a href="https://gist.github.com/ifyouseewendy/c0a3ec5da222779885f0">Example gist using Madeleine</a></p>

<h2 id="the-adapter">The Adapter</h2>

<h3 id="description-6">Description</h3>

<p>An adapter is an object that crosses the chasm between the interface that you have and the interface that you need.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/adapter.png" alt="the adapter" /></p>

<p>The client expects the target to have a certain interface. But unknown to the client, the target object is really an adapter, and buried inside of the adapter is a reference to a second object, the adaptee, which actually per- forms the work.</p>

<h3 id="adapt-or-modify">Adapt or Modify?</h3>

<p>The choice of using an adapter or modifying the object really comes down to how well you understand the class in question and the issue of encapsulation.</p>

<p>Lean toward modifying the class in the following circumstances:</p>

<ul>
  <li>The modifications are simple and clear.</li>
  <li>You understand the class you are modifying and the way in which it is used.</li>
</ul>

<p>Lean toward an adapter solution in the following situations:</p>

<ul>
  <li>The interface mismatch is extensive and complex. </li>
  <li>You have no idea how this class works.</li>
</ul>

<p>Engineering is all about trade-offs. Adapters preserve encapsulation at the cost of some complexity. Modifying a class may buy you some simplification, but at the cost of tinkering with the plumbing.</p>

<h3 id="using-and-abusing-6">Using and Abusing</h3>

<p>One of the advantages that Ruby’s duck typing gives to adapter writers is that it allows us to create adapters that support only that part of the target interface that the client will actually use. Partially implemented adapters are something of a double-edged sword: On the one hand, it is very convenient to implement only what you absolutely need; on the other hand, your program can come to grief if the client decides to call a method that you didn’t think you needed.</p>

<h3 id="in-the-wild-6">In the Wild</h3>

<p><code>ActiveRecord</code> deals with all of these differences by defining a standardized interface, encapsulated in a class called <code>AbstractAdapter</code>. The <code>AbstractAdapter</code> class defines the interface to a database that is used throughout <code>ActiveRecord</code>. </p>

<p><code>AbstractAdapter</code> defines a standard method to execute a SQL select statement and return the results, called <code>select_all</code>. Each individual adapter implements the <code>select_all</code> method in terms of the API of the underlying database system.</p>

<h2 id="the-proxy">The Proxy</h2>

<h3 id="description-7">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/proxy.png" alt="the proxy" /></p>

<p>The Proxy pattern is essentially built around a little white lie. The counterfeit object, called the <strong>proxy</strong> by the GoF, has a reference to the real object, the <strong>subject</strong>, hidden inside. Whenever the client code calls a method on the proxy, the proxy simply forwards the request to the real object.</p>

<p>Inside the proxy is hidden a reference to the other, real object—an object that the GoF referred to as the subject.</p>

<p>Once we have a proxy, we have a place to stand squarely between the client and the real object. The proxy provides the ideal pinch point to exert control.</p>

<p>The proxy serves as a pinch point between the client and the subject:</p>

<ul>
  <li>“Is this operation authorized?” asks the protection proxy.</li>
  <li>“Does the subject actually live on this other machine?” asks the remote proxy. </li>
  <li>“Have I actually created the subject yet?” asks the virtual proxy. </li>
</ul>

<p>In short, the proxy controls access to the subject.</p>

<h3 id="the-protection-proxy">The Protection Proxy</h3>

<p>A proxy that controls access to the subject.</p>

<p>```ruby
require ‘etc’</p>

<p>class AccountProtectionProxy
  def initialize(real_account, owner_name)
    @subject = real_account
    @owner_name = owner_name
  end</p>

<p>def deposit(amount)
    check_access
    return @subject.deposit(amount)
  end</p>

<p>def withdraw(amount)
    check_access
    return @subject.withdraw(amount)
  end</p>

<p>def check_access
    if Etc.getlogin != @owner_name
      raise “Illegal access: #{Etc.getlogin} cannot access account.”
    end
  end
end
```</p>

<p>The advantage of using a proxy for protection is that it gives us a nice separation of concerns: The proxy worries about who is or is not allowed to do what. The only thing that the real bank account object need be concerned with is, well, the bank account.</p>

<p>By splitting the protection cleanly off from the workings of the real object, we can minimize the chance that any important information will inadvertently leak out through our protective shield.</p>

<h3 id="the-remove-proxy">The Remove Proxy</h3>

<p>You could hide the complexity behind a remote proxy, an object that lives on the client machine and looks, to the client code, just like the real BankAccount object. When a request comes in, the remote proxy goes through all the horror of pack- aging up the request, sending it over the network, waiting for a response, unpacking the response, and returning the answer to the unsuspecting client.</p>

<p>From the client’s point of view, it called a method on what it thought was the real BankAccount object and sometime later—perhaps an unusually long time later—the answer came back. This is how virtually all remote procedure call (RPC) systems work.</p>

<p>```ruby
require ‘soap/wsdlDriver’</p>

<p>wsdl_url = ‘http://www.webservicex.net/WeatherForecast.asmx?WSDL’</p>

<p>proxy = SOAP::WSDLDriverFactory.new( wsdl_url ).create_rpc_driver
weather_info = proxy.GetWeatherByZipCode(‘ZipCode’=&gt;’19128’)
```</p>

<p>Once the proxy object is set up, the client code no longer has to worry about the fact that the service actually lives at www.webservicex.net. Instead, it simply calls GetWeatherByZipCode and leaves all of the network details to the proxy.</p>

<h3 id="the-virtual-proxy">The Virtual Proxy</h3>

<p>In a sense, the virtual proxy is the biggest liar of the bunch. It pretends to be the real object, but it does not even have a reference to the real object until the client code calls a method. Only when the client actually calls a method does the virtual proxy scurry off and create or otherwise get access to the real object.</p>

<p>```ruby
class VirtualAccountProxy</p>

<p>def initialize(starting_balance=0)
    @starting_balance=starting_balance
  end</p>

<p>def deposit(amount)
    s = subject
    return s.deposit(amount)
  end</p>

<p>def withdraw(amount)
    s = subject
    return s.withdraw(amount)
  end</p>

<p>def balance
    s = subject
    return s.balance
  end</p>

<p>def subject
    @subject || (@subject = BankAccount.new(@starting_balance))
  end
end<br />
```</p>

<p>That approach tangles the proxy and the subject up a little more than we might like. We can improve on this strategy by applying a little of that Ruby code block magic:</p>

<p>```ruby
class VirtualAccountProxy
  def initialize(&amp;creation_block)
    @creation_block = creation_block
  end</p>

<p># Other methods omitted …</p>

<p>def subject
    @subject || (@subject = @creation_block.call)
  end
end
```</p>

<p><strong>Leverage Ruby</strong></p>

<p>Use ghost method <code>method_missing</code> and dynamic dispatch <code>send</code>.</p>

<p>```ruby
class VirtualProxy
  def initialize(&amp;creation_block)
    @creation_block = creation_block
  end</p>

<p>def method_missing(name, *args)
    s = subject
    s.send( name, *args)
  end</p>

<p>def subject
    @subject = @creation_block.call unless @subject
    @subject
  end
end</p>

<p>array = VirtualProxy.new { Array.new }
array « ‘hello’ array « ‘out’ array « ‘there’
```</p>

<h3 id="using-and-abusing-7">Using and Abusing</h3>

<p>Overusing <code>method_missing</code>, like overusing inheritance, is a great way to obscure your code.</p>

<h3 id="in-the-wild-7">In the Wild</h3>

<p><strong>drb using a remote proxy</strong></p>

<p>```ruby
# server
class MathService
  def add(a, b)
    return a + b
  end
end</p>

<p>require ‘drb/drb’</p>

<p>math_service=MathService.new
DRb.start_service(“druby://localhost:3030”, math_service)
DRb.thread.join</p>

<h1 id="client">client</h1>
<p>require ‘drb/drb’
DRb.start_service</p>

<h1 id="the-client-side-mathservice-is-actually-a-remote-proxy-to-the-real">the client-side math_service is actually a remote proxy to the real</h1>
<p># math service, which is running inside the server-side Ruby interpreter.
math_service = DRbObject.new_with_uri(“druby://localhost:3030”)
sum=math_service.add(2,2)
```</p>

<h2 id="the-decorator">The Decorator</h2>

<blockquote>
  <p>But what if you simply need to vary the responsibilities of an object? What do you do when sometimes your object needs to do a little more, but sometimes a little less?</p>
</blockquote>

<h3 id="description-8">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/decorator.png" alt="decorator" /></p>

<p>The ConcreteComponent is the “real” object, the object that implements the basic component functionality.</p>

<p>The Decorator pattern is a straightforward technique that you can use to assemble exactly the functionality that you need at runtime. The Decorator class has a reference to a Component—the next Component in the decorator chain—and it implements all of the methods of the Component type. </p>

<p>Each decorator supports the same core interface, but adds its own twist on that interface. The key implementation idea of the Decorator pattern is that the decorators are essentially shells: Each takes in a method call, adds its own special twist, and passes the call on to the next component in line. </p>

<p>The Decorator pattern lets you start with some basic functionality and layer on extra features, one decorator at a time.</p>

<h3 id="why-not-the-template-method">Why Not The Template Method?</h3>

<p>The trouble is that the inheritance-based approach requires you to come up with all possi- ble combinations of features up-front, at design time.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/out-of-control-inheritance.png" alt="out-of-control inheritance" /></p>

<h3 id="code-usage-2">Code Usage</h3>

<p>```ruby
class WriterDecorator
  def initialize(real_writer)
    @real_writer = real_writer
  end</p>

<p>def write_line(line)
    @real_writer.write_line(line)
  end</p>

<p>def pos
    @real_writer.pos
  end</p>

<p>def rewind
    @real_writer.rewind
  end</p>

<p>def close
    @real_writer.close
  end
end</p>

<p>class NumberingWriter &lt; WriterDecorator
  def initialize(real_writer)
    super(real_writer)
    @line_number = 1
  end</p>

<p>def write_line(line)
    @real_writer.write_line(“#{@line_number}: #{line}”)
    @line_number += 1
   end
end</p>

<p>writer = NumberingWriter.new(SimpleWriter.new(‘final.txt’))
writer.write_line(‘Hello out there’)
```</p>

<p><strong>Fowardable module</strong> <a href="http://www.ruby-doc.org/stdlib-2.0/libdoc/forwardable/rdoc/Forwardable.html">API</a></p>

<p>Ruby provides the <strong>Forwardable</strong> module provides delegation of specified methods to a designated object, using the methods <code>def_delegator</code> and <code>def_delegators</code>.</p>

<p>```ruby
require ‘forwardable’</p>

<p>class WriterDecorator
  extend Forwardable</p>

<p>def_delegators :@real_writer, :write_line, :rewind, :pos, :close</p>

<p>def initialize(real_writer)
    @real_writer = real_writer
  end
end
```</p>

<p>The forwardable module is more of a precision weapon than the <code>method_missing</code> technique. But the <code>method_missing</code> technique really shines when you want to delegate large numbers of calls.</p>

<p><strong>Dynamic Alternatives - Wrapping Methods</strong></p>

<ul>
  <li>Around Alias</li>
  <li>Refinement Wrapper</li>
  <li>Prepended Wrapper</li>
</ul>

<p>Check <a href="http://blog.ifyouseewendy.com/blog/2014/06/03/metaprogrammingi-ruby/#method-wrapper">this</a>.</p>

<p><strong>Dynamic Alternatives - Decorating with Modules</strong></p>

<p>```ruby
w = SimpleWriter.new(‘out’)
w.extend(NumberingWriter)
w.extend(TimeStampingWriter)</p>

<p>w.write_line(‘hello’)
```</p>

<p>With both of these techniques, it is hard to undo the decoration. Unwrapping an aliased method is likely to be tedious, and you simply cannot un-include a module.</p>

<h3 id="using-and-abusing-8">Using and Abusing</h3>

<ul>
  <li>The classic Decorator pattern is loved more by the folks who build the thing than by those who use it.</li>
  <li>One thing to keep in mind when implementing the Decorator pattern is that you need to keep the component interface simple.</li>
  <li>Another potential drawback of the Decorator pattern is the performance overhead associated with a long chain of decorators.</li>
  <li>Finally, one drawback of the method-aliasing technique for decorating objects is that it tends to make your code harder to debug.</li>
</ul>

<h3 id="in-the-wild-8">In the Wild</h3>

<p><strong><code>alias_method_chain</code> in ActiveSupport</strong></p>

<p>```ruby
def write_line(line)
  puts(line)
end</p>

<p>def write_line_with_timestamp(line)
  write_line_without_timestamp(“#{Time.new}: #{line}”)
end</p>

<p>alias_method_chain :write_line, :timestamp
```</p>

<p>The <code>alias_method_chain</code> method will rename the original <code>write_line</code> method to <code>write_line_without_timestamp</code> and rename <code>write_line_with_timestamp</code> to plain old <code>write_line</code>, essentially creating a chain of methods. The nice thing about <code>alias_method_chain</code> is that, as its name suggests, you can chain together a number of enhancing methods.</p>

<h3 id="adapter-proxy-or-decorator">Adapter, Proxy or Decorator</h3>

<p>They are all “<em>one object stands for another</em>”, and the basic idea is <em>Delegate, delegate, delegate</em>.</p>

<ul>
  <li><strong>The Adapter</strong> hides the fact that some object has the wrong interface by wrapping it with an object that has the right interface. </li>
  <li><strong>The Proxy</strong> also wraps another object, but not with the intent of changing the interface. Instead, the proxy has the same interface as the object that it is wrapping. The proxy isn’t there to tre; it is there to control. Proxies are good for tasks such as enforcing security, hiding the fact that an object really lives across the network, and delaying the creation of the real object until the last possible moment. </li>
  <li><strong>The Decorator</strong> enables you to layer features on to a basic object.</li>
</ul>

<h2 id="singleton">Singleton</h2>

<p>A singleton class has exactly one instance, and access to that one instance is available globally.</p>

<h3 id="code-usage-3">Code Usage</h3>

<ol>
  <li>Creating the class variable and initializing it with the singleton instance</li>
  <li>Creating the class-level <code>instance</code> method</li>
  <li>Make <code>new</code> private.</li>
</ol>

<p>```ruby
class SimpleLogger
  # Lots of code deleted…</p>

<p>@@instance = SimpleLogger.new</p>

<p>def self.instance
    return @@instance
  end</p>

<p># make sure there is only one
  private_class_method :new
end</p>

<p>logger1 = SimpleLogger.instance   # Returns the logger
logger2 = SimpleLogger.instance   # Returns exactly the same logger
```</p>

<p>Creating the singleton instance before you actually need it is called <em>eager instantiation</em>.</p>

<p><strong>Singleton module</strong> <a href="http://www.ruby-doc.org/stdlib-1.9.3/libdoc/singleton/rdoc/Singleton.html">API</a></p>

<p>```ruby
require ‘singleton’</p>

<p>class SimpleLogger
  include Singleton</p>

<p># Lots of code deleted…
end
```</p>

<p>The Singleton module, waits until someone calls instance before it actually creates its singleton. This technique is known as <em>lazy instantiation</em>.</p>

<h3 id="alternatives">Alternatives</h3>

<p><strong>Global Variables and Constants</strong></p>

<ol>
  <li>If you use a global variable or a constant for this purpose, there is no way to delay the creation of the singleton object until you need it.</li>
  <li>Neither of these techniques does anything to prevent someone from creating a second or third instance of your supposedly singleton class.</li>
</ol>

<p><strong>Class and Module methods</strong></p>

<p>Lazy initialization remains and all of those <code>self.methods</code> and <code>@@variables</code> makes a strange feel.</p>

<h3 id="using-and-abusing-9">Using and Abusing</h3>

<p><strong>Don’t expect the Singleton module really prevent anything</strong></p>

<p>```ruby
require ‘singleton’</p>

<p>class Manager
  include Singleton</p>

<p>def manage_resources
    puts(“I am managing my resources”)
  end
end
```</p>

<p>Use <code>public_class_method</code>.</p>

<p>```ruby
m = Manager.new # =&gt; private method ‘new’ called for Manager:Class</p>

<p>class Manager
  public_class_method :new
end</p>

<p>m = Manager.new
```</p>

<p>Use <code>clone</code></p>

<p>```ruby
m = Manager.instance.close
# =&gt; TypeError: can’t clone instance of singleton Manager</p>

<p>Foo = Manager.clone
Foo.instance.manage_resources
# =&gt; I am managing my resources
```</p>

<p>The Ruby philosophy is that if you decide to circumvent the very clear intent of the author of the ClassBasedLogger class by cloning it, the language is there to help you out. You are in the driver’s seat, not the language. By keeping almost everything open to modification, Ruby allows you to do the things that you say you want to do—but it is up to you to say the right things.</p>

<p><strong>Coupling concern</strong></p>

<p>Create a singleton, and you have just made it possible for widely separated bits of your program to use that singleton as a secret channel to communicate with each other and, in the process, tightly couple themselves to each other. The horrible consequences of this coupling are why software engineering got out of the global variable business in the first place.</p>

<p>There is only one solution to this problem: <em>Don’t do that</em>.</p>

<p><strong>Considering the count, Do I really only need one instance?</strong></p>

<p><strong>a Need-to-Know Basis</strong></p>

<p>Another mistake that many people make is to spread the knowledge of a class’s singleton-ness far and wide.</p>

<p>```ruby
require ‘singleton’</p>

<p>class DatabaseConnectionManager
  include Singleton</p>

<p>def get_connection
    # Return the database connection…
  end
end
```</p>

<p><em>Which classes are actually aware that DatabaseConnectionManager is a singleton?</em></p>

<p>```ruby
class PreferenceManager
  def initialize
    @reader = PrefReader.new
    @writer = PrefWriter.new
    @preferences = { :display_splash=&gt;false, :background_color=&gt;:blue }
  end</p>

<p>def save_preferences
    preferences = {}
    # Preference are in
    @writer.write(@preferences)
  end</p>

<p>def get_preferences
    @preferences = @reader.read
  end
end</p>

<p>class PrefWriter
  def write(preferences)
    connection = DatabaseConnectionManager.instance.get_connection
    # Write the preferences out
  end
end</p>

<p>class PrefReader
  def read
    connection = DatabaseConnectionManager.instance.get_connection
    # Read the preferences and return them…
  end
end
```</p>

<p>A better approach might be to concentrate the knowledge that <code>DatabaseConnectionManager</code> is a singleton in the <code>PreferenceManager</code> class and simply pass it into the preference reader and writer:</p>

<p>```ruby
class PreferenceManager
  def initialize
    @reader = PrefReader.new
    @writer = PrefWriter.new
    @preferences = { :display_splash=&gt;false, :background_color=&gt;:blue }
  end</p>

<p>def save_preferences
    preferences = {}
    # Preference are in
    @writer.write(DatabaseConnectionManager.instance, @preferences)
  end</p>

<p>def get_preferences
    @preferences = @reader.read(DatabaseConnectionManager.instance)
  end
end
```</p>

<p><strong>Test Interferes</strong></p>

<p>As the Singleton saves the state, there is one exceedingly nasty thing about the Singleton pattern is the way that it interferes with unit testing.</p>

<p>One way to deal with this problem is to create two classes: an ordinary (i.e., non-singleton) class that contains all of the code, and a subclass of the first class that is a singleton. </p>

<p>```ruby
require ‘singleton’</p>

<p>class SimpleLogger
  # All of the logging functionality in this class…
end</p>

<p>class SingletonLogger &lt; SimpleLogger
  include Singleton
end
```</p>

<p>The actual application code uses the <code>SingletonLogger</code>, while the tests can use the plain old, non-singleton <code>Logger</code> class.</p>

<h3 id="in-the-wild-9">In the Wild</h3>

<p><strong>Inflections in ActiveSupport</strong></p>

<p>The <code>Inflections</code> class is a singleton, which saves space and ensures that the same inflection rules are available everywhere.</p>

<p><strong>Rake::Application in rake</strong> <a href="http://ruby-doc.org/stdlib-2.0/libdoc/rake/rdoc/Rake/Application.html">API</a></p>

<p>As it runs, rake—like most build tools—reads in information about what it needs to do: which directories to create, which files to copy, and so on.3 All of this information needs to be available to all of the moving parts of rake, so rake stores it all in a single object (the <code>Rake::Application</code> object, to be precise) that is available as a singleton to the entire rake program.</p>

<h2 id="factory">Factory</h2>

<blockquote>
  <p>picking the right class for the circumstances</p>
</blockquote>

<h3 id="description-9">Description</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/factory.png" alt="factory" /></p>

<p>The GoF called this technique of pushing the “which class” decision down on a subclass the Factory Method pattern.</p>

<ul>
  <li>The <strong>creators</strong> are the base and concrete classes that contain the factory methods.</li>
  <li>The <strong>products</strong> are the objects being created.</li>
</ul>

<p>At its heart, this pattern is really just the Template Method pattern applied to the problem of creating new objects. In both the Factory Method pattern and the Template Method pattern, a generic part of the algorithm is coded in the generic base class, and subclasses fill in the blanks left in the base class.  </p>

<p><strong>Parameterized Factory Method</strong></p>

<p>Parameterized factory method is a method that can produce either a plant or an animal, depending on the symbol that is passed in:</p>

<p>```ruby
class Pond
  def initialize(number_animals, number_plants)
    @animals = []
    number_animals.times do |i|
      animal = new_organism(:animal, “Animal#{i}”)
      @animals « animal
    end</p>

<pre><code>@plants = []
number_plants.times do |i|
  plant = new_organism(:plant, "Plant#{i}")
  @plants &lt;&lt; plant
end   end   # ... end
</code></pre>

<p>class DuckWaterLilyPond &lt; Pond
  def new_organism(type, name)
    if type == :animal
      Duck.new(name)
    elsif type == :plant
      WaterLily.new(name)
    else
      raise “Unknown organism type: #{type}”
    end
  end
end</p>

<p>pond = DuckWaterLilyPond.new(3, 2)
```</p>

<p><strong>Claasses Are Just Objects, Too</strong></p>

<p>While the GoF concentrated on inheritance-based implementations of their fac- tories, we can get the same results with much less code by taking advantage of the fact that in Ruby, classes are just objects.</p>

<p>```ruby
class Pond
  def initialize(number_animals, animal_class,
                 number_plants, plant_class)
    @animal_class = animal_class
    @plant_class = plant_class</p>

<pre><code>@animals = []
number_animals.times do |i|
  animal = new_organism(:animal, "Animal#{i}")
  @animals &lt;&lt; animal
end

@plants = []
number_plants.times do |i|
  plant = new_organism(:plant, "Plant#{i}")
  @plants &lt;&lt; plant
end   end
</code></pre>

<p>def simulate_one_day
    @plants.each {|plant| plant.grow}
    @animals.each {|animal| animal.speak}
    @animals.each {|animal| animal.eat}
    @animals.each {|animal| animal.sleep}
  end</p>

<p>def new_organism(type, name)
    if type == :animal
      @animal_class.new(name)
    elsif type == :plant
      @plant_class.new(name)
    else
      raise “Unknown organism type: #{type}”
    end
  end
end</p>

<p>pond = Pond.new(3, Duck, 2, WaterLily)
pond.simulate_one_day
```</p>

<h3 id="abstract-factory">Abstract Factory</h3>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/abstract-factory.png" alt="abstract factory" /></p>

<p>An object dedicated to creating a compatible set of objects is called an abstract factory. </p>

<p>The problem is that you need to create sets of compatible objects. The solution is that you write a separate class to handle that creation.</p>

<p>The important thing about the abstract factory is that it encapsulates the knowledge of which product types go together. You can express that encapsulation with classes and subclasses, or you can get to it by storing the class objects as we did in the code above. Either way, you end up with an object that knows which kind of things belong together.</p>

<p>```ruby
class PondOrganismFactory
  def new_animal(name)
    Frog.new(name)
  end
  def new_plant(name)
    Algae.new(name)
  end
end</p>

<p>class JungleOrganismFactory
  def new_animal(name)
    Tiger.new(name)
  end
  def new_plant(name)
    Tree.new(name)
  end
end</p>

<p>class Habitat
  def initialize(number_animals, number_plants, organism_factory)
    @organism_factory = organism_factory</p>

<pre><code>@animals = []
number_animals.times do |i|
  animal = @organism_factory.new_animal("Animal#{i}")
  @animals &lt;&lt; animal
end

@plants = []
number_plants.times do |i|
  plant = @organism_factory.new_plant("Plant#{i}")
  @plants &lt;&lt; plant
end   end end
</code></pre>

<p>jungle = Habitat.new(1, 4, JungleOrganismFactory.new)
jungle.simulate_one_day</p>

<p>pond = Habitat.new( 2, 4, PondOrganismFactory.new)
pond.simulate_one_day<br />
```</p>

<p><strong>Claasses Are Just Objects, Too</strong></p>

<p>```ruby
class OrganismFactory
  def initialize(plant_class, animal_class)
    @plant_class = plant_class
    @animal_class = animal_class
  end</p>

<p>def new_animal(name)
    @animal_class.new(name)
  end</p>

<p>def new_plant(name)
    @plant_class.new(name)
  end
end</p>

<p>jungle_organism_factory = OrganismFactory.new(Tree, Tiger)
pond_organism_factory = OrganismFactory.new(WaterLily, Frog)</p>

<p>jungle = Habitat.new(1, 4, jungle_organism_factory)
jungle.simulate_one_day
pond = Habitat.new( 2, 4, pond_organism_factory)
pond.simulate_one_day
```</p>

<p><strong>Naming</strong></p>

<p>Another way that we can simplify the implementation of abstract factories is to rely on a consistent naming convention for the product classes. </p>

<h3 id="factory--abstract-factory">Factory &amp;&amp; Abstract Factory</h3>

<ul>
  <li>The Factory Method pattern is really the Template Method pattern applied to object creation.</li>
  <li>the Abstract Factory pattern is simply the Strategy pattern applied to the same problem.</li>
</ul>

<h3 id="using-and-abusing-10">Using and Abusing</h3>

<p>Not every object needs to be produced by a factory. (<em>You Ain’t Goona Need It</em>).</p>

<p>Engineers do have a tendency to build the Queen Mary (or perhaps the Titanic?) when a canoe will suffice. If you have a choice of exactly one class at the moment, put off adding in a factory.</p>

<h3 id="in-the-wild-10">In the Wild</h3>

<p><strong>Base in ActiveRecord</strong></p>

<p><code>ruby
adapter = "mysql"
method_name = "#{adapter}_connection"
Base.send(method_name, config)
</code></p>

<h2 id="builder">Builder</h2>

<h3 id="description-10">Description</h3>

<p>Builder pattern, a pattern designed to help you configure those complex objects. The builder class takes charge of assembling all of the components of a complex object.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/buidler.png" alt="builder" /></p>

<p>The client of the builder object the <strong>director</strong> because it directs the builder in the construction of the new object (called the <strong>product</strong>). Builders not only ease the burden of creating complex objects, but also hide the implementation details.</p>

<p>The idea behind the Builder pattern is that if your object is hard to build, if you have to write a lot of code to configure each object, then you should factor all of that creation code into a separate class, the builder.</p>

<p>The builders are less concerned about picking the right class and more focused on helping you configure your object.</p>

<ul>
  <li>Take control of configuring your object</li>
  <li>Prevent you from constructing an invalid object</li>
</ul>

<h3 id="code-usage-4">Code Usage</h3>

<p>```ruby
class ComputerBuilder
  attr_reader :computer</p>

<p>def initialize
    @computer = Computer.new
  end</p>

<p>def turbo(has_turbo_cpu=true)
    @computer.motherboard.cpu = TurboCPU.new
  end</p>

<p>def display=(display)
    @computer.display=display
  end</p>

<p>def memory_size=(size_in_mb)
    @computer.motherboard.memory_size = size_in_mb
  end</p>

<p>def add_cd(writer=false)
    @computer.drives « Drive.new(:cd, 760, writer)
  end</p>

<p>def add_dvd(writer=false)
    @computer.drives « Drive.new(:dvd, 4000, writer)
  end</p>

<p>def add_hard_disk(size_in_mb)
    @computer.drives « Drive.new(:hard_disk, size_in_mb, true)
  end
end</p>

<p>builder = ComputerBuilder.new
builder.turbo
builder.add_cd(true)
builder.add_dvd
builder.add_hard_disk(100000)</p>

<p>computer = builder.computer
```</p>

<h3 id="builders-can-ensure-sane-objects">Builders Can Ensure Sane Objects</h3>

<p>That final “give me my object” method makes an ideal place to check that the configuration requested by the client really makes sense and that it adheres to the appropriate business rules.</p>

<p><code>ruby
def computer
  raise "Not enough memory" if @computer.motherboard.memory_size &lt; 250
  raise "Too many drives" if @computer.drives.size &gt; 4
  hard_disk = @computer.drives.find {|drive| drive.type == :hard_disk}
  raise "No hard disk." unless hard_disk
  @computer
end
</code></p>

<h3 id="resuable-buidlers">Resuable Buidlers</h3>

<p>An important issue to consider when writing and using builders is whether you can use a single builder instance to create multiple objects. </p>

<p>One way to deal with this issue is to equip your builder with a <code>reset</code> method, which reinitializes the object under construction.</p>

<p><code>ruby
class LaptopBuilder
  # Lots of code omitted...
  def reset
    @computer = LaptopComputer.new
  end
end
</code></p>

<p>The reset method will let you reuse the builder instance, but it also means that you have to start the configuration process all over again for each computer. If you want to perform the configuration once and then have the builder produce any number of objects based on that configuration, you need to store all of the configuration information in instance attributes and create the actual product only when the client asks for it.</p>

<h3 id="better-builders-with-magic-methods">Better Builders with Magic Methods</h3>

<p>Use ghost method <code>method_missing</code>.</p>

<p>```ruby
def method_missing(name, *args)
  words = name.to_s.split(“_”)
  return super(name, *args) unless words.shift == ‘add’
  words.each do |word|
    next if word == ‘and’
    add_cd if word == ‘cd’
    add_dvd if word == ‘dvd’
    add_hard_disk(100000) if word == ‘harddisk’
    turbo if word == ‘turbo’
  end
end</p>

<p>builder.add_dvd_and_harddisk
builder.add_turbo_and_dvd_and_harddisk
```</p>

<h3 id="using-and-abusing-11">Using and Abusing</h3>

<p>It is usually fairly easy to spot code that is missing a builder: You can find the same object creation logic scat- tered all over the place. Another hint that you need a builder is when your code starts producing invalid objects.====</p>

<p>Builder pattern sometimes creeps up on you as your application becomes increasingly complex.</p>

<h3 id="in-the-wild-11">In the Wild</h3>

<h2 id="interpreter">Interpreter</h2>

<h3 id="description-11">Description</h3>

<p>Interpreter pattern, which suggests that sometimes the best way to solve a problem is to invent a new language for just that purpose. </p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/interpreter.png" alt="Interpreter" /></p>

<p>The heart of the Interpreter pattern is the abstract syntax tree.</p>

<p>The GoF called such values or conditions supplied at the time the AST is interpreted the <code>context</code>.</p>

<ol>
  <li>The parser reads in the program text and produces a data structure, called an abstract syntax tree (AST).</li>
  <li>The AST is evaluated against some set of external conditions, or context, to produce the desired computation.</li>
</ol>

<blockquote>
  <p>ASTs are, in fact, specialized examples of the Composite pattern, with the nonterminal expressions playing the parts of the composites.</p>
</blockquote>

<p>You can supply your clients with an API for building up the tree in code, or you can write a parser that takes strings and turns them into the AST.</p>

<h3 id="with-a-parser">With a Parser</h3>

<p>```ruby
class Parser
  def initialize(text)
    @tokens = text.scan(/(|)|[\w.*]+/)
  end</p>

<p>def next_token
    @tokens.shift
  end</p>

<p>def expression
    token = next_token
    if token == nil
      return nil
    elsif token == ‘(‘
      result = expression
      raise ‘Expected )’ unless next_token == ‘)’
      result
    elsif token == ‘all’
      return All.new
    elsif token == ‘writable’
      return Writable.new
    elsif token == ‘bigger’
      return Bigger.new(next_token.to_i)
    elsif token == ‘filename’
      return FileName.new(next_token)
    elsif token == ‘not’
      return Not.new(expression)
    elsif token == ‘and’
      return And.new(expression, expression)
    elsif token == ‘or’
      return Or.new(expression, expression)
    else
      raise “Unexpected token: #{token}”
    end
  end
end  </p>

<p>parser = Parser.new “and (and(bigger 1024)(filename *.mp3)) writable”
ast = parser.expression
```</p>

<p><strong>Let XML or YAML Do The Parsing</strong></p>

<p>Keep in mind that the main motivation behind building an interpreter is to give your users a natural way to express the kind of processing that needs to be done.</p>

<p><strong>Racc</strong></p>

<p>Racc is modeled (and named) after the venerable UNIX YACC utility. Racc takes as input a description of the grammar for your language and spits out a parser, written in Ruby for that language.</p>

<h3 id="without-a-parser">Without a Parser</h3>

<p>Internal Domain-Specifc Languages.</p>

<p>You may implement your Interpreter pattern in such a way that users could write their pro-grams in actual Ruby code. Maybe you could design your AST API in such a way that the code flows so naturally that your users might be unaware that they are, in fact, writing Ruby code.</p>

<h3 id="using-and-abusing-12">Using and Abusing</h3>

<ul>
  <li>The complexity issue. (The sheer number of components is why the Interpreter pattern is in practice limited to relatively simple languages.)</li>
  <li>Program efficiency, it is probably best to limit your use of the Interpreter pattern to areas that do not demand high performance.</li>
</ul>

<h3 id="in-the-wild-12">In the Wild</h3>

<p><strong>SQL</strong></p>

<p><strong>HTML</strong></p>

<p><strong>Ruby</strong>, of course, an interpreted language.</p>

<p><strong>regular expression</strong></p>

<h2 id="domain-specific-languages">Domain-Specific Languages</h2>

<h3 id="description-12">Description</h3>

<p>The DSL pattern suggests that you should focus on the language itself, not on the interpreter.</p>

<p>External DSLs are external in the sense that there is a parser and an interpreter for the DSL, and there are the programs written in the DSL.</p>

<p>An internal DSL, according to Fowler, is one in which we start with some implementation language, perhaps Ruby, and we simply bend that one language into being our DSL.</p>

<h3 id="using-and-abusing-13">Using and Abusing</h3>

<ol>
  <li>You are limited to what you can parse with a Ruby- based internal DSL.</li>
  <li>Error messages.</li>
</ol>

<h3 id="in-the-wild-13">In the Wild</h3>

<p>The most prominent example of a pure internal DSL in the Ruby world is probably rake, Ruby’s answer to ant or make. </p>

<p><strong>rake</strong>, Ruby’s answer to ant or make.</p>

<h2 id="custom-objects">Custom Objects</h2>

<p>Meta-programming certainly takes a different tack in producing the right object, at its heart this pattern focuses on leveraging the flexibility of Ruby.</p>

<ul>
  <li>We can start with a simple object and add individual methods or even whole modules full of methods to it. </li>
  <li>Using <code>class_eval</code>, we can generate completely new methods at runtime.</li>
  <li>We can take advantage of Ruby’s reflection facilities, which allow a program to examine its own structure</li>
</ul>

<p>A note:</p>

<blockquote>
  <p>The <code>attr_accessor</code> method and its friends live in the module <code>Module</code>, which is included by the <code>Object</code> class. If you go looking for the Ruby code for <code>attr_accessor</code>, <code>attr_reader</code>, and <code>attr_writer</code>, however, you are destined to be disappointed. For the sake of efficiency—but purely for efficiency—these methods are written in C.</p>
</blockquote>

<h3 id="custom-tailoring-technique">Custom-Tailoring Technique</h3>

<p>This custom-tailoring technique is particularly useful when you have lots of orthogonal features that you need to assemble into a single object.</p>

<p>Of course, there really is no rule that says you need to start your customizations with a plain-vanilla instance of Object. In real life, you will likely want to start with an instance of a class that provides some base level of functionality and then tweak the methods from there.</p>

<p>```ruby
def new_animal(diet, awake)
  animal = Object.new
  if diet == :meat
    animal.extend(Carnivore)
  else
    animal.extend(Herbivore)
  end</p>

<p>…
end<br />
```</p>

<p>No matter whether you tailor your objects one method at a time or in module- sized chunks, the ultimate effect is to create a customized object, uniquely made to order for the requirements of the moment.</p>

<h3 id="reflections">Reflections</h3>

<blockquote>
  <p>If you are meta-programming new functionality into your classes on the fly, how can you tell what any given instance can do?</p>
</blockquote>

<p>Reflection features like <code>public_methods</code> and <code>respond_to?</code> are handy anytime but become real assets as you dive deeper and deeper into meta-programming, when what your objects can do depends more on their history than on their class.</p>

<h3 id="using-and-abusing-14">Using and Abusing</h3>

<p>Tests are absolutely mandatory for systems that use a lot of meta- programming.</p>

<h2 id="convention-over-configuration">Convention Over Configuration</h2>

<blockquote>
  <p>The common message is that you should not just take your lan- guage as you find it, but rather mold it into something closer to the tool that you need to solve the problem at hand.</p>
</blockquote>

<h3 id="description-13">Description</h3>

<p>The Convention Over Configuration pattern suggests that you define a convention that a sensible engineer might use anyway.</p>

<ul>
  <li>
    <p>Try to deduce how your users will behave.</p>
  </li>
  <li>
    <p>You can give your user a kick start by supplying him or her with a model, a template, or an example to follow. You could also supply a utility to generate the outline or <strong>scaffold</strong> of a class. It is easy to discount the value of this scaffold-generating script.</p>
  </li>
</ul>

<h3 id="using-and-abusing-15">Using and Abusing</h3>

<p>One danger in building convention-based systems is that your convention might be incomplete, thereby limiting the range of things that your system can do.</p>

<blockquote>
  <p>Our message gateway, for example, does not really do a thorough job of transforming host names into Ruby class names. The code in this chapter will work fine with a simple host name like <em>russolsen.com</em>, transforming it into <em>RussOlsenDotCom</em>. But feed our current system something like <em>icl-gis.co</em>m and it will go looking for the very illegal <em>Icl-gisDotComAuthorizer</em> class.</p>
</blockquote>

<p>Another potential source of trouble is the possibility that a system that uses a lot of conventions may seem like it is operating by magic to the new user. Configuration files may be a pain in the neck to write and maintain, but they do provide a sort of road map—perhaps a very complicated and hard-to-interpret road map, but a map nevertheless—to the inner workings of the system. A well-done convention-based system, by contrast, needs to supply its operational road map in the form of (gasp!) <strong>documentation</strong>.</p>

<p>Also keep in mind that as the convention magic becomes deeper and more complex, you will need ever more thorough unit tests to ensure that your conventions behave, well, conventionally. </p>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://github.com/nslocum/design-patterns-in-ruby">Examples from the book Design Patterns</a> by <a href="https://github.com/nslocum">Nick Slocum</a></li>
</ul>
]]></content>
  </entry>
  
</feed>
