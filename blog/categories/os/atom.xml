<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: OS | Wendi's Blog]]></title>
  <link href="http://blog.ifyouseewendy.com/blog/categories/os/atom.xml" rel="self"/>
  <link href="http://blog.ifyouseewendy.com/"/>
  <updated>2021-03-01T20:51:19-08:00</updated>
  <id>http://blog.ifyouseewendy.com/</id>
  <author>
    <name><![CDATA[wendi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Review] Concurrency - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces/"/>
    <updated>2015-12-26T11:33:57+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/12/26/review-concurrency-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#concurrency">Concurrency</a>    <ul>
      <li><a href="#chapter-26---introduction">Chapter 26 - Introduction</a></li>
      <li><a href="#chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</a></li>
      <li><a href="#chapter-28---locks">Chapter 28 - Locks</a></li>
      <li><a href="#chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</a></li>
      <li><a href="#chapter-30---condition-variables">Chapter 30 - Condition Variables</a></li>
      <li><a href="#chapter-31---semaphores">Chapter 31 - Semaphores</a></li>
      <li><a href="#chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</a></li>
      <li><a href="#chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</a></li>
    </ul>
  </li>
</ul>

<h1 id="concurrency">Concurrency</h1>

<h2 id="chapter-26---introduction">Chapter 26 - Introduction</h2>

<p><strong>Background</strong></p>

<p>With time sharing, we can take a single physical CPU and turn it into multiple virtual CPUs, thus enabling the illusion of multiple programs running at the same time, through time sharing.</p>

<p>With paging (base and bounds, segmentation), we can create the illusion of a large, private virtual memory for each process; this abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk).</p>

<p>But the abstraction of running program we use along is the process, and it’s a classic view of a single point of execution within a program. Now we introduce a new abstraction, thread. And  a <strong>multi-threaded</strong> program has more than one point of execution.</p>

<p>Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus ca access the same data.</p>

<p><strong>Thread vs. Process</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_vs_process.png" alt="os-thread_vs_process.png" /></p>

<p>Address space</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_address_space.png" alt="os-thread_address_space.png" /></p>

<p><strong>Advantage</strong></p>

<p>Efficiency, as they share the same address space.</p>

<ul>
  <li>Save storage</li>
  <li>Easy context switching (no need to change page)</li>
</ul>

<p><strong>Issues</strong></p>

<ul>
  <li><strong>Sharing data</strong>, that of accessing shared variables and the need to support atomicity for critical sections.</li>
  <li><strong>Waiting for another</strong>, sleeping and waking interaction, where one thread must wait for another to complete some action before it continues.</li>
</ul>

<p><strong>Shared Data</strong></p>

<p>The heart of the problem is <strong>uncontrolled scheduling</strong>.</p>

<p>It is a wonderful and hard problem, and should make your mind hurt (a bit). If it doesn’t, then you don’t understand! Keep working until your head hurts; you then know you’re headed in the right directinn.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_sharing_data.png" alt="os-thread_sharing_data.png" /></p>

<p><strong>Key Concurrency Terms</strong> (from Edsger Dijkstra)</p>

<p>A <strong>critical section</strong> is a piece of code that accesses a shared resource, usually a variable or data structure.</p>

<p>A <strong>race condition</strong> arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps un- desirable) outcome. The results depend on the timing execution of the code.</p>

<p>An <strong>indeterminate</strong> program consists of one or more races onditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems.</p>

<p>To avoid these problems, threads should use some kind of <strong>mutual exclusion primitives</strong>; doing so guarantees that only a single thread ever enters a critical section, thus avoiding racoes, and resulting in deterministic program outputs.</p>

<p><strong>Atomic</strong></p>

<p>Atomic operations are one of the most powerful underlying techniques in building computer systems.</p>

<p>The idea behind making a series of actions <strong>atomic</strong> is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a <strong>transaction</strong>.</p>

<p>In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution.</p>

<p><strong>The Wish For Atomicity</strong></p>

<p>Hardware guarantees the instructions is atomic, and provide a general set we call <strong>synchronisation primitives</strong> to ensure atomicity.</p>

<p>Hardware guarantees that the instructions execute atomically. It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state.</p>

<p>But, would we really want the hardware to support an “atomic update of B-tree” instruction?</p>

<p>No. Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call <strong>synchronization primitives</strong>. By using these hardware synchronization primitives, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution.</p>

<p><strong>Why in OS Class?</strong></p>

<p>“History” is the one-word answer; the OS was the first concurrent program, and many techniques were created for use within the OS. Later, with multi-threaded processes, application programmers also had to consider such things.</p>

<p>OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.</p>

<h2 id="chapter-27---interlude-thread-api">Chapter 27 - Interlude: Thread API</h2>

<p><strong>Guidelines</strong></p>

<p>There are a number of small but important things to remember when you use the POSIX thread library.</p>

<ul>
  <li><strong>Keep it simple</strong>. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interactions lead to bugs.</li>
  <li>Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum.</li>
  <li><strong>Each thread has its own stack</strong>. If you have a locally-allocated variable inside of some function a thread is exe- cuting, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the values must be in the heap or otherwise some locale that is globally accessible.</li>
  <li><strong>Be careful with how you pass arguments to, and return values from, threads.</strong> In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong.</li>
  <li><strong>Check your return codes.</strong> Of course, in any C and UNIX program- ming you do, you should be checking each and every return code, and it’s true here as well.</li>
  <li><strong>Always use condition variables to signal between threads.</strong> While it is often tempting to use a simple flag, don’t do it.</li>
  <li><strong>Initialize locks and condition variables.</strong> Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways.</li>
  <li><strong>Use the manual pages.</strong> On Linux, in particular, the pthread man pages (man -k pthread) are highly informative and discuss much of the nuances pre- sented here, often in even more detail.</li>
</ul>

<p><strong>Thread Creation</strong></p>

<p><code>c
#include &lt;pthread.h&gt;
int pthread_create(pthread_t * thread,
                     const pthread_attr_t *  attr,
                     void * (*start_routine)(void*),
                     void *  arg);
</code></p>

<ul>
  <li><code>thread</code>, is a pointer to a structure of type pthread t; we’ll use this structure to interact with this thread</li>
  <li><code>attr</code>, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread.</li>
  <li>The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (<code>start routine</code>), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer).</li>
  <li><code>arg</code>, is exactly the argument to be passed to the function where the thread begins execution.</li>
</ul>

<p><strong><em>Why do we need these void pointers?</em></strong></p>

<p>Having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result.</p>

<p><strong>Thread Completion</strong></p>

<p><code>c
int pthread_join(pthread_t thread, void **value_ptr);
</code></p>

<ul>
  <li><code>thread</code> is used to specify which thread to wait for</li>
  <li><code>value_ptr</code> is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo.png" alt="os-thread_waiting_demo.png" /></p>

<p>Note that one has to be extremely careful with how values are returned from a thread. In particular, never return a pointer which refers to something allocated on the thread’s call stack.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-thread_waiting_demo_wrong.png" alt="os-thread_waiting_demo_wrong.png" /></p>

<p>However, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results.</p>

<p>Not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely. Such long-lived programs thus may not need to join.</p>

<p><strong>Locks</strong></p>

<p>Providing mutual exclusion to a critical section via locks.</p>

<p><code>c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
</code></p>

<p>When you have a region of code you realize is a critical section, and thus needs to be pro- tected by locks in order to operate as desired.</p>

<p>```c
pthread_mutex_t lock;</p>

<p>Pthread_mutex_init(&amp;lock);</p>

<p>Pthread_mutex_lock(&amp;lock);
x = x + 1; // or whatever your critical section is
Pthread_mutex_unlock(&amp;olock);</p>

<p>// Always check for failure
void Pthread_mutex_init(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_init(&amp;lock, NULL); // dynamic initialisation, or PTHREAD_MUTEX_INITIALIZER
    assert(rc == 0); // always check success!
}
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_lock(mutex);
    assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *mutex) {
    int rc = pthread_mutex_unlock(mutex);
    assert(rc == 0);
}
```</p>

<p><strong>Condition Variables</strong></p>

<p>Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue.</p>

<p>To use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of the above routines, this lock should be held.</p>

<p><code>c
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
int pthread_cond_signal(pthread_cond_t *cond);
</code></p>

<p>pthread_cond_wait(), puts the calling thread to sleep, ad thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about.</p>

<p><code>c
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t  cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&amp;lock);
while (ready == 0)
    Pthread_cond_wait(&amp;cond, &amp;lock);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>After initialization of the relevant lock and condition, a thread checks to see if the variable ready has yet been set to something other than zero. If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.</p>

<p><code>c
Pthread_mutex_lock(&amp;lock);
ready = 1;
Pthread_cond_signal(&amp;cond);
Pthread_mutex_unlock(&amp;lock);
</code></p>

<p>Notice 1</p>

<p>When signaling (as well as when modifying the global variable ready), we always make sure to have the lock held. This ensures that we don’t accidentally introduce a race condition into our code.</p>

<p>Notice 2</p>

<p>Notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep.</p>

<p>Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However, before returning after being woken, the pthread_cond_wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.</p>

<p>Notice 3</p>

<p>The waiting thread re-checks the condition in a while loop, instead of a simple if statement. Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not.</p>

<p>Notice 4</p>

<p>Don’t ever use these ad hoc synchronisations.</p>

<p>```c
// waitingnwhile (ready == 0)
    ; // spin</p>

<p>// signaling
ready = 1;
```</p>

<p>First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone.</p>

<p><strong>Others</strong></p>

<p>On the link line, you must also explicitly link with the pthreads library, by adding the -pthread flag.</p>

<p><code>sh
prompt&gt; gcc -o main main.c -Wall -pthread
</code></p>

<h2 id="chapter-28---locks">Chapter 28 - Locks</h2>

<p><strong>The Basic Idea</strong></p>

<p>Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction.</p>

<p>This lock variable (or just “lock” for short) holds the state of the lock at any instant in time. It is either available (or unlocked or free) and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_demo.png" alt="os-lock_demo.png" /></p>

<p>In general, we view thre
ads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code.</p>

<p>The name that the <strong>POSIX</strong> library uses for a lock is a <strong>mutex</strong>, as it is used to provide <strong>mutual exclusion</strong> between threads.</p>

<p><strong>Building A Lock</strong></p>

<p>Some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux).</p>

<p><strong>Evaluating Locks</strong></p>

<ul>
  <li>The first is whether the lock does its basic task, which is to provide <strong>mutual exclusion</strong>. Basically, does the lock work, preventing multiple threads from entering a critical section?</li>
  <li>The second is <strong>fairness</strong>. Does each thread contending for the lock get a fair shot at acquiring it once it is free?</li>
  <li>The final criterion is <strong>performance</strong>, specifically the time overheads added by using the lock.</li>
</ul>

<p><strong>Controlling Interrupts</strong></p>

<p>Turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow.</p>

<p><strong>Plain Solution</strong></p>

<p>Without hardware support, just use a flag.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_plain_solution.png" alt="os-lock_plain_solution.png" /></p>

<p>The core issue is that the testing and setting part can be interrupted by context switch, and both thread enters the critical section.</p>

<p>You should get used to this thinking about concurrent programming. Maybe pretend yourself as a <strong>malicious scheduler</strong> to understand the concurrent execution.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_no_mutal_exclusion.png" alt="os-lock_no_mutal_exclusion.png" /></p>

<p><strong>Test And Set (Atomic Exchange)</strong></p>

<p>Let hardware provides a transaction-like instrument to ensure the sequence of operations is performed <strong>atomically</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_test_and_set.png" alt="os-lock_test_and_set.png" /></p>

<p>The key, of course, is that this sequence of operations is performed atomically. The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_test_and_set.png" alt="os-lock_spin_lock_by_test_and_set.png" /></p>

<p>By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock.</p>

<p><strong>Compare-And-Swap</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_compare_and_swap.png" alt="os-lock_compare_and_swap.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_compare_and_swap.png" alt="os-lock_spin_lock_by_compare_and_swap.png" /></p>

<p>compare-and-swap is a more powerful instruction than test-and-set. We will make some use of this power in the future when we briefly delve into <strong>wait-free synchronisation</strong>.</p>

<p><strong>Load-Linked and Store-Conditional</strong></p>

<p>Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture, for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_load_linked_store_conditional.png" alt="os-lock_load_linked_store_conditional.png" />
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_spin_lock_by_load_linked_store_conditional.png" alt="os-lock_spin_lock_by_load_linked_store_conditional.png" /></p>

<p><strong>Fetch-And-Add</strong></p>

<p>Fetch-and-add atomically increments a value while returning the old value at a particular address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_fetch_and_add.png" alt="os-lock_fetch_and_add.png" /></p>

<p>Fetch-and-add could build a <em>ticket lock</em>, this solution uses a ticket and turn variable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (myturn). The globally shared lock-&gt;turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. It has the advantage of the fairness, ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_ticket_lock_by_fetch_and_add.png" alt="os-lock_ticket_lock_by_fetch_and_add.png" /></p>

<p><strong>Spin Lock</strong></p>

<p>We use a while loop to endlessly check the value of a flag, this technique is known as <strong>spin-waiting</strong>. Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)!</p>

<p><strong>Spin lock</strong> is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a <strong>preemptive scheduler</strong>. (Remember that SJF is non-preemptive, but STCF is preemptive, which means permitting one thread to be interrupted).</p>

<p>Evaluating</p>

<ul>
  <li>√ correctness, the spin lock only allows a single thread to enter the critical section at a time.</li>
  <li>X fairness, spin locks don’t provide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Spin locks are not fair and may lead to starvation.</li>
  <li>X performance, bad in the single CPU case. The problem gets worse with N threads contending for a lock; N − 1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock.</li>
</ul>

<p><strong>Avoid Spinning by Yield</strong></p>

<blockquote>
  <p>“just yield, baby!”</p>
</blockquote>

<p>Hardware support alone cannot solve the problem. We’ll need OS support too! Assume an operating system primitive <strong>yield()</strong> which a thread can call when it wants to give up the CPU and let another thread run. A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller from the <strong>running</strong> state to the <strong>ready</strong> state, and thus promotes another thread to running. Thus, the yielding process essentially <strong>deschedules</strong> itself.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield.png" alt="os-lock_with_test_and_set_and_yield.png" /></p>

<p>This approach eliminates the spinning time, but still costly when context switching. And we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section.</p>

<p><strong>Avoid Spnning by Queues</strong></p>

<p>The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread runs that must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation.</p>

<p>Thus, we must explicitly exert some control over who gets to acquire the lock next after the current holder releases it.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_with_test_and_set_and_yield_and_queue.png" alt="os-lock_with_test_and_set_and_yield_and_queue.png" /></p>

<p>This approach thus doesn’t avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.</p>

<p>With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the first thread would then sleep forever (potentially). This problem is sometimes called the <strong>wakeup/waiting race</strong>.</p>

<p>Solaris solves this problem by adding a third system call: <strong>setpark()</strong>. By calling this routine, a thread can indicate it is about to park. If it then happens t be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.</p>

<p>You might also notice the interesting fact that the flag does not get set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from park(); however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; flag is not set to 0 in-between.</p>

<p><strong>Linux Support</strong></p>

<p>Linux provides something called a <strong>futex</strong> which is similar to the Solaris interface but provides a bit more in-kernel functionality. Specifically, each futex has associated with it a specific physical memory location; associated with each such memory location is an in-kernel queue.</p>

<ul>
  <li><code>futex_wait(address, expected)</code> puts the calling thread to sleep, assouming the value at address is equal to expected. If it is not equal, the call returns immediately.</li>
  <li><code>futex_wake(address)</code> wakes one thread that is wait- ing on the queue.</li>
</ul>

<p>Linux approach has the flavor of an old approach that has been used on and off for years, , and is now referred to as a <strong>two-phase lock</strong>. A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So in the first phase, the lock spins for a while, hoping that it can acquire the lock. However, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.</p>

<h2 id="chapter-29---lock-based-concurrent-data-structures">Chapter 29 - Lock-based Concurrent Data Structures</h2>

<p><strong>Background</strong></p>

<p>Adding locks to a data structure to make it usable by threads makes the structure <strong>thread safe</strong>. There is always a standard method to make a concurrent data structure: add a big lock. But sometimes we need to ensure the scalability.</p>

<p>To evaluate the concurrent data structures, theres are two factors to concern:</p>

<ul>
  <li>Correctness</li>
  <li>Performance. MORE CONCURRENCY ISN’T NECESSARILY FASTER. If the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important. Build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do.</li>
</ul>

<p>Ideally, you’d like to see the threads complete just as quickly on multiple processors as the single thread does on one. Achieving this end is called <strong>perfect scaling</strong>.</p>

<p><strong>Guidelines</strong></p>

<ul>
  <li>Be careful with acquisition and release of locks around control flow changes</li>
  <li>Enabling more concurrency does not necessarily increase performance</li>
  <li>Performance problems should only be remedied once they exist, avoiding premature optimization, is central to any performance-minded developer</li>
  <li>There is no value in making something faster if doing so will not improve the overall performance of the application.</li>
</ul>

<p><strong>Concurrent Counters</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_performance_concurrent_counters.png" alt="os-lock_performance_concurrent_counters.png" /></p>

<p>Traditional Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_traditional_counter.png" alt="os-lock_traditional_counter.png" /></p>

<p>In this manner, it is similar to a data structure built with <strong>monitors</strong>, where locks are acquired and released automatically as you call and return from object methods.</p>

<p>The performance of the synchronized counter scales poorly.</p>

<p>Sloppy Counter</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter.png" alt="os-lock_sloppy_counter.png" /></p>

<p>The sloppy counter works by representing a single logical counter via numerous local physical counters, one per CPU core, as well as a single global counter.
When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock.
How often this local-to-global transfer occurs is determined by a threshold, which we call S here (for sloppiness). The smaller S is, the more the counter behaves like the non-scalable counter above; the bigger S is, the more scalable the counter, but the further off the global value might be from the actual count.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_sloppy_counter_scaling.png" alt="os-lock_sloppy_counter_scaling.png" /></p>

<p><strong>Concurrent Linked Lists</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list.png" alt="os-lock_concurrent_link_list.png" /></p>

<p>One small tricky issue arises if malloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert. This kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths.</p>

<p>BE WARY OF LOCKS AND CONTROL FLOW</p>

<p>Many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone. Thus, it is best to structure code to minimize this pattern.</p>

<p>Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_link_list_optimized.png" alt="os-lock_concurrent_link_list_optimized.png" /></p>

<p>Once again we are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency within a list is something called <strong>hand-over-hand locking</strong> (a.k.a. <strong>lock coupling</strong>).</p>

<p>Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing the list, the code first grabs the next node’s lock and then releases the current node’s lock.</p>

<p>It enables a high degree of concurrency in list operations. However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive. Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating.</p>

<p><strong>Concurrent Queues</strong></p>

<p>Look at a slightly more concurrent queue designed by Michael and Scott.</p>

<p>There are two locks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. One trick used by the Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_queue.png" alt="os-lock_concurrent_queue.png" /></p>

<p><strong>Concurrent Hash Table</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_concurrent_hash_table.png" alt="os-lock_concurrent_hash_table.png" /></p>

<p>This concurrent hash table is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well. The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lock_scaling_hash_table.png" alt="os-lock_scaling_hash_table.png" /></p>

<p>AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)</p>

<blockquote>
  <p>“Premature optimization is the root of all evil.”</p>
</blockquote>

<p>Many operating systems utilized a single lock when first transitioning to multiprocessors, including Sun OS and Linux. In the latter, this lock even had a name, the <strong>big kernel lock (BKL)</strong>. When multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck. Thus, it was finally time to add the optimization of improved concurrency to these systems. Within Linux, the more straightforward approach was taken: replace one lock with many. Within Sun, a more radical decision was made: build a brand new operating system, known as Solaris, that incorporates concurrency more fundamentally from day one.</p>

<h2 id="chapter-30---condition-variables">Chapter 30 - Condition Variables</h2>

<p><strong>Background</strong></p>

<p>There are many cases where a thread wishes to check whether a condition is true before continuing its execution. For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a <code>join()</code>).</p>

<p>In multi-threaded programs, it is often useful for a thread to wait for some conditio to become true before proceeding. The simple approach, of just spinning until the condition becomes true, is grossly inefficient and wastes CPU cycles, and in some cases, can be incorrect.</p>

<p><strong>Definition and Routines</strong></p>

<p>To wait for a condition to become true, a thread can make use of what is known as a condition variable. A <strong>condition variable</strong> is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition) is not as desired (by <strong>waiting</strong> on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by <strong>signaling</strong> on the condition).</p>

<p>By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the famous (and still important) producer/consumer problem, as well as covering conditions.</p>

<p>A condition variable has two operations associated with it: <strong>wait()</strong> and <strong>signal()</strong>.</p>

<ul>
  <li>The <strong>wait()</strong> call is executed when a thread wishes to put itself to sleep</li>
  <li>The <strong>signal()</strong> call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition.</li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo.png" alt="os-cv_waiting_demo.png" /></p>

<p><strong><em>Is the state variable <code>done</code> necessary?</em></strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_waiting_demo_2.png" alt="os-cv_waiting_demo_2.png" /></p>

<p>Yes. Imagine the case where the child runs immediately and calls thr exit() immediately; in this case, the child will signal, but there is no thread asleep on the condition. When the parent runs, it will simply call wait and be stuck; no thread will ever wake it. From this example, you should appreciate the importance of the state variable done; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it.</p>

<p><strong><em>Is there a need to hold the lock while singaling?</em></strong></p>

<p>Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables. The generalization of this tip is correct: hold the lock when calling signal or wait, and you will always be in good shape.</p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>The producer/consumer problem, or sometimes as the bounded buffer problem, which was first posed by Dijkstra. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized <strong>semaphore</strong> (which can be used as either a lock or a condition variable).</p>

<p>A bounded buffer is also used when you pipe the output of one program into another, e.g.,</p>

<p><code>sh
// grep process is the producer
// wc process is the consumer
// between them is an in-kernel bounded buffer
grep foo file.txt | wc -l
</code></p>

<p>Basic operations: <code>put()</code> and <code>get()</code></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_put_and_get_v1.png" alt="os-cv_put_and_get_v1.png" /></p>

<p><strong>Plain Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_v1.png" alt="os-cv_producer_and_consumer_v1.png" /></p>

<p><strong>Single CV and If</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if.png" alt="os-cv_producer_and_consumer_single_cv_and_if.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_if_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_if_trace.png" /></p>

<p><strong>Single CV and While</strong></p>

<p>Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired. This interpretation of what a signal means is often referred to as <strong>Mesa semantics</strong>, after the first research that built a condition variable in such a manner. Virtually every system ever built employs Mesa semantincs.</p>

<p>Thanks to Mesa semantics, a simple rule to remember with condition variables is to <strong>always use while loops</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while.png" alt="os-cv_producer_and_consumer_single_cv_and_while.png" /></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_cv_and_while_trace.png" alt="os-cv_producer_and_consumer_single_cv_and_while_trace.png" /></p>

<p><strong>Two CVs and While</strong></p>

<p>Signaling is clearly needed, but must be more directed. <strong>A consumer should not wake other consumers, only producers</strong>, and vice-versa.</p>

<p>Use two condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Producer threads wait on the condition empty, and signals fill. Conversely, consumer threads wait on fill and signal empty.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_single_two_cv_and_while.png" alt="os-cv_producer_and_consumer_single_two_cv_and_while.png" /></p>

<p><strong>Final Solution</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_final_solution.png" alt="os-cv_producer_and_consumer_final_solution.png" /></p>

<p><strong>Covering Conditions</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-cv_producer_and_consumer_covering_conditions.png" alt="os-cv_producer_and_consumer_covering_conditions.png" /></p>

<p>Assume there are zero bytes free; thread Ta calls <code>allocate(100)</code>, followed by thread Tb which asks for less memory by calling <code>allocate(10)</code>. Both Ta and Tb thus wait on the condition and go to sleep; there aren’t enough free bytes to satisfy either of these requests. At that point, assume a third thread, Tc, calls <code>free(50)</code>. Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed; Ta should remain waiting, as not enough memory is yet free. Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.</p>

<p>The solution suggested by Lampson and Redell is straightforward: replace the <code>pthread_cond_signal()</code> call in the code above with a call to <code>pthread_cond_broadcast()</code>, which wakes up all waiting threads. Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.</p>

<p>Lampson and Redell call such a condition a <strong>covering condition</strong>, as it covers all the cases where a thread needs to wake up (conservatively); the cost, is that too many threads might be woken.</p>

<p>In general, if you find that your program only works when you change your signals to broadcasts (but you don’t think it should need to), you probably have a bug; fix it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available.</p>

<h2 id="chapter-31---semaphores">Chapter 31 - Semaphores</h2>

<p><strong>Background</strong></p>

<p>As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems. One of the first people to realize this years ago was Edsger Dijkstra. Dijkstra and colleagues invented the semaphore as a single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables.</p>

<p><strong>Definition</strong></p>

<p>A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are sem <code>wait()</code> and sem <code>post()</code>. The initial value of the semaphore determines its behaviour.</p>

<p>Semaphores are a powerful and flexible primitive for writing concurrent programs. Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility.</p>

<p>In my view, semaphore is an primitive, which can be made by locks and condition variables, also can’t be used as locks and condition variables.</p>

<p>Initialization</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_init.png" alt="os-semaphore_init.png" /></p>

<p>Usage
<img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_definition.png" alt="os-semaphore_definition.png" /></p>

<ul>
  <li><code>sem_wait()</code> will either return right away (because the value of the semaphore was one or higher when we called <code>sem_wait()</code>), or it will cause the caller to suspend execution waiting for a subsequent post.</li>
  <li><code>sem_post()</code> does not wait for some particular condition to hold like <code>sem_wait()</code> does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.</li>
  <li>The value of the semaphore, when negative, is equal to the number of waiting threads</li>
</ul>

<p><strong>Semaphores As Locks</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_locks.png" alt="os-semaphore_as_locks.png" /></p>

<p>Because locks only have two states (held and not held), this usage is sometimes known as a <strong>binary semaphore</strong>.</p>

<p><strong>Semaphores As Condition Variables</strong></p>

<p>Semaphores are also useful when a thread wants to halt its progress waiting for a
 condition to become true. In this pattern of usage, we often find a thread waiting for something to happen, and a different thread making that something happen and then signaling that it has happened, thus waking the waiting thread.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_as_cv.png" alt="os-semaphore_as_cv.png" /></p>

<p><strong>Producer/Consumer (Bounded Buffer)</strong></p>

<p>Plain Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_plain.png" alt="os-semaphore_producer_and_consumer_plain.png" /></p>

<p>The condition variable (semaphore based) controls the execution order, which can let multiple threads enter the critical section at the same time. It still needs a lock.</p>

<p>Adding Mutual Exclusion</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex.png" alt="os-semaphore_producer_and_consumer_add_mutex.png" /></p>

<p>The consumer holds the mutex and is waiting for the someone to signal full. The producer could si!gnal full but is waiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.</p>

<p>To avoid the deadlock, we can simply move the mutex acquire and release to be just around the critical section. The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_producer_and_consumer_add_mutex_correctly.png" alt="os-semaphore_producer_and_consumer_add_mutex_correctly.png" /></p>

<p><strong>Reader-Writer Locks</strong></p>

<p>Another classic problem stems from the desire for a more flexible <strong>locking primitive</strong> that admits that different data structure accesses might require different kinds of locking.</p>

<p>Imagine a number of concurrent list operations, including inserts and simple lookups. While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a <strong>reader-writer lock</strong>.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_reader_writer_lock.png" alt="os-semaphore_reader_writer_lock.png" /></p>

<p>Once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until all readers are finished; the last one to exit the critical section calls sem <code>post()</code> on “writelock” and thus enables a waiting writer to acquire the lock.</p>

<p>This approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it would be relatively easy for readers to starve writers. It should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives.</p>

<p>SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)</p>

<p>You should never underestimate the notion that the simple and dumb approach can be the best one. Always try the simple and dumb approach first.</p>

<p><strong>The Dining Philosophers</strong></p>

<p>One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem.</p>

<p>There are five “philosophers” sitting around a table. Between each pair of philosophers is a single fork (and thus, five total). The philosophers each have times where they think, and don’t need any forks, and times where they eat. In order to eat, a philosopher needs two forks, both the one on their left and the one on their right. The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers.png" alt="os-semaphore_dinning_philosophers.png" /></p>

<p>Broken Solution</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_deadlock_solution.png" alt="os-semaphore_dinning_philosophers_deadlock_solution.png" /></p>

<p>The problem is deadlock. If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever.</p>

<p>A Solution: Breaking The Dependency</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_dinning_philosophers_solution.png" alt="os-semaphore_dinning_philosophers_solution.png" /></p>

<p><strong>Implement Semaphores</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-semaphore_implementation.png" alt="os-semaphore_implementation.png" /></p>

<h2 id="chapter-32---common-concurrency-problems">Chapter 32 - Common Concurrency Problems</h2>

<p><strong>Background</strong></p>

<p>Lu et al has made a study, which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_bugs.png" alt="os-concurrency_bugs.png" /></p>

<p><strong>Non-Deadlock Bugs</strong></p>

<ul>
  <li>Atomicity violation bugs. The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution). Solve by locks.</li>
  <li>Order violation bugs. The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution). Solve by condition variables.</li>
</ul>

<p><strong>Deadlock Bugs</strong></p>

<p>Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_dependency.png" alt="os-concurrency_deadlock_dependency.png" /></p>

<p><strong>Caused by</strong></p>

<p>One reason is that in large code bases, complex dependencies arise between cmponents. The design of locking strategies in large systems must be carefully done to avoid deadlock in the case of <strong>circular dependencies</strong> that may occur naturally in the code.</p>

<p>Another reason is due to the nature of <strong>encapsulation</strong>. As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_by_encapsulation.png" alt="os-concurrency_deadlock_by_encapsulation.png" /></p>

<p><strong>Conditions for Deadlock</strong></p>

<ul>
  <li><strong>Mutual exclusion</strong>: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).</li>
  <li><strong>Hold-and-wait</strong>: Threads hold resources allocated to them (e.g.,locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).</li>
  <li><strong>No preemption (hold)</strong>: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.</li>
  <li><strong>Circular wait (wait)</strong>: There exists a coircular chain of threads such that each thread holds one more resources (e.g., locks) that are being requested by the next thread in the chain.</li>
</ul>

<p><strong>Prevention Based on Four Conditions</strong></p>

<p>Mutual Exclusion</p>

<p>To avoid the need for mutual exclusion at all. Herlihy had the idea that one could design various data structures to be <strong>wait-free</strong>. The idea here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_wait_free.png" alt="os-concurrency_deadlock_wait_free.png" /></p>

<p>However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.</p>

<p>Hold-and-wait</p>

<p>The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_hold_and_wait.png" alt="os-concurrency_deadlock_hold_and_wait.png" /></p>

<p>By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided.</p>

<p>Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.</p>

<p>No Preemption</p>

<p>Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, a <code>trylock()</code> routine will grab the lock (if it is available) or return -1 indicating that the lock is held right now and that you should try again later if you want to grab that lock.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_no_preemption.png" alt="os-concurrency_deadlock_no_preemption.png" /></p>

<p>One new problem does arise, however: <strong>livelock</strong>. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name lovelock. One could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.</p>

<p>Another issues arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement.</p>

<p>Circular Wait</p>

<p>The best solution in practice is to be careful, develop a lock acquisition order, and thus prevent deadlock from occurring in the first place.</p>

<ul>
  <li>The most straightforward way to do that is to provide a <strong>total ordering</strong> on lock acquisition. For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.</li>
  <li>A <strong>partial ordering</strong> can be a useful way to structure lock acquisition so as to avoid deadlock.</li>
</ul>

<p><strong>Avoidance via Scheduling</strong></p>

<p>Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-concurrency_deadlock_avoid_via_scheduling.png" alt="os-concurrency_deadlock_avoid_via_scheduling.png" /></p>

<p>Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution</p>

<p><strong>Detect and Recover</strong></p>

<p>One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected.</p>

<p>Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted.</p>

<p>DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)</p>

<p>Tom West says famously, “Not everything worth doing is worth doing well”, which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small.</p>

<p><strong>Others</strong></p>

<p>Perhaps the best solution is to develop new concurrent programming models: in systems such as <strong>MapReduce</strong> (from Google), programmers can describe certain types of parallel computations without any locks whatsoever.</p>

<h2 id="chapter-33---event-based-concurrency-advanced">Chapter 33 - Event-based Concurrency (Advanced)</h2>

<p><strong>Background</strong></p>

<p>A different style of concurrent programming is often used in both GUI-based applications as well as some types of internet servers. This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js, but its roots are found in C/UNIX systems that we’ll discuss below.</p>

<p>Event-based servers give control of scheduling to the application itself, but do so at some cost in complexity and difficulty of integration with other aspects of modern systems (e.g., paging). Because of these challenges, no single approach has emerged as best; thus, both threads and events are likely to persist as two different approaches to the same concurrency problem for many years to come.</p>

<p>The problem that event-based concurrency addresses is two-fold.</p>

<ul>
  <li>The first is that managing concurrency correctly in multi-threaded applications can be challenging.</li>
  <li>The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs.</li>
</ul>

<p><strong>The Basic Idea: An Event Loop</strong></p>

<p>The approach is quite simple: you simply wait for something (i.e., an “event”) to occur; when it does, you check what type of  event it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.). That’s it!</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_loop.png" alt="os-event_loop.png" /></p>

<p>Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle ext is equivalent to scheduling. This explicit control over scheduling is one of the fundamental advantages of the event- based approach.</p>

<p>But there is a big question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O? Specifically, how can an event server tell if a message has arrived for it?</p>

<p><strong>An Important API: select() (or poll())</strong></p>

<p>In most systems, a basic API is available, via either the <strong>select()</strong> or <strong>poll()</strong> system calls. Either way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed.</p>

<p>What these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_api.png" alt="os-event_select_api.png" /></p>

<p>First, note that it lets you check whether descriptors can be reand from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).</p>

<p>Second, note the timeout argument. One common usage here is to set the timeout to <code>NULL</code>, which causes <code>select()</code> to block indefinitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to <code>select()</code> to return immediately.</p>

<p>Now linux uses <strong>epoll</strong>, FreeBSD (Mac OS) uses <strong>kqueue</strong>, and Windows uses <strong>IOCP</strong>.</p>

<p>BLOCKING VS. NON-BLOCKING INTERFACES</p>

<ul>
  <li>Blocking (or synchronous) interfaces do all of their work before returning to the caller. The usual culprit in blocking calls is I/O of some kind.</li>
  <li>Non-blocking (or asynchronous) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background. Non-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.</li>
</ul>

<p>DON’T BLOCK IN EVENT-BASED SERVERS</p>

<p>Event-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution the caller can ever be made; failing to obey this design tip will result in a blocked event-based server.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_select_code_demo.png" alt="os-event_select_code_demo.png" /></p>

<p>Advantage</p>

<p>With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Specifically, because only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread because it is decidedly single threaded. Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach.</p>

<p><strong>Issue: Blocking System Calls</strong></p>

<p>For example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request). Both the open() and read() calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service.</p>

<p>With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress. Indeed, this natural <strong>overlap</strong> of I/O and other computation is what makes thread-based programming quite natural and straight-forward.</p>

<p>With an event-based approach, however, there are no other threads to run: just the main event loop. And this implies that if an event handler issues a call that blocks, the entire server will do just that: block until the call completes.</p>

<p>We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed.</p>

<p>Solution: Asynchronous I/O</p>

<p>To overcome this limit, many modern operating systems have intro- duced new ways to issue I/O requests to the disk system, referred to generically as asynchronous I/O. These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed; additional interfaces enable an application to determine whether various I/Os have completed.</p>

<p>The APIs revolve around a basic structure, the struct aiocb or <strong>AIO control block</strong> in common terminology.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_aio_control_block.png" alt="os-event_aio_control_block.png" /></p>

<ul>
  <li>An application can periodically poll the system via a call to aio error() to determine whether said I/O has yet completed.</li>
  <li>Some systems provide an approach based on the interrupt. This method uses UNIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system.</li>
</ul>

<p>In systems without asynchronous I/O, the pure event-based approach cannot be implemented. However, clever researchers have derived methods that work fairly well in their place. For example, Pai et al describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os.</p>

<p>UNIX SIGNALS</p>

<p>A huge and fascinating infrastructure known as <strong>signals</strong> is present in all mod ern UNIX variants. At its simplest, signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a <strong>signal handler</strong>, i.e., some code in the application to handle that signal. When finished, the process just resumes its previous behaviour. A program can be configured to catch that signal. Or when a signal is sent to a process not config- ured to handle that signal, some default behavior is enacted; for SEGV, the process is killed.</p>

<p><strong>Issue: State Management</strong></p>

<p>When an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread. Adya et al. call this work <strong>manual stack management</strong>, and it is fundamental to event-based programming.</p>

<p>Solution: Continuation</p>

<p>Use an old programming language construct known as a <strong>continuation</strong>. Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-event_state_management.png" alt="os-event_state_management.png" /></p>

<p>Record the socket descriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (fd). When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller.</p>

<p><strong>What Is Still Difficult With Events</strong></p>

<p>Multiple CPUS. When systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared. Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel; when doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed. Thus, on modern multicore systems, simple event handling without locks is no longer possible.</p>

<p>Implicit blocking. It does not integrate well with certain kinds of systems activity, such as paging. For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes. Even though the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.</p>

<p>API changes all the time. That event-based code can be hard to manage over time, as the exact semantics of various routines changes]. For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces. Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.</p>

<p>Async network I/O. Though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there, and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think. For example, while one would simply like to use the select() interface to manage all outstanding I/Os, usually some combination of select() for networking and the AIO calls for disk I/O are required.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Review] Virtualization - Operating Systems Three Easy Pieces]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces/"/>
    <updated>2015-11-22T13:44:38+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2015/11/22/review-virtualization-operating-systems-three-easy-pieces</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Book</strong></td>
      <td>Operating Systems: Three Easy Pieces</td>
    </tr>
    <tr>
      <td><strong>Author</strong></td>
      <td><a href="http://www.cs.wisc.edu/~remzi">Remzi H. Arpaci-Dusseau</a> and <a href="http://www.cs.wisc.edu/~dusseau">Andrea C. Arpaci-Dusseau</a></td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/">pages.cs.wisc.edu/~remzi/OSTEP</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#cpu-virtualisation">CPU Virtualisation</a>    <ul>
      <li><a href="#process">Process</a>        <ul>
          <li><a href="#chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</a></li>
          <li><a href="#chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</a></li>
        </ul>
      </li>
      <li><a href="#mechanism">Mechanism</a>        <ul>
          <li><a href="#chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</a></li>
        </ul>
      </li>
      <li><a href="#scheduling">Scheduling</a>        <ul>
          <li><a href="#chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</a></li>
          <li><a href="#chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</a></li>
          <li><a href="#chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</a></li>
          <li><a href="#chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#memory-virtualisation">Memory Virtualisation</a>    <ul>
      <li><a href="#address-space">Address Space</a>        <ul>
          <li><a href="#chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</a></li>
          <li><a href="#chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</a></li>
        </ul>
      </li>
      <li><a href="#dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</a>        <ul>
          <li><a href="#chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</a></li>
          <li><a href="#chapter-16-segmentation">Chapter 16 Segmentation</a></li>
          <li><a href="#chapter-17---free-space-management">Chapter 17 - Free-Space Management</a></li>
        </ul>
      </li>
      <li><a href="#paging">Paging</a>        <ul>
          <li><a href="#chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</a></li>
          <li><a href="#chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</a></li>
          <li><a href="#note-on-cache-management">Note on Cache Management</a></li>
          <li><a href="#chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</a></li>
        </ul>
      </li>
      <li><a href="#beyond-physical-memory">Beyond Physical Memory</a>        <ul>
          <li><a href="#chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</a></li>
          <li><a href="#chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</a></li>
          <li><a href="#chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>

<p><strong>The Crux of the whole book</strong></p>

<p>How does the operating system virtualize resources?
What mechanisms and policies are implemented by the OS to attain virtualization?
How does the OS do so efficiently?</p>

<p><strong>The Von Neumann model of computing</strong></p>

<p>Many millions (and these days, even billions) of times every second, the processor <strong>fetches</strong> an instruction from memory, <strong>decodes</strong> it (i.e., figures out which instruction this is), and <strong>executes</strong> it.</p>

<p><strong>The OS is sometimes known as a resource manager</strong></p>

<p>The primary way the OS does this is through a general technique that we call virtualization. That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more general, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a <strong>virtual machine</strong>.</p>

<p><strong>Virtualizing the CPU</strong></p>

<p>Turning a single CPU (or small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to seemingly run at once is what we call virtualizing the CPU.</p>

<p><strong>Virtualizing the Memory</strong></p>

<p>Memory is just an array of bytes; to <strong>read</strong> memory, one must specify an <strong>address</strong> to be able to access the data stored there; to <strong>write</strong> (or update) memory, one must also specify the data to be written to the given address.</p>

<p>The OS is virtualizing memory. Each process accesses its own private <strong>virtual address space</strong> (sometimes just called its address space)</p>

<p><strong>Concurrency</strong></p>

<p>Three instructions: one to <strong>load</strong> the value of the counter from memory into a register, one to <strong>increment</strong> it, and one to <strong>store</strong> it back into memory. Because these three instructions do not execute atomically (all at once), strange things can happen.</p>

<p><strong>Persistence</strong></p>

<p>The software in the operating system that usually manages the disk is called the <strong>file system</strong>; it is thus responsible for storing any files the user creates in a reliable and efficient manner on the disks of the system.</p>

<p>For performance reasos, most file systems first <strong>delay</strong> such writes for a while, hoping to batch them into larger groups. To handle the problems of system crashes during writes, most file systems incorporate some kind of intricate write protocol, such as <strong>journaling</strong> or <strong>copy-on-write</strong>, carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards.</p>

<p><strong>Design Goals</strong></p>

<p>What an OS actually does: it takes physical <strong>resources</strong>, such as a CPU, memory, or disk, and <strong>virtualizes</strong> them. It handles tough and tricky issues related to <strong>concurrency</strong>. And it stores files <strong>persistently</strong>, thus making them safe over the long-term.</p>

<ol>
  <li>To build up some <strong>abstractions</strong> in order to make the system convenient and easy to use.</li>
  <li>To provide high <strong>performance</strong>, another way to say this is our goal is to minimize the overheads of the OS.</li>
  <li>To provide <strong>protection</strong> between applications, as well as between the OS and applications. Protection is at nthe heart of one of the main principles underlying an operating system, which is that of <strong>isolation</strong>; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.</li>
</ol>

<p><strong>Some History</strong></p>

<ol>
  <li>Early Operating Systems: Just Libraries.  This mode of computing was known as <strong>batch</strong> processing.</li>
  <li>Beyond Libraries: Protection. The idea of a system call was invented. The key difference between a <strong>system call</strong> and a <strong>procedure call</strong> is that a system call transfers control (i.e., jumps) into the OS while simultaneously raising the hardware privilege level. User applications run in what is referred to as user mode which means the hardware restricts what applications can do; When a system call is initiated (usually through a special hardware instruction called a trap), the hardware transfers control to a pre-specified trap handler (that the OS set up previously) and simultaneously raises the privilege level to kernel mode.</li>
  <li>The Era of Multiprogramming by minicomputer. In particular, multiprogramming became commonplace due to the desire to make better use of machine resources. One of the major practical advances of the time was the introduction of the <strong>UNIX</strong> operating system, primarily thanks to <strong>Ken Thompson</strong> (and <strong>Dennis Ritchie</strong>) at Bell Labs (yes, the phone company). <strong>Bill Joy</strong>, made a wonderful distribution (the Berkeley Systems Distribution, or <strong>BSD</strong>) which had some advanced virtual memory, file system, and networking subsystems. Joy later co-founded Sun Microsystems.</li>
  <li>The Modern Era by PC with DOS, Mac OS.</li>
</ol>

<h1 id="cpu-virtualisation">CPU Virtualisation</h1>

<h2 id="process">Process</h2>

<h3 id="chapter-4---the-abstraction-the-process">Chapter 4 - The Abstraction: The Process</h3>

<p><strong>Process</strong></p>

<p>The definition of a process, informally, is quite simple: it is a running program.</p>

<p><strong>How to provide the illusion of many CPUs?</strong></p>

<p>This basic technique, known as <strong>time sharing</strong> of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.</p>

<p><strong>Mechanisms</strong></p>

<p>Mechanisms are low-level methods or protocols that implement a needed piece of functionality.</p>

<p><strong>Policies</strong></p>

<p>On top of these mechanisms resides some of the intelligence in the OS, in the form of policies.</p>

<p><strong>Tip: Separate policy and mechanism</strong></p>

<p>In many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms. You can think of the mechanism as providing the answer to a <strong>how</strong> question about a system; for example, how does an operating system perform a context switch? The policy provides the answer
 to a <strong>which</strong> question; for example, which process should the operating system run right now?</p>

<p><strong>Machine State</strong></p>

<p>To understand what constitutes a process, we thus have to understand its <strong>machine state</strong>: what a program can read or update when it is running. At any given time, what parts of the machine are important to the execution of this program?</p>

<ol>
  <li>Memory. The memory that the process can address (called its <strong>address space</strong>) is part of the process.</li>
  <li>Registry. There are some particularly special registers that form part of this machine state. For example, the <strong>program counter</strong> (PC) (sometimes called the instruction pointer or IP). similarly a stack pointer and associated <strong>frame pointer</strong> are used to manage the stack for function parameters, local variables, and return addresses.</li>
  <li>I/O information. Programs often access persistent storage devices too. Such I/O information might include a list of the files the process currently has open.</li>
</ol>

<p><strong>Process API</strong></p>

<ol>
  <li>Create</li>
  <li>Destroy</li>
  <li>Wait</li>
  <li>Miscellaneous Control (suspend, resume)</li>
  <li>Status</li>
</ol>

<p><strong>How does the OS get a program up and running?</strong></p>

<ol>
  <li>To <strong>load</strong> its code and any static data (e.g., initialized variables) into memory, into the <strong>address space</strong> of the process. In early (or simple) operating systems, the loading process is done <strong>eagerly</strong>; modern OSes perform the process <strong>lazily</strong>, i.e., by loading pieces of code or data only as they are needed during program execution. To truly understand how lazy loading of pieces of code and data works, you’ll have to understand more about the machinery of <strong>paging</strong> and <strong>swapping</strong>.</li>
  <li>Once the code and static data are loaded into memory, there are a few other things the OS needs to do before running the process. Some memory must be allocated for the program’s <strong>run-time stack</strong> (or just stack). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.</li>
  <li>The OS may also allocate some memory for the program’s <strong>heap</strong>. In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling malloc() and free it explicitly by calling free(). The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures.</li>
  <li>The OS will also do some other initialization tasks, particularly as related to input/output (I/O). For example, in UNIX systems, each process by default has three open <strong>file descriptors</strong>.</li>
  <li>To start the program running at the entry point, namely main(), the OS transfers control of the CPU to the newly-created process, and thus the program begins its execution.</li>
</ol>

<p><strong>Process States</strong></p>

<ol>
  <li>Running</li>
  <li>Ready</li>
  <li>Blocked</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-process_state_transitions.png" alt="os-process_state_transitions.png" /></p>

<p><strong>Data Structures</strong></p>

<p>To track the state of each process, for example, the OS likely will keep some kind of <strong>process list</strong> for all processes that are ready, as well as some additional information to track which process is currently running.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-the_xv6_proc_structure.png" alt="os-the_xv6_proc_structure.png" /></p>

<p>The <strong>register context</strong> will hold, for a stopped process, the contents of its registers. When a process is stopped, its registers will be saved to this memory location; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running the process.</p>

<p>Sometimes people refer to the individual structure that stores information about a process as a <strong>Process Control Block (PCB)</strong>.</p>

<h3 id="chapter-5---interlude-process-api">Chapter 5 - Interlude: Process API</h3>

<p>UNIX presents one of the most intriguing ways to create a new process with a pair of system calls:</p>

<p><strong>fork()</strong></p>

<p>The newly-created process (called the <strong>child</strong>, in contrast to the creating <strong>parent</strong>) desn’t start running at main(), like you might expect (note, the “hello, world” message only got printed out once); rather, it just comes into life as if it had called fork() itself. You might have noticed: the child isn’t an exact copy. Specifically, al- though it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of fork() is different.</p>

<p>The output is <strong>not deterministic</strong>. When the child process is created, there are now two active processes in the system that we care about: the parent and the child.</p>

<p><strong>wait()</strong></p>

<p>Adding a wait() call to the code above makes the output <strong>deterministic</strong>.</p>

<p><strong>exec()</strong></p>

<p>It does not create a new process; rather, it transforms the currently running program (formerly p3) into a different running program (wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns.</p>

<p><strong>Why? Motivating The API</strong></p>

<p>Why would we build sucho an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of fork() and exec() is essential in building a UNIX shell, because it lets the shell run code after the call to fork() but before the call to exec(); this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.</p>

<p><strong>How Does Shell Utilise The API?</strong></p>

<p>The shell is just a user program.</p>

<ol>
  <li>It shows you a prompt and then waits for you to type something into it.</li>
  <li>You then type a command (i.e., the name of an executable program, plus any arguments) into it;</li>
  <li>In most cases, the shell then figures out where in the file system the executable resides</li>
  <li>calls fork() to create a new child process to run the command</li>
  <li>calls some variant of exec() to run the command</li>
  <li>waits for the command to complete by calling wait().</li>
  <li>When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command.</li>
</ol>

<p>eg. prompt&gt; wc p3.c &gt; newfile.txt</p>

<p>When the child is created, before calling exec(), the shell closes standard output and opens the file newfile.txt.</p>

<h2 id="mechanism">Mechanism</h2>

<h3 id="chapter-6---mechanism-limited-direct-execution">Chapter 6 - Mechanism: Limited Direct Execution</h3>

<p><strong>The Crux</strong></p>

<ul>
  <li>performance: how can we implement virtualization without adding excessive overhead to the system?</li>
  <li>control: how can we run processes efficiently while retaining control over the CPU?</li>
</ul>

<p>Attaining performance while maintaining control is thus one of the central challenges in building an operating system.</p>

<p><strong>Basic Technique: Limited Direct Execution</strong></p>

<p>The basic idea is straightforward: just run the program you want to run on the CPU, but first make sure to set up the hardware so as to limit what the process can do without OS assistance.</p>

<p>In an analogous manner, the OS “baby proofs” the CPU, by first (during boot time) setting up the <strong>trap handlers</strong> and starting an <strong>interrupt timer</strong>, and then by only running processes in a restricted mode. By doing so, the OS can feel quite assured that processes can run efficiently, only requir- ing OS intervention to perform privileged operations or when they have monopolized the CPU for too long and thus need to be switched out.</p>

<p><strong>Problem #1: Restricted Operations</strong></p>

<p>Use Protected Control Transfer</p>

<p>The hardware assists the OS by providing different modes of execution. In <strong>user mode</strong>, applications do not have full access to hardware resources. In <strong>kernel mode</strong>, the OS has access to the full resources of the machine. When the user process wants to perform some kinds of privileged operation, it can perform a <strong>system call</strong>.</p>

<p><strong>System Call</strong></p>

<p>To execute a system call, a program must execute a special <strong>trap</strong> instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now per- form whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When finished, the OS calls a special <strong>return-from-trap</strong> instruction</p>

<p><strong>Why System Calls Look Like Procedure Calls?</strong></p>

<p>It is a procedure call, but hidden inside that procedure call is the famous trap instruction. More specifically, when you call open() (for example), you are executing a procedure call into the C library. The parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and return values correctly, as well as execute the hardware-specific trap instruction. And now you know why you personally don’t have to write assembly code to trap into an OS; somebody has already written that assembly for you.</p>

<p><strong>How does the trap know which code to run inside the OS?</strong></p>

<p>The kernel does so by setting up a <strong>trap table</strong> at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be. The OS informs the hardware of the locations of these <strong>trap handlers</strong>.</p>

<p><strong>Limited Direct Execution Protocol</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-limited_directed_execution_protocol.png" alt="os-limited_directed_execution_protocol.png" /></p>

<p>There are two phases in the LDE protocol:</p>

<p>In the first (at boot time), the kernel initializes the <strong>trap table</strong>, and the CPU remembers its location for subsequent use.</p>

<p>In the second (when running a process), the kernel sets up a few things (e.g., allocating a node on the process list, allocating memory) before using a <strong>return-from-trap</strong> instruction to start the execution of the process; this switches the CPU to user mode and begins running the process.</p>

<p>Normal flow:</p>

<p>When the process wishes to issue a system call, it traps back into the OS, which handles it and once again returns control via a return-from-trap to the process. The process then completes its work, and returns from main(); this usually will return into some stub code which will properly exit the program (say, by calling the exit() system call, which traps into the OS).</p>

<p><strong>Problem #2: Switching Between Processes</strong></p>

<p>How can the operating system regain control of the CPU so that it can switch between processes?</p>

<p>In a <strong>cooperative</strong> scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some kind to take place.</p>

<p>How can the OS gain control of the CPU even if processes are not being cooperative? What can the OS do to ensure a rogue process does not take over the machine?</p>

<p><strong>Timer Interrupt</strong></p>

<p>A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.</p>

<p>The OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is of course a privileged operation.</p>

<p><strong>Scheduler</strong></p>

<p>Whether to continue running the currently-running process, or switch to a different one. This decision is made by a part of the operating system known as the scheduler.</p>

<p>If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a <strong>context switch</strong>. A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its kernel stack).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-timer_interrupt.png" alt="os-timer_interrupt.png" /></p>

<h2 id="scheduling">Scheduling</h2>

<h3 id="chapter-7---scheduling-introduction">Chapter 7 - Scheduling: Introduction</h3>

<p><strong>Scheduling Metrics</strong></p>

<ul>
  <li>performance
    <ul>
      <li>turnaround = T(completion) - T(arrival)</li>
      <li>responsive time = T(first run) - T(arrival)</li>
    </ul>
  </li>
  <li>fairness</li>
</ul>

<p>Performance and fairness are often at odds in scheduling.</p>

<p>The introduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born: response time.</p>

<p><strong>Assumption</strong></p>

<ol>
  <li>Each job runs for the same amount of time.</li>
  <li>All jobs arrive at the same time.</li>
  <li>Once started, each job runs to completion.</li>
  <li>All jobs only use the CPU (i.e., they perform no I/O)</li>
  <li>The run-time (length) of each job is known.</li>
</ol>

<p><strong>Policy 1-1 FIFO</strong></p>

<p>under assumption: 1,2,3,4,5</p>

<p>Given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm.</p>

<p><strong>Policy 1-2 SJF (Shortest Job First)</strong></p>

<p>under assumption: <del>1,</del>2,3,4,5</p>

<p>Why is FIFO not good?</p>

<p>If Assumption(1) is false, there will be the <strong>convoy effect</strong>, where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.</p>

<p>Is SJF preemptive?</p>

<p>No, it’s <strong>non-preemptive</strong>. In the old days of batch computing, a number of non-preemptive scheulers were developed; such systems would run each job to completi before considering whether to run a new job. Virtually all modern schedulers are <strong>preemptive</strong>, and quite willing to stop one process from running in order to run another.</p>

<p><strong>Policy 1-3 STCF (Shortest Time-to-Completion First) or PSJF (Preemptive Shortest Job First)</strong></p>

<p>under assumption: <del>1,2,3,</del>4,5</p>

<p>Notice that there a significant difference between SJF and STCF. As SJF is non-preemptive, system would run each job to completion before running other jobs. But STCF prefers the shortest time-to-completion jobs, which should preempt CPU to make sense. That’s why STCF also has another name, PSFJ, Preemptive Shortest Job First.</p>

<p><strong>Policy 2 RR (Round-Robin)</strong></p>

<p>The basic idea is simple: instead of running jobs to completion, RR runs a job for a <strong>time slice</strong> (sometimes called a scheduling quantum) and then switches to the next job in the run queue.</p>

<p>The length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance. Thus, de- ciding on the length of the time slice presents a trade-off to a system de- signer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive.</p>

<p>RR, with a reonasonable time slice, is thus an excellent scheduler if response time is our only metric. It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric.</p>

<p><strong>Policy 1 vs. Policy 2</strong></p>

<p>There is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to com- pletion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time. This type of trade-off is common in systems</p>

<p><strong>Incorporate I/O by overlap</strong></p>

<p>under assumption: 4</p>

<p>We see how a scheduler might incorporate I/O. By treating each CPU burst as a job, the scheduler makes sure processes that are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.</p>

<h3 id="chapter-8---scheduling-the-multi-level-feedback-queue">Chapter 8 - Scheduling: The Multi-Level Feedback Queue</h3>

<p><strong>MLFQ</strong></p>

<p>it has <strong>multiple levels of queues</strong>, and <strong>uses feedback to determine the priority</strong> of a given job.</p>

<p>Instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads.</p>

<p><em>Multi-Level</em></p>

<p>The MLFQ has a number of distinct queues, each assigned a different <strong>priority level</strong>. At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run. Of course, more than one job may be on a given queue, and thus have the same priority. In this case, we will just use round-robin scheduling among those jobs.</p>

<p><em>Feedback</em></p>

<p>Thus, the key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.</p>

<p><strong>How To Change Priority</strong></p>

<p>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).
Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).
Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level.</p>

<p><em>Problems</em></p>

<ol>
  <li>Starvation</li>
  <li>Smart user could rewrite their program to game the scheduler.</li>
  <li>A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.</li>
</ol>

<p><strong>How to prevent gaming of our scheduler?</strong></p>

<p>Rules 4a and 4b, let a job retain its priority by relinquishing the CPU before the time slice expires. The solution here is to perform better <strong>accounting</strong> of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue.</p>

<p>Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</p>

<p><strong>Priority Boost</strong></p>

<p>The simple idea here is to periodically boost the priority of all the jobs in system.</p>

<p>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</p>

<p><strong>Tuning MLFQ</strong></p>

<p>One big question is how to <strong>parameterize</strong> such a scheduler.</p>

<ul>
  <li>How many queues should there be?</li>
  <li>How big should the time slice be per queue?</li>
  <li>How often should priority be boosted in order to avoid starvation and account for changes in behavior?</li>
</ul>

<p><em>Some Variants</em></p>

<p>Most MLFQ variants allow for <strong>varying time-slice length</strong> across different queues. The high-priority queues are usually given short time slices; the low-priority queues, in contrast, contain long-running jobs that are CPU-bound; hence, longer time slices work well.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-lower_priority_longer_quanta.png" alt="os-lower_priority_longer_quanta.png" /></p>

<p>The FreeBSD scheduler (version 4.3) uses a formula to calculate the current priority level of a job, basing it on how much CPU the process has used.</p>

<p>Some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow some user advice to help set priorities; for example, by using the command-line utility nice.</p>

<p><strong>Refined Rules</strong></p>

<ul>
  <li>Rule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t).</li>
  <li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR.</li>
  <li>Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).</li>
  <li>Rule 4: Once a job uses up its time allotment at a given level (re- gardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).</li>
  <li>Rule 5: After some time period S, move all the jobs in the system to the topmost queue.</li>
</ul>

<h3 id="chapter-9---scheduling-proportional-share">Chapter 9 - Scheduling: Proportional Share</h3>

<p><strong>0. Basic Idea</strong></p>

<p><strong>Proportional-share scheduler</strong>, also sometimes referred to as a <strong>fair-share scheduler</strong>. Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.</p>

<p><strong>Implementations</strong></p>

<ul>
  <li><strong>lottery</strong> scheduling, lottery uses randomness in a clever way to achieve proportional share</li>
  <li><strong>stride</strong> scheduling, stride does so deterministically</li>
</ul>

<p><strong>Application</strong></p>

<p>One is that such approaches do not particularly mesh well with I/O [AC97]; another is that they leave open the hard problem of ticket assignment, i.e., how do you know how many tickets your browser should be allocated?</p>

<p>As a result, proportional-share schedulers are more useful in domains where some of these problems (such as assignment of shares) are rela- tively easy to solve. For example, in a virtualized data centre.</p>

<p><strong>1. Lottery Scheduling</strong></p>

<p>The basic idea is quite simple: every so often, hold a lottery to determine which process should get to run next; processes that should run more often should be given more chances to win the lottery. One of the most beautiful aspects of lottery scheduling is its use of randomness.</p>

<p><strong>Advantage</strong></p>

<ul>
  <li>randomness
    <ul>
      <li>First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling.</li>
      <li>Second, random also is lightweight, requiring little state to track alternatives.</li>
      <li>Finally, random can be quite fast.</li>
    </ul>
  </li>
  <li>simplicity of implementation</li>
  <li>no global state</li>
</ul>

<p><strong>Disadvantage</strong></p>

<ul>
  <li>Hard to assign tickets to jobs</li>
  <li>Not deterministic. Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired outcome.</li>
</ul>

<p><strong>Ticket</strong></p>

<p>Tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question.</p>

<p><strong>Ticket Mechanisms</strong></p>

<p>Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways.</p>

<ul>
  <li>ticket currency</li>
  <li>ticket transfer</li>
  <li>ticket inflation</li>
</ul>

<p><strong>Implementation</strong></p>

<p>Probably the most amazing thing about lottery scheduling is the simplicity of its implementation.</p>

<ul>
  <li>a good random number generator to pick the winning ticket</li>
  <li>a data structure to track the processes of the system (e.g., a list)</li>
  <li>the total number of tickets.</li>
</ul>

<p><strong>2. Stride Scheduling</strong></p>

<p>a <strong>deterministic</strong> fair-share scheduler.</p>

<p>Respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. We call this value the <strong>stride</strong> of each process.</p>

<p>Jobs A, B, and C, with 100, 50, and 250 tickets. if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40.</p>

<p>Every time a process runs, we will increment a counter for it (called its <strong>pass</strong> value) by its stride to track its global progress. The scheduler then uses the stride and pass to determine which process should run next.</p>

<p>The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride.</p>

<p><strong>Advantage</strong></p>

<p>Lottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right at the end of each scheduling cycle.</p>

<p><strong>Disadvantage</strong></p>

<p>Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there. In this way, lottery makes it much easier to incorporate new processes in a sensible manner.</p>

<h3 id="chapter-10---multiprocessor-scheduling">Chapter 10 - Multiprocessor Scheduling</h3>

<p><em>TODO after reading Concurrency</em></p>

<h1 id="memory-virtualisation">Memory Virtualisation</h1>

<h2 id="address-space">Address Space</h2>

<h3 id="chapter-13---the-abstraction-address-spaces">Chapter 13 - The Abstraction: Address Spaces</h3>

<p><strong>Multiprogramming</strong> (多道程序), in which multiple processes were ready to run at a given time, and the OS would switch between them.</p>

<p><strong>Time sharing</strong>, One way to implement time sharing would be to run one process for a short while, giving it full access to all memory, then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process’s state, run it for a while, and thus implement some kind of crude sharing of the machine. Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows.</p>

<p><strong>Address space</strong></p>

<p>Address space, easy to use abstraction of physical memory, and it is the running program’s view of memory in the system. Understanding this fundamental OS ab- straction of memory is key to understanding how memory is virtualized.</p>

<p>When the OS does this, we say the OS is <strong>virtualizing memory</strong>.</p>

<p><strong>Goals</strong></p>

<p>The VM system is responsible for providing the illusion of a large, sparse, private address space to programs, which hold all of their instructions and data therein.</p>

<ul>
  <li>transparency</li>
  <li>efficiency</li>
  <li>protection (isolation)</li>
</ul>

<p><strong>EVERY ADDRESS YOU SEE IS VIRTUAL</strong></p>

<p>Any address you can see as a programmer of a user-level program is a virtual address, if you print out an address in a program, it’s a virtual one.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-every_address_you_see_is_virtual.png" alt="os-every_address_you_see_is_virtual.png" /></p>

<h3 id="chapter-14---interlude-memory-api">Chapter 14 - Interlude: Memory API</h3>

<p><strong>Types of Memory</strong></p>

<ul>
  <li><strong>stack memory</strong>, allocations and deallocations of it are managed implicitly by the compiler for you, the programmer.</li>
  <li><strong>heap memory</strong>, it is this need for long-lived memory, where all allocations and deallocations are explicitly handled by you, the programmer.</li>
</ul>

<p>Example</p>

<p><code>c
void func() {     int *x = (int *) malloc(sizeof(int));     ... }
</code></p>

<p>First, you might no- tice that both stack and heap allocation occur on this line: first the com- piler knows to make room for a pointer to an integer when it sees your declaration of said pointer (int *x); subsequently, when the program calls malloc(), it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program.</p>

<p><strong>API</strong></p>

<ul>
  <li><strong>malloc()</strong></li>
  <li><strong>free()</strong></li>
</ul>

<p>There are really two levels of memory management in the system. The first is level of memory management is performed by the OS, which hands out memory to processes when they run, and takes them back when processes exit (or otherwise die). The second level of management is within each process, for example within the heap when you call malloc() and free().</p>

<p>They are not system calls, but rather library calls. Thus the malloc library manages space within your virtual address space, but itself is built on top of some system calls.</p>

<ul>
  <li><strong>mmap()</strong></li>
</ul>

<p>You can also obtain memory from the operating system via the <code>mmap()</code> call. By passing in the correct arguments, mmap() can create an anonymous memory region within your program — a region which is not associated with any particular file but rather with swap space. This memory can then also be treated like a heap and managed as such.</p>

<ul>
  <li><strong>calloc()</strong></li>
</ul>

<p>Allocates memory and also zeroes it before returning; this prevents some errors where you assume that memory is zeroed and forget to initialize it yourself.</p>

<ul>
  <li><strong>realloc()</strong></li>
</ul>

<p>when you’ve allocated space for something (say, an array), and then need to add something to it: realloc() makes a new larger region of memory, copies the old region into it, and returns the pointer to the new region.</p>

<p><strong>Common Errors</strong></p>

<ul>
  <li>Forgetting To Allocate Memory - <strong>segmentation fault</strong>, which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY. Forget to allocate memory.</li>
  <li>Not Allocating Enough Memory - <strong>buffer overflow</strong></li>
  <li>Forgetting to Initialize Allocated Memory - <strong>uninitialized read</strong></li>
  <li>Forgetting To Free Memory - <strong>memory leak</strong></li>
  <li>Freeing Memory Before You Are Done With It - <strong>dangling pointer</strong></li>
  <li>Freeing Memory Repeatedly - <strong>double free</strong></li>
</ul>

<p><strong>Tools</strong></p>

<ul>
  <li><strong>gdb</strong>, add -g flag to gcc, then run it with gdb. eg. gcc -g null.c -o null -Wall &amp;&amp; gdb null</li>
  <li><strong>valgrind</strong>, eg. valgrind —leak-check=yes null</li>
</ul>

<h2 id="dynamic-allocation-and-segmentation">Dynamic Allocation and Segmentation</h2>

<h3 id="chapter-15---mechanism-address-translation">Chapter 15 - Mechanism: Address Translation</h3>

<p><strong>hardware-based address translation</strong></p>

<p>With address translation, the hardware transforms each memory access (e.g., an instruction fetch, load, or store), changing the <strong>virtual</strong> address provided by the instruction to a <strong>physical</strong> address where the desired information is actually located.</p>

<p>Transforming a virtual address into a physical address is exactly the technique we refer to as address translation.</p>

<p>Key to the efficiency of this technique is hardware support, which performs the translation quickly for each access, turning virtual addresses (the process’s view of memory) into physical ones (the actual view).</p>

<p><strong>Static (Software-based) Relocation</strong></p>

<p>A piece of software known as the loader takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memory.</p>

<p><strong>Dynamic (Hardware-based) Relocation</strong></p>

<p>The <strong>base and bounds</strong> technique is also referred to as dynamic relocation. With dynamic relocation, a little hardware goes a long way. Namely, a <strong>base</strong> register is used to transform virtual addresses (generated by the program) into physical addresses. A <strong>bounds</strong> (or <strong>limit</strong>) register ensures that such addresses are within the confines of the address space. Together they provide a simple and efficient virtualization of memory.</p>

<p>Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as dynamic relocation.</p>

<p>We should note that the base and bounds registers are hardware stru tures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the <strong>memory management unit (MMU)</strong>.</p>

<p><strong>Disadvantage</strong></p>

<p>The simple approach of using a base and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t fit into memory; thus, base and bounds is not as flexible as we would like.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-base_and_bounds.png" alt="os-base_and_bounds.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>The hardware should provide special instructions to modify the base and bounds registers, allowing the OS to change them when different processes run. These instructions are privileged; only in kernel (or privileged) mode can the registers be modified.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynaimic_relocation_hardware_requirement.png" alt="os-dynaimic_relocation_hardware_requirement.png" /></p>

<p><strong>Operating System Support</strong></p>

<p>The combination of hardware support and OS management leads to the implementation of a simple virtual memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_os_responsibility.png" alt="os-dynamic_relocation_os_responsibility.png" /></p>

<p><strong>Limited Direct Execution Protocol (Dynamic Relocation)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-dynamic_relocation_LDE.png" alt="os-dynamic_relocation_LDE.png" /></p>

<h3 id="chapter-16-segmentation">Chapter 16 Segmentation</h3>

<p><strong>Segmentation: Generalized Base/Bounds</strong></p>

<p>Considering the disadvantage of the simple base and bounds, instead of having just one base and bounds pair in our <strong>MMU</strong>, why not <strong>have a base and bounds pair per logical segment of the address space</strong>? A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different segments: code, stack, and heap.</p>

<p>The hardware structure in our <strong>MMU</strong> required to support segmenta- tion is just what you’d expect: in this case, a set of three base and bounds register pairs.</p>

<p><strong>Advantage</strong></p>

<p>Remove the Inner Fragmentation.</p>

<p>What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation.png" alt="os-segmentation.png" /></p>

<p><strong>THE SEGMENTATION FAULT</strong></p>

<p>The term segmentation fault or violation arises from a memory access on a segmented machine to an illegal address. Humorously, the term persists, even on machines with no support for segmentation at all. Or not so humorously, if you can’t figure why your code keeps faulting</p>

<p><strong>Implementation</strong></p>

<p>One common approach, sometimes referred to as an explicit approach, is to chop up the address space into segments based on the top few bits of the virtual address.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_implementation.png" alt="os-segmentation_implementation.png" /></p>

<p><strong>Hardware Support</strong></p>

<p>Negative growth for stack, and protection bits for code sharing. (to save memory, sometimes it is useful to share certain memory segments between address spaces. In particular, <strong>code sharing</strong> is common and still in use in systems today.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_register_with_protection.png" alt="os-segmentation_register_with_protection.png" /></p>

<p><strong>Fine-grained vs. Coarse-grained Segmentation</strong></p>

<ul>
  <li>Coarse-grained, with just a few segments (i.e., code, stack, heap).</li>
  <li>Fine-grained, to consist of a large number smaller segments, with (further hardware support) a <strong>segment table</strong> of some kind stored in memory.</li>
</ul>

<p><strong>Disadvantage</strong></p>

<p>The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones. We call this problem <strong>external fragmentation</strong>.</p>

<p>Because segments are variablesized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be difficult. One can try to use smart algorithms or periodically compact memory, but the problem is fundamental and hard to avoid. (compact physical memory by rearranging the existing segments, is memory-intensive and generally uses a fair amount of processor time.)</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-segmentation_compact_memory.png" alt="os-segmentation_compact_memory.png" /></p>

<p>Segmentation still isn’t flexible enough to support our fully generalized, sparse address space.</p>

<h3 id="chapter-17---free-space-management">Chapter 17 - Free-Space Management</h3>

<p>Managing free space can certainly be easy, as we will see when we discuss the concept of paging. It is easy when the space you are managing is divided into fixed-sized units; in such a case, you just keep a list of these fixed-sized units; when a client requests one of them, return the first entry.</p>

<p>Where free-space management becomes more difficult (and interesting) is when the free space you are managing consists of variable-sized units; this arises in a user-level memory-allocation library (as in malloc() and free()) and in an OS managing physical memory when using segmentation to implement virtual memory. In either case, the problem that exists is known as <strong>external fragmentation</strong>: the free space gets chopped into little pieces of different sizes and is thus fragmented; subsequent requests may fail because there is no single contiguous space that can satisfy the request, even though the total amount of free space exceeds the size of the request.</p>

<p><strong>Target</strong></p>

<p>The more you know about the exact workload presented to an <strong>allocator</strong>, the more you could do to tune it to work better for that workload.</p>

<p><strong>Assumptions</strong></p>

<p>Focus on the great history of allocators found in user-level memory-allocation libraries. The space that this library manages is known historically as the heap, and the geeric data structure used to manage free space in the heap is some kind of <strong>free list</strong>. This structure contains references to all of the free chunks of space in the managed region of memory.</p>

<p>Example</p>

<p>void free(void *ptr) takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when freeing the space, does not inform the library of its size; thus, the library must be able to figure out how big a chunk of memory is when handed just a pointer to it.</p>

<p><strong>Splitting and Coalescing</strong></p>

<ul>
  <li>The split is commonly used in allocators when requests are smaller than the size of any particular free chunk.</li>
  <li>Coalesce free space when a chunk of memory is freed.</li>
</ul>

<p><strong>Tracking The Size Of Allocated Regions</strong></p>

<p>To accomplish this task, most allocators store a little bit of extra information in a <strong>header</strong> block which is kept in memory, usually just before the handed-out chunk of memory.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewenndy.github.io/raw/source/image-repo/os-free_space_management_non_coalesced_free_list.png" alt="os-free_space_management_non_coalesced_free_list.png" /></p>

<h2 id="paging">Paging</h2>

<h3 id="chapter-18---paging-introduction">Chapter 18 - Paging: Introduction</h3>

<p><strong>Background</strong></p>

<p>The operating system takes one of two approaches when solving most any space-management problem.</p>

<ol>
  <li>The first approach is to chop things up into <strong>variable-sized</strong> pieces, as we saw with segmenta- tion in virtual memory.</li>
  <li>To chop up space into <strong>fixed-sized</strong> pieces. In virtual memory, we call this idea paging.</li>
</ol>

<p><strong>Page vs. Page Frame</strong></p>

<ul>
  <li>From perspective of address space, the fixed-sized unit is called page.</li>
  <li>From perspective of physical space, the fixed-sized unit is called page frame.</li>
</ul>

<p>So, the address translation is to translate page to relevant page frame.</p>

<p><strong>32 bits vs. 64 bits</strong></p>

<p>Sometimes we say the OS is 32 bits or 64 bits, we may infer that</p>

<ul>
  <li>32 bits OS has 4GB address space</li>
  <li>64 bits OS has 10mGB address space</li>
</ul>

<p><strong>Advantage</strong></p>

<ul>
  <li>First, it does not lead to external fragmentation, as paging (by design) divides memory into fixed-sized units.</li>
  <li>Second, it is quite flexible, enabling the sparse use of virtual address spaces.</li>
</ul>

<p><strong>Translation</strong></p>

<p>To translate this virtual address that the process generated, we have to first split it into two components: the <strong>virtual page number (VPN)</strong>, and the <strong>offset</strong> within the page.</p>

<p>With our virtual page number, we can now index our page table, to get the <strong>physical frame number (PFN)</strong> (also sometimes called the <strong>physical page number or PPN</strong>).</p>

<p>Note the offset stays the same (i.e., it is not translated), because the offset just tells us which byte within the page we want.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_address_translation_process.png" alt="os-paging_address_translation_process.png" /></p>

<p><strong>Page Table</strong></p>

<p>The operating system usually keeps a per-process data structure known as a page table.</p>

<p>One of the most important data structures in the memory management subsystem of a modern OS is the page table. In general, a page table stores virtual-to-physical address translations</p>

<p>The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical addresses (physical frame numbers). The OS indexes the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_page_table.png" alt="os-paging_page_table.png" /></p>

<p><strong>Storage</strong></p>

<p>Because page tables are so big, we don’t keep any special on-chip hard- ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in memory somewhere.</p>

<p><strong>Page Table Entry (PTE)</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_x86_pte_example.png" alt="os-paging_x86_pte_example.png" /></p>

<p><strong>Page Table Base Register (PTBR)</strong></p>

<p>PTBR contains the physical address of the starting location of the page table.</p>

<p>Code Example</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_access_memory_code_demo.png" alt="os-paging_access_memory_code_demo.png" /></p>

<h3 id="chapter-19---paging-faster-translations-tlbs">Chapter 19 - Paging: Faster Translations (TLBs)</h3>

<p><strong>Background</strong></p>

<p>Using paging as the core mechanism to support virtual memory can lead to high performance overheads. By chopping the address space into small, fixed-sized units (i.e., pages), paging requires a large amount of mapping information. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow.</p>

<p><strong>Translation Lookaside Buffer (TLB)</strong></p>

<p>To speed address translation, we are going to add what is called (for historical reasons) a <strong>translation-lookaside buffer</strong>, or <strong>TLB</strong>. A TLB is part of the chip’s <strong>memory-management unit (MMU)</strong>, and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an <strong>address-translation cache</strong>.</p>

<p><strong>Advantage</strong></p>

<p>By providing a small, dedicated on-chip TLB as an address-translation cache, most memory references will hopefully be handled without having to access the page table in main memory.</p>

<p><strong>Algorithm</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow.png" alt="os-paging_tlb_control_flow.png" /></p>

<p>Goal is to improve the TLB <strong>hit rate</strong>.</p>

<p><strong>TLB Content</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_content.png" alt="os-paging_tlb_content.png" /></p>

<p>TLB contains both VPN and PFN in each entry, in hardware terms, the TLB is known as a <strong>fully-associative</strong> cache.</p>

<p><strong>TLB Miss Handling</strong></p>

<p>Two answers are possible: the hardware, or the software (OS).</p>

<p>A modern system that uses <strong>software-managed TLBs</strong>. On a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler. As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_control_flow_os_handled.png" alt="os-paging_tlb_control_flow_os_handled.png" /></p>

<p><strong>Performance Matters</strong></p>

<p>Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties. The idea behind hardware caches is to take advantage of <strong>locality</strong> in instruction and data references. Hardware caches, whether for instructions, data, or address translations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory.</p>

<ol>
  <li><strong>spatial locality</strong>, the idea is that if a program accesses memory at address x, it will likely soon access memory near x.</li>
  <li><strong>temporal locality</strong>, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future.</li>
  <li>page size, why don’t we just make bigger caches and keep all of our data in them? Because any large cache by definition is slow, and thus defeats the purpose.</li>
</ol>

<p><strong>Issue 1: Context Switch</strong></p>

<p>Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.</p>

<ol>
  <li><strong>flush</strong> the TLB on context switches, thus emptying it before running the next process. But there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high.</li>
  <li><strong>address space identifier (ASID)</strong>, which you can think of the ASID as a process identifier (PID), to enable sharing of the TLB across context switches.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_with_asid.png" alt="os-paging_tlb_with_asid.png" /></p>

<p><strong>Issue 2: Replacement Policy</strong></p>

<p>When we are installing a new entry in the TLB, we have to replace an old one, which one to replace?</p>

<ul>
  <li><strong>least-recently-used (LRU)</strong></li>
  <li><strong>random policy</strong></li>
</ul>

<p>LRU tries to take advantage of locality in the memory-reference stream, and what the random policy exists for?</p>

<p>Random policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over n + 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better.</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>
    <p>Exceeding the TLB coverage, and it can be quite a problem for certain programs. Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed.</p>

    <p><strong>RAM isn’t always RAM</strong>. Sometimes randomly accessing your address space, particular if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties. Because one of our advisors, David Culler, used to always point to the TLB as the source of many performance problems, we name this law in his honor: <strong>Culler’s Law</strong>.</p>
  </li>
  <li>
    <p>TLB access can easily become a bottleneck in the CPU pipeline, in particular with what is called a <strong>physically-indexed cache</strong>. With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. A <strong>virtually-indexed cach</strong>e solves some performance problems, but introduces new issues into hardware design as well.</p>
  </li>
</ol>

<h3 id="note-on-cache-management">Note on Cache Management</h3>

<p>Define cache miss and hit, and goal is to improve the cache rate. Normally, better <strong>replacement policy</strong> lead to higher cache rate.</p>

<p><strong>Find the best replacement policy</strong></p>

<ul>
  <li>Find the optimal</li>
  <li>Find the easiest</li>
  <li>Improve toward optimal, considering Principle of Locality</li>
  <li>Think about corner case</li>
</ul>

<p><strong>Reference: Optimal Replacement Policy</strong></p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Reference: Easiest Replacement Policy</strong></p>

<p>Random policy, with an extraordinary advantage, can avoid corner case.</p>

<p><strong>Reference: Principle of Locality</strong></p>

<p>Programs tend to access certain code sequences (e.g., in a loop) and data structures (e.g., an array accessed by the loop) quite frequently.</p>

<ul>
  <li>spatial locality</li>
  <li>temporal locality, e.g., LRU</li>
  <li>operation expense, e.g., When swapping out pages, dirty pages are much more expensive</li>
</ul>

<p><strong>Reference: Types of Cache Misses</strong></p>

<p>In the computer architecture world, architects sometimes find it useful to characterize misses by type, into one of three categories, sometimes called the Three C’s.</p>

<ul>
  <li><strong>Compulsory miss</strong> (cold-start miss) occurs because the cache is empty to begin with and this is the first reference to the item.</li>
  <li><strong>Capacity miss</strong> occurs because the cache ran out of space and had to evict an item to bring a new item into the cache.</li>
  <li><strong>Conflict miss</strong> arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as set-associativity; it does not arise in the OS page cache because such caches are always fully-associative, i.e., there are no restrictions on where in memory a page can be placed.</li>
</ul>

<h3 id="chapter-20---paging-smaller-tables">Chapter 20 - Paging: Smaller Tables</h3>

<p><strong>Crux</strong></p>

<p>How to get rid of all those invalid regions in the page table instead of keeping them all in memory?</p>

<p><strong>Background</strong></p>

<p>Page tables are t big and thus consume too much memory.</p>

<p>Assume again a 32-bit address space (2^32 bytes), with 4KB (2^12 byte) pages and a 4-byte page-table entry. An address space thus has roughly one million virtual pages in it ( 2^20 ); multiply by the page-table entry size and you see that our page table is 4MB in size. Recall also: we usually have one page table for every process in the system! With a hundred active processes (not uncommon on a modern system), we will be allocating hundreds of megabytes of memory just for page tables!</p>

<p><strong>Solution 1 - Bigger Pages</strong></p>

<p>Big pages lead to waste within each page, a problem known as internal fragmentation. Thus, most systems use relatively small page sizes in the common case: 4KB (as in x86).</p>

<p><strong>Solution 2 - Hybrid Approach: Paging and Segments</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_tlb_hybrid_approach.png" alt="os-paging_tlb_hybrid_approach.png" /></p>

<p><strong>Algorithm</strong></p>

<p>Instead of having a single page table for the entire address soopace of the process, have one per logical segment. In this example, we might thus have three page tables.</p>

<p>Remember with segmentation, we had a <strong>base</strong> register that told us where each segment lived in physical memory, and a <strong>bound</strong> or limit register that told us the size of said segment.</p>

<ol>
  <li>Each logical segment (code, stack, and heap) has one page table.</li>
  <li>Each segment has one pair of base and bounds resisters.</li>
  <li>Base register points to the page table of the segment, and bounds is used to indicate the end of the page table.</li>
</ol>

<p><strong>Advantage</strong></p>

<p>In this manner, our hybrid approach realizes a significant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).</p>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>It still requires us to use segmentation, as it assumes a certain usage pattern of the address space; if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste.</li>
  <li>This hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, finding free space for them in memory is more complicated.</li>
</ol>

<p><strong>Solution 3 - Multi-level Page Tables</strong></p>

<p>It turns the linear page table into something like a tree (<strong>page directory</strong>). This approach is so effective that many modern systems employ it (e.g., x86).</p>

<p><strong>Algorithm</strong></p>

<p>First, chop up the page table into page-sized units; if an entire page of page-table entries (PTEs) is invalid, don’t allocate that page of the page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory. The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.</p>

<p>The page directory, in a simple two-level table, contains one entry per page of the page table. It consists of a number of <strong>page directory entries (PDE)</strong>. A PDE (minimally) has a <strong>valid bit</strong> <strong>and a page frame number (PFN)</strong>, similar to a PTE.</p>

<p>VA contains VPN and offset, and VPN can be splitted into <strong>page directory index</strong> and <strong>page table index</strong>.</p>

<ol>
  <li>Use <strong>page directory index</strong> to search page directory, to get <strong>page directory entry</strong>, to get <strong>page frame number</strong>, to get the specific <strong>page table</strong>.</li>
  <li>Use <strong>page table index</strong> to search the page table, to get <strong>page table entry</strong>, to get the real <strong>physical frame number</strong>.</li>
</ol>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo.png" alt="os-paging_multi_level_page_table_demo.png" /></p>

<p>Demo code</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_demo_code.png" alt="os-paging_multi_level_page_table_demo_code.png" /></p>

<p><strong>Advantage</strong></p>

<ol>
  <li>The multi-level table only allocates page-table space in proportion to the amount of address space you are usig; thus it is generally compact and supports sparse address spaces.</li>
  <li>
    <p>If carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page table.</p>

    <p>Contrast this to a simple (non-paged) linear page table, for a large page table (say 4MB), finding such a large chunk of unused contiguous free physical memory can be quite a challenge. With a multi-level structure, the indirection allows us to place page-table pages wherever we would like in physical memory.</p>
  </li>
</ol>

<p><strong>Disadvantage</strong></p>

<ol>
  <li>Time-space trade-off. It should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself).</li>
  <li>Another obvious negative is complexity. Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubt- nedly more involved than a simple linear page-table lookup.</li>
</ol>

<p><strong>Example</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example.png" alt="os-paging_multi_level_page_table_example.png" /></p>

<p>Virtual Address format</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_va.png" alt="os-paging_multi_level_page_table_example_va.png" /></p>

<p>Explanation</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-paging_multi_level_page_table_example_explanation.png" alt="os-paging_multi_level_page_table_example_explanation.png" /></p>

<p><strong>Issues</strong></p>

<p><strong><em>What if the page directory gets too big?</em></strong></p>

<p>Make it more than two levels, add index to page directory index.</p>

<p><strong><em>How to make it extreme space savings?</em></strong></p>

<p>Inverted page tables. Instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each physical page of the system. The entry tells us which process is using this page, and which virtual page of that process maps to this physical page.</p>

<p>A hash table is often built over the base structure to speed lookups.</p>

<p><strong><em>How to choose page table size?</em></strong></p>

<p>In a memory-constrained system (like many older systems), small structures make sense; in a system with a reasonable amount of memory and with workloads that actively use a large number of pages, a bigger table that speeds up TLB misses might be the right choice.</p>

<p><strong><em>What if the page tables are too big to fit into memory all at once?</em></strong></p>

<p>Thus far, we have assumed that page tables reside in kernel-owned physical memory. Some systems place such page tables in <strong>kernel virtual memory</strong>, thereby allowing the system to swap some of these page tables to disk when memory pressure gets a little tight.</p>

<h2 id="beyond-physical-memory">Beyond Physical Memory</h2>

<h3 id="chapter-21---beyond-physical-memory-mechanisms">Chapter 21 - Beyond Physical Memory: Mechanisms</h3>

<p><strong>Background</strong></p>

<p>In fact, we’ve been assuming that every address space of every running process fits into memory. We will now relax these big assumptions, and assume that we wish to support many concurrently-running large address spaces.</p>

<p>To support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren’t in great demand. In modern systems, this role is usually served by a hard disk drive.</p>

<p><strong>Mechanism</strong></p>

<p>To do so requires more complexity in page-table structures, as a <strong>present bit</strong> (of some kind) must be included to tell us whether the page is present in memory or not. When not, the operating system <strong>page-fault handler</strong> runs to service the <strong>page fault</strong>, and thus arranges for the transfer of the desired page from disk to memory, perhaps first replacing some pages in memory to make room for those soon to be swapped in.</p>

<p><strong>Swap Space</strong></p>

<p>To reserve some space on the disk for moving pages back and forth. We will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to remember the <strong>disk address</strong> of a given page (PTE).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_example.png" alt="os-swap_example.png" /></p>

<p>The size of the swap space is important, as ultimately it determines the <strong>maximum number of memory pages</strong> that can be in use by a system at a given time.</p>

<p>We should note that swap space is not the only on-disk location for swapping traffic.</p>

<blockquote>
  <p>For example, assume you are running a program binary (e.g., ls, or your own compiled main program). The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution, or, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re-use the memry space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system.</p>
</blockquote>

<p><strong>Present Bit</strong></p>

<p>OS use this piece of information in each page-table entry to flag if the page is in physical memory or swap space.</p>

<p>If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above; if it is set to zero, the page is not in memory but rather on disk somewhere.</p>

<p><strong>Page Faut</strong></p>

<p>The act of accessing a page that is not in physical memory is commonly referred to as a <strong>page fault</strong> (it should be called a <strong>page miss</strong>. But when something the hardware doesn’t know how to handle occurs, the hardware simply transfers control to the OS. In perspective of the hardware it is a page fault).</p>

<p><strong>Page Fault Handler</strong></p>

<p>Upon a page fault, the OS is invoked to service the page fault. A particular piece of code, known as a <strong>page-fault handler</strong>, runs, and must service the page fault.</p>

<p>The appropriately-named <strong>OS page-fault handler</strong> runso to determine what to do. Virtually all systems handle page faults in software; even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.</p>

<p><strong>Page Fault Control Flow</strong></p>

<p>Hardware</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow.png" alt="os-swap_page_fault_control_flow.png" /></p>

<p>Software</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-swap_page_fault_control_flow_software.png" alt="os-swap_page_fault_control_flow_software.png" /></p>

<p>How to handle or how will the OS know where to find the desired page?</p>

<ol>
  <li>The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.</li>
  <li>When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN field of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction.</li>
  <li>Then generate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB when servicing the page fault to avoid this step)</li>
  <li>Finally, a last restart would find the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address.</li>
</ol>

<p>Note that while the I/O is in flight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes while the page fault is being serviced.</p>

<p><strong><em>What If Memory Is Full?</em></strong></p>

<p>OS might like to first page out one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or replace is known as the <strong>page-replacement policy</strong>.</p>

<p><strong><em>When Replacements Really Occur?</em></strong></p>

<p>There are many reasons for the OS to keep a small portion of memory free more proactively. To keep a small amount of memory free, most operating systems thus have some kind of <strong>high watermark (HW)</strong> and <strong>low watermark (LW)</strong> to help decide when to start evicting pages from memory.</p>

<p>When the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The background thread, sometimes called the <strong>swap daemon</strong> or <strong>page daemon</strong>, then goes to sleep, happy that it has freed some memory for running processes and the OS to use.</p>

<p>So, instead of performing a replacement directly, the algorithm would instead simply check if there are any free pages available. If not, it would inform the <strong>page daemon</strong> that free pages are needed; when the thread frees up some pages, it would re-awaken the original thread, which could then page in the desired page and go about its work.</p>

<p><strong><em>How To Make Replacement Efficient?</em></strong></p>

<p>Many systems will cluster or group a number of pages and write them out at once to the swap partition, thus increasing the efficiency of the disk.</p>

<h3 id="chapter-22---beyond-physical-memory-policies">Chapter 22 - Beyond Physical Memory: Policies</h3>

<p><strong>Background</strong></p>

<p>In such a case, this memory pressure forces the OS to start <strong>paging out</strong> pages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the <strong>replacement policy</strong> of the OS.</p>

<p><strong>Cache Management</strong></p>

<p>Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. And our goal as maximizing the number of <strong>cache hits</strong>.</p>

<p>Knowing the number of cache hits and misses let us calculate the <strong>average memory access time (AMAT)</strong> for a program.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_amat.png" alt="os-replacement_amat.png" /></p>

<p>Example</p>

<p>Suppose T(M) = 100ns (10^-7), T(D) = 10ms (10^-2)</p>

<ul>
  <li>P(Hit) = 90%, P(Miss) = 10%, AMAT = 1ms + 90ns</li>
  <li>P(Hit) = 99.9%, P(Miss) = 0.1%, AMAT = 0.01ms + 99.9ns</li>
</ul>

<p>The cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs.</p>

<p><strong>Polices</strong></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_summary.png" alt="os-replacement_summary.png" /></p>

<p><strong>Policy 1. Optimal Replacement Policy</strong></p>

<p>Replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses.</p>

<p>In the development of scheduling policies, the future is not generally known; you can’t build the optimal policy for a general-purpose operating system.</p>

<p>Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies.</p>

<ul>
  <li>It makes your improvement meaningful, comparing to optimal policy</li>
  <li>It can show you how much improvement still possible</li>
  <li>It can tell you when to stop making your policy better, because it is close enough to the ideal</li>
</ul>

<p><strong>Policy 2. FIFO</strong></p>

<p>Normal efficiency, easy to implement, and has corner case.</p>

<p>In some cases, when increasing the cache size, hit rate may get lower. This odd behavior is generally referred to as <strong>Belady’s Anomaly</strong>.</p>

<p><strong>Policy 3. Random</strong></p>

<p>Normal efficiency, easy to implement, but remember, it can avoid corner case.</p>

<p><strong>Policy 4. LRU</strong></p>

<p>LRU has what is known as a stack property. When increasing the cache size, hit rate will either stay the same or improve.</p>

<p><strong>Comparison with Workload</strong></p>

<p>No locality workload</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_no_locality_workload.png" alt="os-replacement_no_locality_workload.png" /></p>

<p>The 80-20 Workload, 80% of the references are made to 20% of the pages (the “hot” pages).</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload.png" alt="os-replacement_80_20_workload.png" /></p>

<p>The Looping-Sequential Workload</p>

<p>Looping sequential workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, …, up to page 49, and then we lp, repeating those accesses.</p>

<p>It represents a worst-case for both LRU and FIFO, but no influence on Random. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_looping_sequential_workload.png" alt="os-replacement_looping_sequential_workload.png" /></p>

<p><strong>Implementation - Approximating LRU</strong></p>

<p>To keep track of which pages have been least- and most-recently used, the system has to do some accounting work on every memory reference. Unfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive.</p>

<p>Idea</p>

<p>Approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a <strong>use bit</strong> (sometimes called the <strong>reference bit</strong>).</p>

<ul>
  <li>Whenever a page is referenced (i.ooe., read or written), the use bit is set by hardware to 1.</li>
  <li>The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS.</li>
</ul>

<p>Implementation by Clock Algorithm</p>

<ul>
  <li>Imagine all the pages of the system arranged in a circular list. A clock hand points to some particular page to begin with.</li>
  <li>When a replacement must occur, the OS iterating the circular list checking on use bit.
    <ul>
      <li>If 1, clear use bit to 0, and find next</li>
      <li>If 0, use it</li>
    </ul>
  </li>
</ul>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-replacement_80_20_workload_with_clock.png" alt="os-replacement_80_20_workload_with_clock.png" /></p>

<p><strong>Considering Dirty Pages</strong></p>

<p>Consider the locality by the expense on swapping out pages.</p>

<ul>
  <li>If a page has been <strong>modified</strong> and is thus <strong>dirty</strong>, it must be written back to disk to evict it, which is expensive.</li>
  <li>If it has not been modified (and is thus clean), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O.
Idea</li>
</ul>

<p>To support this behavior, the hardware should include a <strong>modified bit</strong> (a.k.a. <strong>dirty bit</strong>).</p>

<p>Implementation by Clock Algorithm</p>

<p>The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; failing to find those, then for unused pages that are dirty, and so forth.</p>

<p><strong>Other VM Policies</strong></p>

<p><strong><em>When the OS bring a page into memory?</em></strong></p>

<p>Page selection policy. The OS simply uses <strong>demand paging</strong>, which means the OS brings the page into memory when it is accessed, “on demand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as <strong>prefetching</strong>.</p>

<p><strong><em>How the OS writes pages out to disk?</em></strong></p>

<p>Any systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write. This behavior is usually called <strong>clustering</strong> or simply <strong>grouping</strong> of writes, and is effective because of the nature of disk drives.</p>

<p><strong><em>What about
 the memory demands of the set of running processes simply exceeds the available physical memory? (condition sometimes referred to as thrashing)</em></strong></p>

<p>Given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes working sets (the pages that they are using actively) fit in memory and thus can make progress. This approach, generally known as <strong>admission control</strong>, states that it is sometimes better to do less work well than to try to do everything at once poorly.</p>

<p>Some versions of Linux run an <strong>out-of-memory killer</strong> when memory is oversubscribed; this daemon chooses a memory- intensive process and kills it, thus reducing memory in a none-too-subtle manner.</p>

<h3 id="chapter-23---the-vaxvms-virtual-memory-system">Chapter 23 - The VAX/VMS Virtual Memory System</h3>

<p><strong>Background</strong></p>

<p>The VAX-11 minicomputer architecture was introduced in the late 1970’s by Digital Equipment Corporation (DEC).</p>

<p>As an additional issue, VMS is an excellent example of software innovations used to hide some of the inheret flaws of the architecture.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/os-vax_vms_address_space.png" alt="os-vax_vms_address_space.png" /></p>

<p><strong>Reduce Page Table Pressure</strong></p>

<p>First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process; thus, no page-table space is needed for the unused portion of the address space between the stack and the heap.</p>

<p>Second, the OS reduces memory pressure even further by placing user page tables (for P0 and P1, thus two per process) in kernel virtual memory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S. If memory comes undersevere pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.</p>

<p><strong>Replacement policy: Segmented FIFO with Page Clustering</strong></p>

<p>Each process has a maximum number of pages it can keep in memory, known as its <strong>residentn set size (RSS)</strong>. Each of these pages is kept on a FIFO list; when a process exceeds its RSS, the “first-in” page is evicted. FIFO clearly does not need any support from the hardware (no use bit), and is thus easy to implement.</p>

<p>To improve FIFO’s performance, VMS introduced two <strong>second-chance lists</strong> where pages are placed before getting evicted from memory, specifically a global clean-page free list and dirty-page list. The bigger these global second-chance lists are, the closer the segmented FIFO algorithm performs to LRU.</p>

<p>Clustering is used in most modern systems, as the freedom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve performance.</p>

<p><strong>Optimisation: Be Lazy</strong></p>

<p>Laziness can put off work until later, which is beneficial within an OS for a number of reasons.</p>

<ul>
  <li>First, putting off work might reduce the latency of the current operation, thus improving responsiveness; for example, operating systems often report that writes to a file succeeded immediately, and only write them to disk later in the background.</li>
  <li>Second, and more importantly, laziness sometimes obviates the need to do the work at all; for example, delaying a write until the file is deleted removes the need to do the write at all.</li>
</ul>

<p><strong>Lazy Optimisation: Demanding Zero</strong></p>

<p>With demand zeroing, the OS instead does very little work when the page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or writes the page, a trap into the OS takes place. When handling the trap, the OS notices that this is actually a demand-zero page; at this point, the OS then does the needed work of finding a physical page, zeroing it, and mapping it into the process’s address space. If the process never accesses the page, all of this work is avoided, and thus the virtue of demand zeroing.</p>

<p><strong>Lazy Optimisation: Copy-on-write</strong></p>

<p>When the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces.</p>

<ul>
  <li>If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.</li>
  <li>If, however, one of the address spaces does indeed try to write to the page, it will trap into the OS. The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process. The process then continues and now has its own private copy of the page.</li>
</ul>

<p>In UNIX systems, COW is even more critical, due to the semantics of <code>fork()</code> and <code>exec()</code>. <code>fork()</code> creates an exact copy of the address space of the caller; with a large address space, making such a copy is slow and data intensive. Even worse, most of the address space is immediately over-written by a subsequent call to <code>exec()</code>, which overlays the calling process’s address space with that of the soon-to-be-exec’d program. By instead performing a copy-on-write <code>fork()</code>, the OS avoids much of the needless copying and thus retains the correct semantics while improving performance.</p>
]]></content>
  </entry>
  
</feed>
