<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | Wendi's Blog]]></title>
  <link href="http://blog.ifyouseewendy.com/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://blog.ifyouseewendy.com/"/>
  <updated>2024-08-06T10:43:55-07:00</updated>
  <id>http://blog.ifyouseewendy.com/</id>
  <author>
    <name><![CDATA[wendi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Review] SOE-YCSCS1 Compilers]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2020/06/07/review-soe-ycscs1-compilers/"/>
    <updated>2020-06-07T11:47:14-04:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2020/06/07/review-soe-ycscs1-compilers</id>
    <content type="html"><![CDATA[<table class="custom">
  <tbody>
    <tr>
      <td><strong>Course</strong></td>
      <td>Compilers</td>
    </tr>
    <tr>
      <td><strong>Instructor</strong></td>
      <td>Alex Aiken</td>
    </tr>
    <tr>
      <td><strong>Link</strong></td>
      <td><a href="https://online.stanford.edu/courses/soe-ycscs1-compilers">online.stanford.edu/courses/soe-ycscs1-compilers</a></td>
    </tr>
  </tbody>
</table>

<ul id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v" id="markdown-toc-in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v">in evaluation, this means in a given context, expression e evaluates to value v</a></li>
  <li><a href="#intro" id="markdown-toc-intro">0 Intro</a>    <ul>
      <li><a href="#what-a-compiler-does" id="markdown-toc-what-a-compiler-does">0.1 What a compiler does</a></li>
      <li><a href="#economy-of-programming-language" id="markdown-toc-economy-of-programming-language">0.2 Economy of programming language</a></li>
    </ul>
  </li>
  <li><a href="#lexical-analysis" id="markdown-toc-lexical-analysis">1 Lexical Analysis</a>    <ul>
      <li><a href="#intro-1" id="markdown-toc-intro-1">1.1 Intro</a></li>
      <li><a href="#regular-language" id="markdown-toc-regular-language">1.2 Regular Language</a></li>
      <li><a href="#formal-language" id="markdown-toc-formal-language">1.3 Formal Language</a></li>
      <li><a href="#lexcial-specification" id="markdown-toc-lexcial-specification">1.4 Lexcial Specification</a></li>
      <li><a href="#finite-automata" id="markdown-toc-finite-automata">1.5 Finite Automata</a></li>
    </ul>
  </li>
  <li><a href="#parsing" id="markdown-toc-parsing">2 Parsing</a>    <ul>
      <li><a href="#intro-2" id="markdown-toc-intro-2">2.1 Intro</a></li>
      <li><a href="#context-free-grammars" id="markdown-toc-context-free-grammars">2.2 Context Free Grammars</a>        <ul>
          <li><a href="#cfg---intro" id="markdown-toc-cfg---intro">2.2.1 CFG - Intro</a></li>
          <li><a href="#cfg---deriviations" id="markdown-toc-cfg---deriviations">2.2.2 CFG - DERIVIATIONS</a></li>
          <li><a href="#cfg---ambiguity" id="markdown-toc-cfg---ambiguity">2.2.3 CFG - AMBIGUITY</a></li>
          <li><a href="#cfg---error-handling" id="markdown-toc-cfg---error-handling">2.2.4 CFG - ERROR HANDLING</a></li>
        </ul>
      </li>
      <li><a href="#abstract-syntax-tree" id="markdown-toc-abstract-syntax-tree">2.3 Abstract Syntax Tree</a></li>
      <li><a href="#section" id="markdown-toc-section">```</a></li>
      <li><a href="#recursive-descent-parsing" id="markdown-toc-recursive-descent-parsing">2.4 Recursive-Descent Parsing</a></li>
      <li><a href="#predictive-parsing" id="markdown-toc-predictive-parsing">2.5 Predictive Parsing</a></li>
      <li><a href="#bottom-up-parsing" id="markdown-toc-bottom-up-parsing">2.6 Bottom-up Parsing</a>        <ul>
          <li><a href="#shift--reduce" id="markdown-toc-shift--reduce">SHIFT &amp; REDUCE</a></li>
          <li><a href="#handles" id="markdown-toc-handles">HANDLES</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#semantic-analysis" id="markdown-toc-semantic-analysis">3 Semantic Analysis</a>    <ul>
      <li><a href="#identifiers" id="markdown-toc-identifiers">3.1 Identifiers</a>        <ul>
          <li><a href="#scope" id="markdown-toc-scope">SCOPE</a></li>
          <li><a href="#symbol-tables" id="markdown-toc-symbol-tables">SYMBOL TABLES</a></li>
        </ul>
      </li>
      <li><a href="#types" id="markdown-toc-types">3.2 Types</a>        <ul>
          <li><a href="#type-checking" id="markdown-toc-type-checking">3.2.1 TYPE CHECKING</a></li>
        </ul>
      </li>
      <li><a href="#section-1" id="markdown-toc-section-1">```</a>        <ul>
          <li><a href="#type-environment" id="markdown-toc-type-environment">3.2.2 TYPE ENVIRONMENT</a></li>
          <li><a href="#subtype-methods-and-implementation" id="markdown-toc-subtype-methods-and-implementation">3.2.3 SUBTYPE, METHODS and IMPLEMENTATION</a></li>
        </ul>
      </li>
      <li><a href="#ti--ti-for-1--i--n" id="markdown-toc-ti--ti-for-1--i--n">Ti &lt;= Ti’ for 1 &lt;= i &lt;= n</a></li>
      <li><a href="#ti--ti-for-1--i--n-1" id="markdown-toc-ti--ti-for-1--i--n-1">Ti &lt;= Ti’ for 1 &lt;= i &lt;= n</a>        <ul>
          <li><a href="#static-vs-dynamic" id="markdown-toc-static-vs-dynamic">3.2.4 STATIC VS. DYNAMIC</a></li>
          <li><a href="#selftype" id="markdown-toc-selftype">3.2.5 SELF_TYPE</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#runtime-organizations" id="markdown-toc-runtime-organizations">4 Runtime Organizations</a>    <ul>
      <li><a href="#activations" id="markdown-toc-activations">4.1 Activations</a></li>
      <li><a href="#activation-records" id="markdown-toc-activation-records">4.2 Activation Records</a></li>
      <li><a href="#global--heaps" id="markdown-toc-global--heaps">4.3 Global &amp;&amp; Heaps</a></li>
      <li><a href="#alignment" id="markdown-toc-alignment">4.4 Alignment</a></li>
    </ul>
  </li>
  <li><a href="#code-generation" id="markdown-toc-code-generation">5 Code Generation</a>    <ul>
      <li><a href="#stack-machines" id="markdown-toc-stack-machines">5.1 Stack Machines</a>        <ul>
          <li><a href="#stack-machine-vs-register-machine" id="markdown-toc-stack-machine-vs-register-machine">5.1.1 Stack machine vs Register machine</a></li>
          <li><a href="#n-register-stack-machine" id="markdown-toc-n-register-stack-machine">5.1.2 n-register stack machine</a></li>
        </ul>
      </li>
      <li><a href="#intro-3" id="markdown-toc-intro-3">5.2 Intro</a></li>
    </ul>
  </li>
  <li><a href="#register---immediate-value" id="markdown-toc-register---immediate-value">register - immediate value</a></li>
  <li><a href="#register---add-operation" id="markdown-toc-register---add-operation">register - add operation</a>    <ul>
      <li><a href="#code-gen" id="markdown-toc-code-gen">5.3 Code Gen</a></li>
      <li><a href="#code-gen---object-layout" id="markdown-toc-code-gen---object-layout">5.4 Code Gen - Object Layout</a></li>
      <li><a href="#evaluation-semantics" id="markdown-toc-evaluation-semantics">5.5 Evaluation Semantics</a></li>
      <li><a href="#operational-semantics" id="markdown-toc-operational-semantics">5.6 Operational Semantics</a></li>
    </ul>
  </li>
  <li><a href="#in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v-1" id="markdown-toc-in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v-1">in evaluation, this means in a given context, expression e evaluates to value v</a>    <ul>
      <li><a href="#intermediate-language" id="markdown-toc-intermediate-language">5.7 Intermediate language</a></li>
    </ul>
  </li>
  <li><a href="#optimization" id="markdown-toc-optimization">6 Optimization</a>    <ul>
      <li><a href="#intro-4" id="markdown-toc-intro-4">6.1 Intro</a></li>
      <li><a href="#local-optimization" id="markdown-toc-local-optimization">6.2 Local Optimization</a></li>
      <li><a href="#global-optimization" id="markdown-toc-global-optimization">6.3 Global Optimization</a>        <ul>
          <li><a href="#dataflow-analysis" id="markdown-toc-dataflow-analysis">6.3.1 Dataflow analysis</a></li>
          <li><a href="#global-constant-propogation" id="markdown-toc-global-constant-propogation">6.3.2 Global constant propogation</a></li>
          <li><a href="#liveness-analysis" id="markdown-toc-liveness-analysis">6.3.3 Liveness Analysis</a></li>
        </ul>
      </li>
      <li><a href="#register-allocation" id="markdown-toc-register-allocation">6.4 Register Allocation</a>        <ul>
          <li><a href="#graph-coloring" id="markdown-toc-graph-coloring">6.4.2 Graph Coloring</a></li>
        </ul>
      </li>
      <li><a href="#managing-cache" id="markdown-toc-managing-cache">6.5 Managing Cache</a></li>
      <li><a href="#automatic-memory-management-gc" id="markdown-toc-automatic-memory-management-gc">6.6 Automatic Memory Management (GC)</a>        <ul>
          <li><a href="#intro-5" id="markdown-toc-intro-5">6.6.1 Intro</a></li>
          <li><a href="#mark-and-sweep" id="markdown-toc-mark-and-sweep">6.6.2 Mark and Sweep</a></li>
          <li><a href="#stop-and-copy" id="markdown-toc-stop-and-copy">6.6.3 Stop and Copy</a></li>
          <li><a href="#reference-counting" id="markdown-toc-reference-counting">6.6.4 Reference Counting</a></li>
          <li><a href="#advanced-gc-algorithm" id="markdown-toc-advanced-gc-algorithm">6.6.5 Advanced GC Algorithm</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="summary">Summary</h1>

<p><strong>INTRO</strong></p>

<p>Interpreter is “online” and compiler is “offline”</p>

<p>```
Program + Data =&gt; Interpreter =&gt; Output</p>

<p>Program =&gt; Compiler =&gt; exec
                       exec + Data =&gt; Output
```</p>

<p>What a compiler does?</p>

<p>Five phases</p>

<p><code>
Lexical Analysis  (input)   -&gt; tokens
Parsing           (tokens)  -&gt; AST
Semantic Analysis (AST)     -&gt; IL
Optimization      (IL)      -&gt; IL
Code Generation   (IL)      -&gt; Machine code
</code></p>

<p>Why are there new programming languages?</p>

<p>Programming training is the main dominant cost for a programming language. It’s easy to start a new
language when productivity boost is over the training cost.</p>

<p><strong>LEXICAL ANALYSIS</strong></p>

<p>A LA does two things: partition the input string into lexemes, and identify the token class of each
lexeme. We need a way to specify which set of strings belongs to each token class and the usual tool
for doing that is to use Regular Language.</p>

<p>As a sidenote, inside of the compiler, we typically have several different Formal Languages, and
Regular Language is one of them. A Formal Language has a set of alphabet and a meaning function,
that maps syntax to semantics.</p>

<p><code>
                  described by
Lexical Analysis -------------- Lexical Spec
                                      |
                                Formal Language
                                      |           implemented by
                                Regular Language ---------------- Regular Expression
                                                 \--------------- Finite Automata (NFA, DFA)
</code></p>

<p>We use Regular Expression as the Lexical Specification and we use Finite
Automata as the implementation.</p>

<p>To define Deterministic Finite Automata: 1. no e moves; 2. for one input, there is only one
transition from a state.</p>

<p>A token consists of (token class, lexeme).</p>

<p><strong>PARSING</strong></p>

<p>Heuristic: why is there a Parsing stage? A Regular Language is the weakest Formal Language that’s
widely used, no matter is Regular Expression, NFA, DFA, it has its limit on expressing, eg. nested
structure.</p>

<p>Since not all strings of tokens are programs, a parser must distinguish between valid and invalid
string of tokens. We need</p>

<ul>
  <li>a language for describing valid string of tokens – CFG</li>
  <li>a method (algorithm) for distinguishing valid from invalid string of tokens – Parsing algorithm</li>
</ul>

<p>Context Free Grammars (a Formal Language) is help to describe whether a string of tokens is valid. It consist of</p>

<ul>
  <li>a set of terminals, T</li>
  <li>a set of non-terminals, N</li>
  <li>a start symbol, S, where S (- N</li>
  <li>a set of productions or rules</li>
</ul>

<p>Recursive-descent Parsing (top-down): The parse tree is constructed from the top and from left to
right. Start with top-level non-terminal E, try the rules for E <strong>in order</strong>. Recursive-descent
parsing is a simple and general parsing strategy, which is used in GCC frontend.  To use it, left
recursion must be eliminated first.</p>

<p>Predictivce Parsing (top-down): In Recursive-descent parsing, at each step, there are many choices
of production to use. Therefore, we need to backtrack to undo bad choices. Predictive parsing are a
lot like Recursive-descent parsing, but it can “predict” which production to use by looking at the
next few tokens, thus there is no need to backtrace. Predictive parers accept LL(k) grammars. At
each step, there should be at most one choice of production.</p>

<ul>
  <li>Left-to-right</li>
  <li>Left-most derivation</li>
  <li>k tokens looking ahead</li>
</ul>

<p>Bottom-up parsing is more general than top-down parsing, but just as efficient.  Bottom-up builds on
ideas in top-down parsing and is the preferred method for most of generator tools. Bottom-up parsing
reduces a string to the start symbol by inverting productions.</p>

<p><strong>SEMANTIC ANALYSIS</strong></p>

<p>Lexical analysis detects inputs with illegal tokens; Parsing detects with ill-formed parse trees;
and Semantic Analysis, as last front-end phase, catches all remaning errors, eg</p>

<ul>
  <li>all identifiers are declared</li>
  <li>reserved identifiers are not misused</li>
  <li>types</li>
  <li>inheritance relationships</li>
  <li>classes defined only once</li>
  <li>method in a class defined only once</li>
</ul>

<p>Much of Semantic Analysis can be expressed as recursive descent of an AST:</p>

<p><code>
before: process an AST node n
recurse: process the children of n
after: finish processing the node n
</code></p>

<p>Identifiers. The scope of an identifier is the portion of a program in which that identifier is accesible. Scope
helps match identifier declarations with uses. There are two kinds of scopes:</p>

<ul>
  <li>static, that the scope depends only on the program text not runtime behaviour;</li>
  <li>dynamic, which referes to the cloest enclosing binding in the execution of the program.</li>
</ul>

<p>As we need to know which identifiers are defined, we introduce a data structure that tracks the current bindings of identifiers, which is Symbol Table.</p>

<p>Types. A set of values and a set of operations on those values. A language’s type system specifies which operations are valid for which types. The goal of type checking is to enture that operations are used with the correct types. There is no types in an assembly language, therefore there are no types at the bit level in the machine code. So, type is a
virtual concept at the language level, and to type check is to enforce the intended interpretation of values.</p>

<p>There are a few kinds:</p>

<ul>
  <li>static typed langs: C, Java. A lot of code is written in statically typed lang has an “escape”
mechanism: like unsafe casts in C, Java (void pointer can be anything).</li>
  <li>dynamic typed langs: Lips, Ruby. A lot of dynamically typed lang rewrites their compilers with
static lang for optimization and better debugging.</li>
  <li>untyped langs: machine code</li>
</ul>

<p>Type checking. The Formal Language we use is Logic Rules of Inference, which has the form that “if
Hypothesis is true, then Conclusion is true”.</p>

<p>```
⊢ Hypothesis .. ⊢ Hypothesis
—————————-
         ⊢ Conclusion</p>

<p>⊢ means “it’s provable that…”
```</p>

<p>Type environment. A type environment gives types for free variables/identifiers in the current scope, by free it means
the varaible is not defined.</p>

<p>As an example for defining Assign type checking rule:</p>

<p><code>
O ⊢ e1: T1
O(x) = T0       [Assign]
T1 &lt;= T0
-------------
O ⊢ x = e1: T1
</code></p>

<p>A type environment is built inisde the Symbol Table and gets passed down the AST from root to
leaves. Types are computed up the AST from the leaves towars the root.</p>

<p><strong>RUNTIME ORGANIZATIONS</strong></p>

<p>Before we get into optimization and code generation, we need to understand what we are trying to
generate. A runtime organization controls the management of run-time resources. Particually, to
understand a compiler works, we should understand the correspondenc between static (compile-time)
and dynamic (run-time) strucutres: what is done by the compiler and what is deferred to the
generated program actually runs.</p>

<p>Execution of a program is initially by OS. When a program is invoked: the OS allocates space for the
program; the code is loaded into part of the space; the OS jumps to the entry point (“main”).
A compiler is responsible for generating code and orchestrating code to use the data space.</p>

<p><code>
+-------------+
|     code    | ---+
+-------------+    |
|             |    |
|     data    | &lt;--+
|             |
+-------------+
</code></p>

<p>Activations. An invocation of procedure P is an activation of P. The lifetime of an activation of P
is all the steps of execute P, including all the steps in procedures P calls. We can also say that
the lifetime of a varaible x is the portion of execution in which x is defined. To be noted that,
lifetime is a dynamic (run-time) concept, whereas scope is a static (compile-time) concept. Since
activations are properly nested, we can use a stack to track currently active procedures.</p>

<p>Activation Records. The information needed to manage one procedure activation is called an
Activation Record or Frame. If procedure F calls G, then G’s activation records contains a mix of
info about F and G. Becuase G’s AR should contain information to 1. complete execution of G 2.
resume execution of F.</p>

<p>One of many possible AR designs (which works for C)</p>

<p><code>
+----------------+
| result         |
+----------------+
| argument       |
+----------------+
| control link   |   // who calls the current activation
+----------------+
| return address |   // where to resume execution after the current activation
+----------------+
</code></p>

<p>The compiler must determine, at compile-time, the layout of AR and generate code
that correctly accesses location in the activation records. Thus, the AR layout and the code
generator must be designed together.</p>

<p>Globals cannot be stored in AR as all references to a global variable should point to
the same object. So, globals are assigned at a fixed address once, as statically allocated.
For values that outlive the procedure that creates it cannot be kept in the AR neither, like in
<code>method foo() { new Bar }</code>, that <code>Bar</code> value must survive deallocation of <code>foo</code>’s AR. So, we need to
use heap to store dynamically allocated data.</p>

<p><code>
+-------------+ Low address
|    code     |
+-------------+
|             |
| static data | -- Globals
|             |
+-------------+
|   stack     | -- ARs
|     |       |
|     v       |
|.............|
|             |
|             |
|             |
|             |
|             |
|.............|
|     ^       |
|     |       |
|    heap     |
+-------------+ High address
</code></p>

<p>Alignment. Data is word aligend if it begins at a word boundary. Most machines have some alignment
restrictions or performance penalties for poor alignment.</p>

<p><strong>CODE GENERATION</strong></p>

<p>Stack Machine, the simplest model for code generation. A stack machines use a stack as the only
storage. An instruction <code>r = F(a1,...an)</code> is executed as</p>

<ul>
  <li>Pops n operands from the stack</li>
  <li>Computes the operation F</li>
  <li>Pushes result back to the stack</li>
</ul>

<p>The invariance a stack machine maintains: After evaluating an expression e, the accumulator holds
the value of e and the stack is unchanged. This is a very important property: <strong>Expression
evaluation preserves the stack</strong>.</p>

<p>Stack machine vs Register machine. Location of the operands/result is not explicitly stated, as
which are always on the top of the stack. We consider <code>add</code> as a valid operation, instead of <code>add
r1, r2</code> in a register machine. This leads to more compact programs (space). Java bytecode uses stack
evaluation. However, a register machine is mostly preferred and generally faster (time), because we
can place the data at exactly where we want it to be, which has generally less intermediate
operations and manipulation like pushing and popping off the stack.</p>

<p>N-register stack machine. It’s an intermediate form between pure stack machine and register machine.
Conceptually, keep the top n locations of the pure stack machine’s stack in registers. A 1-register
stack machines is called the accumulator.</p>

<p>A code gen example</p>

<p>```
cgen(e1 + e2) =
  // compile time code prints out runtime code</p>

<p>cgen(e1)
  print “sw $a0 0($sp)”           // push value onto stack
  print “addiu $sp $sp-4”</p>

<p>cgen(e2)</p>

<p>print “lw $t1 4($sp)”           // load value from stack
  print “add $a0 $t1 $a0”         // add
  print “addiu $sp $sp 4”         // pop from stack
```</p>

<p>A code gen example for object layout</p>

<p><code>
+--------------+ First 3 words are headers
| Class tag    |
+--------------+
| Object size  |                   Dispatch Table
+--------------+                   +-------------------+
| Dispatch Ptr | ---------------&gt;  |    |    |    |    |
+--------------+                   +-------------------+
| Attrs        | then attributes
+--------------+
</code></p>

<p>Given a layout for class A, a layout for subclass B can be defined by
extending the layout of A with additional slots of the addition attributes of B. So consider layout of <code>A3 &lt; A2 &lt; A1</code></p>

<p><code>
+--------------+
| Header       |
+--------------+
| A1 attrs     |
+--------------+
| A2 attrs     |
+--------------+
| A3 attrs     |
+--------------+
</code></p>

<p>The offset for an attribute is the same in a class and all of its subclasses.</p>

<p>Dynamic dispatch. Every class has a fixed set of methods, including inherited methods. A dispatch
table is used to index these mtehods. It’s an array of method entrypoints. A method <code>f</code> lives at a
fixed offset in the dispatch table for a class and all of its subclasses. Theorectially we can save
the table directly as we do for attributes. But attributes are states that 100 objects can each have
a different set of attributes values. Methods are static that it makes sense to share the common
table among objects.</p>

<p>Evaluation Semantics. In Code Generation, we need to define an evaluation rule, which is also called Semantics.</p>

<ul>
  <li>The tokens is parsed by Regular Expressions in Lexical Analysis</li>
  <li>The grammar is represented by CFG in Syntactic Analysis</li>
  <li>The typing rule is represented by Inferenece Rule in Semantics Analysis</li>
  <li>The evaluation rules is represented by Semantics in Code Generation and Optimization</li>
</ul>

<p>Operational Semantics. It describes program evaluation via execution rules on an abstract machine,
which is most useful for specifying implementations.</p>

<p>```
# in type checking, this means in a given context, expression e has type C
Context ⊢ e: C</p>

<h1 id="in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v">in evaluation, this means in a given context, expression e evaluates to value v</h1>
<p>Context ⊢ e: v
```</p>

<p>Consider the evaluation of <code>y &lt;- x + 1</code>, we should track variables and their values with:</p>

<ul>
  <li>an environment: where a variable is in memory, <code>E = [a: l1, b: l2]</code></li>
  <li>a store: what is in the memory, <code>S = [l1 -&gt; 5, l2 -&gt; 7]</code></li>
</ul>

<p>```
so, E, S ⊢ e: v, S’</p>

<p>Given
  so as the current value of self
  E as the current variable environment
  S as the current store
If the evaluation of e terminates then
  the value of e is v
  the new store is S’
```</p>

<p><strong>OPTIMIZATION</strong></p>

<p>Most complexity in modern compilers is in the optimizer. Optimization seeks to improve a program’s
resoure utilization: execution time, code size and network messages sent, etc.</p>

<p>What should we perform optimizations on?</p>

<p>On Intermediate language</p>

<ul>
  <li>pro: machine independent</li>
  <li>pro: expose optimization opportunities</li>
  <li>on AST is too high level</li>
  <li>On Assembly is not machine independent</li>
</ul>

<p>Intermediate Language. A language between source and target. With more details than souce and less
than target. Intermidate language can be considered as high-level assmebly. It uses register names,
but has an unlimited number. It uses control structures as assembly language. It uses opcodes but
some are higher level, like <code>push</code> translates to several assembly instructions. Usually, we prefer
to apply optimizations over IL, instead of AST or assembly language.</p>

<p>What are the units of optimization?</p>

<ul>
  <li>A basic block is a maximal sequence of instructions with no labels (except at the first instruction)
and no jumps (except in the last instruction), which makes it a single-entry, single-exit,
straight-line code segment.</li>
  <li>A control-flow graph is a directed graph with basic block as nodes.</li>
</ul>

<p>What are granularities of optimizations? Like in C</p>

<ol>
  <li>Local optimization: apply to a basic block in isolation</li>
  <li>Global optimization (it’s not really global, but to function): apply to a control-flow graph in
isolation</li>
  <li>Inter-procedural optimization: apply across function boundaries.</li>
</ol>

<p>Global optimization. There are many global dataflow analysis, but they all follow the methodology:
The analysis of a complicated program can be expressed as a combination of simple rules relating
the change in information between adjacent statements.</p>

<p>Register Allocation. Register Allocation is a “must have” in compilers: because intermediate code
uses too many temporaries and it makes a big difference in performance. So the optimization is about
to rewrite the intermediate code to use no more temporarie than there are machine registers.</p>

<p>Solution: Construct an undirected graph, that a node for each temporary, an edge between t1 and t2
if they are live simultaneously at some point in the program, which is called REGISTER INTERFERENCE
GRAPH (RIG). Two temporaries can be allocated to the same register if there is no edge connecting
them. After RIG construction, the Register Allocation algorithm is architecture independent. The
algorithm to use is called Graph coloring.</p>

<p>A coloring of a graph is an assignment of colors to nodes, such that nodes connected by an edge have
different colors. A graph is K-COLORABLE if it has a coloring with k colors. For Register
Aollocation, we need to assign colors (registers) to graph nodes (temporaries), and let k be the
number of machine registers. If the RIG is k-colorable then there is a register assignment that uses
no more than k registers.</p>

<p>Managing Cache</p>

<p>```
+———–+—————+———–+
| Registers | 1 cycle       | 256-8000B |
| Cache     | 3 cycles      | 256K-1M   |
| Memory    | 20-100 cycles | 32M-4G    |
| Disk      | 0.5-5M cycles | 4G-1T     |
+———–+—————+———–+</p>

<p>*cycle is the clock frequency
```</p>

<p>The cost of cache miss (for register) is very high, so typically it requires 2-layered cache to bridge
fast processor with large main memory.</p>

<p>Automatic Memory Managemen (GC)</p>

<ul>
  <li>Advantage: it prevents serious storage bugs</li>
  <li>Disadvantge:
    <ul>
      <li>it reduces programmer control, like the layout of data in memory, or when is memory
deallocated;</li>
      <li>inefficient in some cases</li>
      <li>pauses problematic in real-time applications</li>
      <li>memory leaks possible</li>
    </ul>
  </li>
</ul>

<p>Mark and Sweep</p>

<ul>
  <li>Advantage: objects are not moved during GC, works well for languages with pointers like C and C++</li>
  <li>Disadvantge: fragment memory</li>
</ul>

<p>Stop and Copy</p>

<p>Stop and copy is generally believed to be the fastest GC technique</p>

<ul>
  <li>Advantage: Allocation is very cheap (just increment the heap pointer). Collection is relatively
cheap, especially if there is a lot of garbage, as it only touches reachable objects</li>
  <li>Disadvantge: some languages do not allow copy, like C and C++.</li>
</ul>

<p>Reference Counting</p>

<ul>
  <li>Advantage: easy to implement; collects garbage incrementally without large pauses in the execution</li>
  <li>Disadvantge: cannot collect circular structures; manipulating reference counts at each assignment is
very slow</li>
</ul>

<hr />

<h1 id="intro">0 Intro</h1>

<p>Interpreter is “online” and compiler is “offline”</p>

<p>```
Program + Data =&gt; Interpreter =&gt; Output</p>

<p>Program =&gt; Compiler =&gt; exec
                       exec + Data =&gt; Output
```</p>

<p>When John Backus tried to solve program productivity problem in 1950s, compared
to writing machine code, he tried an interpreter language first, which is called
“Speedcoding”, which makes programmer happy but takes too much memory and runs
10x slower (as an interpreter lang does). After that, he developed Fortran
(Formalas Translated), which introduces a two stage development, compile and
execute. Modern compilers preserve the outline of Fortran.</p>

<h2 id="what-a-compiler-does">0.1 What a compiler does</h2>

<p>Five phases</p>

<ol>
  <li>Lexical analysis (syntactic)</li>
  <li>Parsing (syntactic)</li>
  <li>Semantic analysis (types, scopes..)</li>
  <li>Optimization</li>
  <li>Code generation (machine code, or byte code on a VM)</li>
</ol>

<p>Lexical analysis is to divide a program into “words” or “tokens”</p>

<p><code>
This is a sentence.
</code></p>

<p>Once words are understood, the next step, Parsing, is to understand sentence structure</p>

<p><code>
This    line is   a       longer    sentence
----    ---- --   -       ------    --------
article noun verb article adjective noun
   |     /           \       |      /
   subject                object
          \               /
               sentence
</code></p>

<p>Once sentence structure is understood, the next step, Semantic analysis, is to
try understanding the “meaning”. For humans, we do lexical
analysis and parsing, but we don’t know how we understand it. So, this is too
hard for compilers. <strong>Compilers can only perform limited Semantic analysis to
catch inconsistencies.</strong></p>

<p>Eg. 1 Variable bindings</p>

<p>```
Jack said Jerry left his assignment at home.
                     —
                     points to Jack or Jerry?</p>

<p>Jack said Jack left his assignment at home?
—-      —-      —
How many people are involved?
```</p>

<p>The analogy in programming language is variable bindings. Programming lang
define strict rules to avoid such ambiguities. For example, lexical scoped
language</p>

<p><code>
{
  int Jack = 3;
  {
    int Jack = 5;
    cout &lt;&lt; Jack;
  }
}
</code></p>

<p>Eg. 2 Type mismatch</p>

<p><code>
Jack left her homework at home.
          ---
</code></p>

<p>Optimization has no strong counterpart in English, but you can think of what an
editor do. The main purpose here is to modify the program so that they use less
resource, run faster (time) and use less memory (space).</p>

<p>```
But a little bit like editing
    —————–
But akin to           editing
    ——-</p>

<p>Y=X*0 =&gt; Y=0
// only works for intergers, but not floats, as NAN * 0 = NAN
```</p>

<p>Code generation generally means a translation to another language, like in human
language. It usually produces assembly code.</p>

<p>The proportion of each phase does has changed since Fortran</p>

<p><code>
Fortran: [    L    ] [    P    ] [S] [   O   ] [   CG   ]
Modern : [L] [P] [    S    ] [           O              ] [ CG ]
</code></p>

<h2 id="economy-of-programming-language">0.2 Economy of programming language</h2>

<p>Q1. Why are so many programming languages?</p>

<p><strong>Application domains have distinctive/conflicting needs.</strong></p>

<ul>
  <li>In scientific computing, there should be good float point numbers, good
arrays, and parallelism, like Fortran</li>
  <li>In bussiness applications, there should be persistence, report generation and
data analysis, like SQL</li>
  <li>In system programming, there should be resource management and real time
constraints, like C and C++.</li>
</ul>

<p>Q2. Why are there new programming languages?</p>

<p>Claim: <strong>Programming training is the dominant cost for a programming
language</strong></p>

<p>Prediction:</p>

<ol>
  <li>Widely used languages are slow to change (for education cost).</li>
  <li>Easy to start a new language, when productivity boost is over the training
cost.</li>
  <li>Language adopted to fill a void as tech grows with new open niche</li>
  <li>New languages tend to look like old languages (think about the training cost)</li>
</ol>

<p>Q3. What is good programming language? :shrug:</p>

<h1 id="lexical-analysis">1 Lexical Analysis</h1>

<h2 id="intro-1">1.1 Intro</h2>

<p>```
                             one token
String ——-&gt; LA ——–&gt; (token class, lexeme) ——–&gt; Parser</p>

<p>foo = 4                       (Id, “foo”)
                              (Op, “=”)
                              (Int, “4”)
```</p>

<p>What are the valid token classes?</p>

<ul>
  <li>Identifier</li>
  <li>Keyword</li>
  <li>Operator</li>
  <li>Whitespace, a non-empty sequence of blanks, newlines and tabs</li>
  <li>Numbers</li>
  <li>(</li>
  <li>)</li>
  <li>;</li>
  <li>=</li>
</ul>

<p>A LA does two things:</p>

<ol>
  <li>Partition the input string into lexemes.</li>
  <li>Identify the token class of each lexeme.</li>
</ol>

<p>Reading left-to-right and recognizing one token at a time, which requires
lookahead, to help determine the end of the current token and start of next
token.</p>

<p>Fun fact:
1. In FORTRAN, all whitespaces can be omitted
2. In PL/1, keywords are not reserved :P</p>

<h2 id="regular-language">1.2 Regular Language</h2>

<p>The lexical strucutre of a programming language is a set of token classes, and
each one of the token classes consists of some set of strings. We need a way to
specify which set of strings belongs to each token class and the usual tool for
doing that is to use <strong>Regular Language</strong>.</p>

<p>Usually, we use Regular Expression (the syntax) to denote Regular Languages (set
of strings).</p>

<p>Empty string (Epsilon)  {“”}
1-char string           {“c”}
Compound
  Union                 A + B
  Concatenation         AB
  Interation            A*</p>

<p>The regular expressions over Σ(alphabet) are the smallest set of expressions
including:</p>

<p><code>
// Grammar
R = e
  | 'c'
  | R +R
  | RR
  | R*
</code></p>

<p>Example</p>

<p>```
Σ= {0, 1}</p>

<p>1*       - represents “” + 1 + 11 + 111 …
(1 + 0)1 - represents 11, 01
(0 + 1)* - represents all possible strings, we call it Σ*
```</p>

<h2 id="formal-language">1.3 Formal Language</h2>

<p>Inside of the compiler, we typically have several different formal languages
that we are manipulating. A Regular Expression is one example.</p>

<p><strong>A Formal Language has a set of alphabet Σ</strong>. A language over Σ is a set of
strings of characters drawn from Σ.</p>

<table>
  <thead>
    <tr>
      <th>Alphabet</th>
      <th>Language</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>a-z</td>
      <td>English</td>
    </tr>
    <tr>
      <td>ASCII</td>
      <td>C</td>
    </tr>
  </tbody>
</table>

<p><strong>A Formal Language has a meaning function L, that maps syntax to semantics</strong>. Why
it’s necessary:</p>

<ul>
  <li>It makes clear what is syntax, what is semantics</li>
  <li>Allow to abstract notaion as a separate issue (1,2,10 vs I,II,X)</li>
</ul>

<p>Generally, <strong>meaning function L is many to one</strong>. Syntax (expressions) and semantics
(meanings) are not 1-1, but many to one, which means there are different ways,
optimizations, to achieve the same meaning.</p>

<p>```
L(e) = M
L: Exp -&gt; Set of strings</p>

<p>L(e)     = {“”}
L(‘c’)   = {“c”}
L(A + B) = L(A) ⋃ L(B)
L(AB)    = { ab | a ∈ (LA) , b ∈L(B) }
L(A*)    = L(A^i) for i &gt;=0
```</p>

<p>Example, L in Regular Expression</p>

<p><code>
?     = {""}
c     = {"c"}
[AB]  = L(A) ⋃ L(B)
AB    = { ab | a ∈ (LA) , b ∈L(B) }
A*    = L(A^i) for i &gt;=0
A+    = AA*
</code></p>

<h2 id="lexcial-specification">1.4 Lexcial Specification</h2>

<ol>
  <li>Write a rexp for the lexemes of each token class, like R1 = Number, R2 = Identifier..</li>
  <li>Construct R, matching all lexemes of all tokens. R = R1 + R2 ..</li>
  <li>Let input be x1..xn, for <code>1 &lt;= i &lt;= n</code>, check x1..xi ∈ L(R)</li>
  <li>If so, then we know that x1..xi ∈ L(Rj) for some i</li>
  <li>Remove x1..xi from input and go to 3.</li>
</ol>

<p>To resolve ambiguities</p>

<ul>
  <li>Apply “Maximal Munch” to the input, matching <code>==</code> instead of <code>=</code>.</li>
  <li>Choose highest priority match, (usually list the high priority one first), like putting Keyword ahead of Identifiers</li>
</ul>

<p>To handle errors</p>

<ul>
  <li>Better not let it happen; otherwise specify a Error to denote all strings not in the lexical spec, put it last in priority</li>
</ul>

<p>```
Lexical Analysis – partition input into lexemes
                 – identify token of each lexeme</p>

<pre><code>              described by Lexical Analysis -------------- Lexical Spec
                                  |
                            Formal Language
                                  |           implemented by
                            Regular Language ---------------- Regular Expression
                                             \--------------- Finite Automata (NFA, DFA) ```
</code></pre>

<h2 id="finite-automata">1.5 Finite Automata</h2>

<p><strong>We use Regular Expression as the Lexical Specification and we use Finite
Automata as the implementation.</strong></p>

<p>A finite automata consists of
* An input alphabet Σ
* A finite set of states S
* A start state n
* A set of transitions, state + input -&gt; state
* A set of accepting states F ∈ S</p>

<p>For a transition s1 + input -&gt; s2, if it’s in accpeting state =&gt; accept;
otherwise =&gt; reject. Eg. terminating in a state that S not ∈ F or getting stuck
of a state that cannot be moved.</p>

<p>So Language of a FA is the set of accepted strings.</p>

<p>If we allow a transition from s1 + e -&gt; s2, that means for one input, we
have two valid states, which ends up having two syntax mapping to one semantic.
To define <strong>Deterministic Finite Automata</strong>:</p>

<ul>
  <li>No e moves</li>
  <li>For one input, there is only one transition from a state</li>
</ul>

<p>NFA can have e moves. Essentially, deterministic means for one input, there is
one path through the state graph.</p>

<p><code>
DFA: -&gt; s1 -&gt; s2 -&gt; ... F
NFA: -&gt; s1 -&gt; s2  -&gt; s2a
        |        \-&gt; s2b
        e        \-&gt; s2c
        |
        s3  -&gt; s3a
           \-&gt; s3b
           \-&gt; s3c
</code></p>

<p>As for one input, NFA might end up multiple states. We say an NFA accepts if
any one of the path finishes at an accepting state.</p>

<p>NFA, DFA and Regular Expression all have equivalent power for specifying Regular
Language.</p>

<ul>
  <li>DFA are faster to execute, as there are no choices to consider. (TIME)</li>
  <li>NFA are in general (might be exponentially) smaller. (SPACE)</li>
</ul>

<p>NFA TO DFA</p>

<p>e-closure for a state is all the states that it can reach through e move.</p>

<p>An NFA may be in many states at any time, but how many different states? For N
states, there would be 2**N - 1 finite set of possible states (except for the
empty set).</p>

<p>NFA</p>

<p><code>
states: S
start : s ∈S
final : F
transition: a(X) = { y | x ∈X, x + a -&gt; y }
</code></p>

<p>to transit to DFA</p>

<p><code>
states: subsets of S
start : e-closure(s)
final : { X | X ⋂ F != empty }
transition: X + a -&gt; Y if Y = e-closure(a(X))
</code></p>

<p>IMPLEMENTING DFA</p>

<p>A DFA can be implemented by a 2D table T
* One dimension is states
* Ohter dimension is input symbol
* For every transition Si + a -&gt; Sk, define T[i,a] = k</p>

<p>```
input = “01010”;
i = 0;
state = S;</p>

<p>while (input[i]) {
  state = T[state, input[i]];
  i++;
}
```</p>

<h1 id="parsing">2 Parsing</h1>

<h2 id="intro-2">2.1 Intro</h2>

<p>Heuristic: why is there a Parsing stage?</p>

<p>A Regular Language is the weakest Formal Language that’s widely used, no matter is Regular
Expression, NFA, DFA, it has its limit on expressing, eg. nested structure.</p>

<p>```
()
(())
((()))</p>

<p>if … then
  if … then
    if … then
    fi
  fi
fi
```</p>

<p>Parsing is the stage to help apply more analysis onto tokens.</p>

<p><code>
  stream of characters        stream of tokens             parse tree
------------------------ LA -------------------- Parser --------------- ..
</code></p>

<h2 id="context-free-grammars">2.2 Context Free Grammars</h2>

<h3 id="cfg---intro">2.2.1 CFG - Intro</h3>

<p>Since not all strings of tokens are programs, a parser must distinguish between valid and invalid
string of tokens. We need</p>

<ul>
  <li>a language for describing valid string of tokens, and</li>
  <li>a method (algorithm) for distinguishing valid from invalid string of tokens</li>
</ul>

<p>Context Free Grammars is help to solve the first point, which is to answer “yes” or “no” whether a
string of tokens is valid.</p>

<p>Programming languages have recursive strucutres. CFG are a natural notaion for
this recursive structure.</p>

<p><code>
EXPR -&gt; if EXPR then EXPR els EXPR fi
      | while EXPR loop EXPR pool
      | ...
</code></p>

<p>A CFG consists of</p>

<ul>
  <li>a set of terminals, T</li>
  <li>a set of non-terminals, N</li>
  <li>a start symbol, S, where S (- N</li>
  <li>a set of productions or rules, X -&gt; Y1…Yn, where X (- N, Yi (- T + N + {e}</li>
</ul>

<p><code>X-&gt;Y1..Yn</code> means you can replace X with <code>Y1..Yn</code>. This also explains that Terminal means there are
no rules to replace them.</p>

<p>Eg, to represent nested structure</p>

<p><code>
S -&gt; e
S -&gt; (S)
</code></p>

<p>To use CFG to present a language: Let G be a CFG with start symbol S, then the language L(G) is</p>

<p><code>
{ a1..an | V ai (- T, S *-&gt; a1..an }
           for all
</code></p>

<p>You can derive from S (with the defined of productions) to <code>a1..an</code>, which are all valid string of tokens.</p>

<p>Usually we use <code>bison</code> to implement a CFG.</p>

<h3 id="cfg---deriviations">2.2.2 CFG - DERIVIATIONS</h3>

<p>A derivation is a sequence of productions.</p>

<p><code>
S -&gt; * -&gt; * ... -&gt; *
</code></p>

<p>A derivation can be drawn as a tree, Parse Tree.</p>

<ul>
  <li>use start symbol as the root</li>
  <li>for a production <code>X -&gt; Y1..Yn</code>, add children <code>Y1..Yn</code> to node <code>X</code>.</li>
</ul>

<p><code>
  X
/  | \
Y1 Y2 Y3
</code></p>

<p>A parse tree</p>

<ul>
  <li>has terminal at the leaves and non-terminals at the interior nodes</li>
  <li>an in-order traversal of the leaves is the original input</li>
  <li>shows the assocation of operations, the input string does not</li>
</ul>

<p>```
Grammar: E -&gt; E+E | E*E|| (E) | id
Input:   id * id + id
Derivation:</p>

<pre><code>    E    /    |    \   E     +     E / | \         | E *  E        id |    | id   id ```
</code></pre>

<p>A derivation defines a parse tree, but a parse tree may have many derivations.
Left-most and right-most derivations are important in parser implementations.</p>

<h3 id="cfg---ambiguity">2.2.3 CFG - AMBIGUITY</h3>

<p>A grammar is ambiguous if it has more than one parse tree for some string.
For a string of tokens: <code>id * id + id</code>, there are two parse trees can be derived with the following
grammar <code>E -&gt; E+E | E*E | (E) | id</code>.</p>

<p>To fix it, we can rewrite grammar unambiguously, to enforce precedence</p>

<p>```
E -&gt; E’ + E | E’
E’ -&gt; id * E’ | id | (E) * E’ | (E)</p>

<p>E manage +,   as E -&gt; E’ + E -&gt; E’ + E’ + E =&gt; E’ + E’ + .. E’
E’ manage *,  as E’ -&gt; id * E’ -&gt; id * id * E’ =&gt; id * id * .. id
E’ manage (), as E’ -&gt; (E) * E’ -&gt; (E’ + E’ .. E’) * E’
```</p>

<p>It’s impossible to convert automatically. Instead of rewriting, we can also use the more natural
(ambiguous) grammar along with disambiguating declarations. Most tools allow <strong>precedence and
associativity</strong> declarations to disambiguate grammars.</p>

<h3 id="cfg---error-handling">2.2.4 CFG - ERROR HANDLING</h3>

<p>A compiler not only translates the valid programms, but also detects non-valid ones.</p>

<table>
  <thead>
    <tr>
      <th>Erro kind</th>
      <th>Example</th>
      <th>Detected by</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Lexical</td>
      <td>..$..</td>
      <td>Lexer</td>
    </tr>
    <tr>
      <td>Syntax</td>
      <td>..x*%..</td>
      <td>Parser</td>
    </tr>
    <tr>
      <td>Semantic</td>
      <td>int x; y = x(3);</td>
      <td>Type checker</td>
    </tr>
    <tr>
      <td>Correctness</td>
      <td>your program</td>
      <td>User</td>
    </tr>
  </tbody>
</table>

<p>Error recorvery</p>

<ul>
  <li>Panic mode. When an error is encountered, discard tokens until one with a clear role is found. The
synchronizing tokens are typicall the statements or expression terminators, like ;</li>
</ul>

<p><code>
// in bison, there is a special terminal error to describe how much input to skip
E -&gt; int | E + E | (E) | error int | (error)
     ------------------  -------------------
           normal               error
</code></p>

<ul>
  <li>Error productions</li>
</ul>

<p>Add <code>E -&gt; .. | EE</code> to support <code>5x</code> in addition to <code>5 * x</code></p>

<ul>
  <li>Error correction: automatic local or global correction by try token insertions and deletions
(historially, due to the slow compilation cycle)</li>
</ul>

<h2 id="abstract-syntax-tree">2.3 Abstract Syntax Tree</h2>

<p>Rather than working on the elaborate parse tree, we can use a more compact (abstract) one</p>

<h2 id="section">```</h2>
<p>| PLUS | 5 |    |
—————–
               |
               |–&gt; —————-
                    | PLUS | 2 | 3 |
                    —————-
```</p>

<h2 id="recursive-descent-parsing">2.4 Recursive-Descent Parsing</h2>

<p>Top-down. The parse tree is constructed from the top and from left to right. Start with top-level
non-terminal E, try the rules for E <strong>in order</strong>.</p>

<p>Let’s take an example:</p>

<p><code>
E -&gt; T | T + E
T -&gt; int | int * T | (E)
</code></p>

<p>We should define a few helper funcsionts along with a <code>next</code> pointer pointing to the next input token.</p>

<p><code>
term(tok: TOKEN) -&gt; bool { return *next++ == tok } // check a token is terminal
Sn() -&gt; bool                                       // check the nth production of S
S() -&gt; bool                                        // check all productions of S
</code></p>

<p>So</p>

<p>```
E -&gt; T              E1() -&gt; bool { T() }
E -&gt; T + E          E2() -&gt; bool { T() &amp;&amp; term(PLUS) &amp;&amp; E() }
                    E()  -&gt; bool {  save = next;
                                    (next = save, E1()) ||
                                    (next = save, E2()) }</p>

<p>T -&gt; int            T1() -&gt; bool { term(INT) }
T -&gt; int * T        T2() -&gt; bool { term(INT) &amp;&amp; term(TIMES) &amp;&amp; T() }
T -&gt; ( E )          T3() -&gt; bool { term(LEFT) &amp;&amp; E() &amp;&amp; term(RIGHT) }
                    T()  -&gt; bool {  save = next;
                                    (next = save, T1()) ||
                                    (next = save, T2()) ||
                                    (next = save, T3()) }
```</p>

<p>LIMITATIONS</p>

<p>If a production for non-terminal X succeeds, there is no way to backtract and try a different
production for X later. like matching against <code>int * int</code>, our program would stop after parsing the
first <code>int</code>. A general RD algorithm should support such “full” backtracking. We can also rewrite our
grammar to make it work without backtracking, by rewriting to eliminate left recursion.</p>

<p>LEFT RECURSION</p>

<p>Considering a production S -&gt; Sa, there will be a loop</p>

<p><code>
S1() -&gt; bool { return S() &amp;&amp; term(a) }
S()  -&gt; bool { return S1() }
</code></p>

<p>A left-recursive grammar has a non-terminal S, where S -&gt; Sa for some a. Recursive-descent doesn’t
work with in such cases. Generally, given a left-resurive gramar with Recursive-descent algorithm,
it runs into a loop.</p>

<p>```
S -&gt; Sa | b</p>

<p>=&gt;
S -&gt; Sa -&gt; Saa -&gt; Saaa -&gt; … -&gt; baaaaa
```</p>

<p>But, we can rewrite it using right-recursion.</p>

<p>```
S -&gt; bS’
S’ -&gt; aS’ | e</p>

<p>=&gt;
S -&gt; bS’ -&gt; baS’ -&gt; baaS’ -&gt; … -&gt; baaaaa
```</p>

<p>To make it more general</p>

<p><code>
S -&gt; Sa1 | Sa2 .. San | b1 | b2 .. bm
</code></p>

<p>can be rewritten as</p>

<p><code>
S -&gt; b1S' | b2S' .. bmS'
S' =&gt; a1S' | a2S' .. anS' | e
</code></p>

<p>SUMMARY</p>

<p>Recursive-descent parsing is a simple and general parsing strategy, which is used in GCC frontend.
To use it, left recursion must be eliminated first.</p>

<h2 id="predictive-parsing">2.5 Predictive Parsing</h2>

<blockquote>
  <p>deterministic top-down parsing</p>
</blockquote>

<p>In Recursive-descent parsing, at each step, there are many choices of production to use. Therefore,
we need to backtrack to undo bad choices. Predictive parsing are a lot like Recursive-descent
parsing, but it can “predict” which production to use by looking at the next few tokens, thus there
is no need to backtrace.</p>

<p>Predictive parers accept LL(k) grammars. At each step, there should be at most one choice of
production.</p>

<ul>
  <li>Left-to-right</li>
  <li>Left-most derivation</li>
  <li>k tokens looking ahead</li>
</ul>

<p>Eg. in Recursive-descent parsing, we have the grammar</p>

<p><code>
E -&gt; T + E | T
T -&gt; int | int * T | (E)
</code></p>

<p>In Predictive parsing, we should left-factor the grammar to make it LL(k) grammar:</p>

<p>```
E -&gt; TX
X -&gt; + E | e</p>

<p>T -&gt; intY | (E)
Y -&gt; * T | e
```</p>

<p>How to apply the Recursive-descent parsing?</p>

<p>Once we have the LL(1) grammar, we should build a LL(1) parsing table: the row is left-most
non-terminals, and the column is the next input token, content is the production rule to use, and
empty content is the error state.</p>

<p><code>
+---+------+----+----+----+-----+---+
| _ | int  | *  | +  | (  |  )  | $ |
+---+------+----+----+----+-----+---+
| E | TX   |    |    | TX |     |   |
| X |      |    | +E |    | e   | e |
| T | intY |    |    |    | (E) |   |
| Y |      | *T | e  |    | e   | e |
+---+------+----+----+----+-----+---+
</code></p>

<p>The algorithm maintains a stock records frontier of parse tree. Top of the stack is the left-most
pending terminal or non-terminal. It accpets on end of input $ &amp; empty stack. It rejects on reaching
error state.</p>

<p><code>
initialize stack = &lt;S $&gt; and next
repeat
  case stack of
    &lt;X, rest&gt; : if T[X, *next] = Y1...Yn
                then stack = &lt;Y1...yn rest&gt;;
                else error();
    &lt;t, rest&gt; : if t == *next ++
                then stack = &lt;rest&gt;;
                else error();
until stack == &lt;&gt;
</code></p>

<p>How to build build LL(1) parsing table?</p>

<p>Consider non-terminal A, and production A -&gt; a &amp; token t, there are two valid cases for T[A,t] = a:</p>

<ol>
  <li>If a -&gt;* tb, which means a can derive t in the first position, we say t ∈First(a), t belongs to
the First Set of a.</li>
  <li>Else if A -&gt; a and a -&gt;* e and S -&gt;* <em>At</em>, which means A cannot derive t, but t follows up A in
at least one derivation, we say t ∈Follow(A).</li>
</ol>

<p>First Set</p>

<p><code>
First(X) = { t | X -&gt;* ta } ⋃ { e | X -&gt;* e }
</code></p>

<p>Follow Set</p>

<p><code>
Follow(X) = { t | S -&gt;* _Xt_ }
</code></p>

<p>To construct a parsing table T for CFG G, for each production A -&gt; a in G do:</p>

<p><code>
For each terminal t ∈ First(a) do T[A, t] = a
if e ∈First(a), foreach t ∈Follow(A) do T[A, t] = a
</code></p>

<p>If in the table, any entry is multiply defined, then G is not LL(1), eg. G is not left factored,
left recursive, or ambiguous.</p>

<h2 id="bottom-up-parsing">2.6 Bottom-up Parsing</h2>

<p>Bottom-up parsing is more general than (deterministic) top-down parsing, but just as efficient.
Bottom-up builds on ideas in top-down parsing and is the preferred method for most of generator
tools. Bottom-up parsers don’t need left-factored grammar.</p>

<p><code>
E -&gt; T + E | T
T -&gt; int * T | int | (E)
</code></p>

<p>Bottom-up parsing reduces a string to the start symbol by inverting productions.</p>

<p>```
                           input string</p>

<table>
  <tbody>
    <tr>
      <td>int * int + int              ^</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>int * T   + int</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>T         + int  production</td>
      <td> </td>
      <td>reduction</td>
    </tr>
    <tr>
      <td>T         + T</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>T         + E</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>E</td>
      <td>v</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<pre><code>                       start symbol ```
</code></pre>

<p>Important Fact #1 about Bottom-up parsing: A bottom-up parser traces a rightmost derivation in
reverse.</p>

<p>This has an interesting implication: let αβω be a step of a bottom-up parse, assume the next
reduction is by X -&gt; β, then ω must be a string of terminals</p>

<h3 id="shift--reduce">SHIFT &amp; REDUCE</h3>

<p>First, we need a marker and the left handside is called left string and right side is right string.</p>

<p>Bottom-up parsing uses only two kinds of actions:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Shift: Move</td>
          <td>one place to the right <code>ABC|xyz =&gt; ABCx|yz</code></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Reduce: Apply an inverse production at the right end of the left string</li>
</ul>

<p><code>
If A -&gt; xy is a production, then Cbxy|ijk =&gt; CbA|ijk
</code></p>

<p>Left string can be implemented by a stack. Shift move pushes a terminal onto the stack. Reduce move
pops symbols off of the stack (production rhs) and pushes a non-terminal back to the stack
(production lhs).</p>

<h3 id="handles">HANDLES</h3>

<blockquote>
  <p>How do we decide when to shift or reduce?</p>
</blockquote>

<p>Bottom-up parsing algorithms are based on recognizing handles.</p>

<p>We should reduce only if the result can still be reduced to the start stymbol. Assume a rightmost
derivation, <code>S -&gt;* aXw -&gt; abw</code>, then <code>ab</code> is a handle of <code>abw</code>.</p>

<p>Important Fact #2 about Bottom-up parsing: In shift-reduce parsing, handles appear only at the top
of the stack, never inside.</p>

<p>```
stack|input</p>

<p>(E|)
```</p>

<p>In terms of recognizing handles, there are no efficent algorithms, but there are heuristic guessing
always correct for some CFGS.</p>

<p><code>
All CFGs
  Unambiguous CFGs
    LR(k) CFGs
      LALR(k) CFGs
        SLR(k) CFGs
</code></p>

<p><code>a</code> is a <strong>viable prefix</strong> if there is an <code>w</code> such that <code>a|w</code> is a state of shift-reduce parser.</p>

<p>Important fact #3 about Bottom-up parsing: For any grammar, the set of viable prefixs is a regular
language (can be recognized by a FA).</p>

<p>To recognize viable prefixes: we must</p>

<ul>
  <li>recognize a sequence of partial rhs’s of productions, where</li>
  <li>each partial rhs can eventually reduce to part of the missing suffix of its predecessor</li>
</ul>

<p>I DON’T QUITE FOLLOW THIS PART IN DETAILS, HERE IS THE ROADMAP</p>

<ul>
  <li>handles</li>
  <li>items</li>
  <li>valid prefix</li>
  <li>recognize valid prefix</li>
  <li>valid items</li>
</ul>

<h1 id="semantic-analysis">3 Semantic Analysis</h1>

<p>Lexical analysis detects inputs with illegal tokens; Parsing detects with ill-formed parse trees;
and Semantic Analysis, as last front-end phase, catches all remaning errors, eg</p>

<ul>
  <li>all identifiers are declared</li>
  <li>types</li>
  <li>inheritance relationships</li>
  <li>classes defined only once</li>
  <li>method in a class defined only once</li>
  <li>reserved identifiers are not misused</li>
</ul>

<p>Much of Semantic Analysis can be expressed as recursive descent of an AST:</p>

<p>```
before: process an AST node n
recurse: process the children of n
after: finish processing the node n</p>

<p>Eg. let x: Int &lt;- 0 in e
before: add the definition of x to current definitions, overriding any other definition of x
recurse: recurse processing children of e
after: remove definition of x and restore the old definition of x
```</p>

<h2 id="identifiers">3.1 Identifiers</h2>

<h3 id="scope">SCOPE</h3>

<p>The scope of an identifier is the portion of a program in which that identifier is accesible. Scope
helps match identifier declarations with uses. There are two kinds of scopes:</p>

<ul>
  <li>most languages use static scope, which means scope depends only on the program text, not runitme behaviour;</li>
  <li>a few languages are dynamically scoped, like LISP. A dynamically-scoped variable refers to the closest
enclosing binding in the execution of the program.</li>
</ul>

<p>```
f(x) = a + 1</p>

<p>let a = 4 in {
  f(x)
}
```</p>

<p>Static scopes in Cool lang is introduced by:</p>

<ul>
  <li>class declarations</li>
  <li>method definitions</li>
  <li>attribute definitions</li>
  <li>let expression</li>
  <li>case expressions</li>
  <li>formal parameters</li>
</ul>

<h3 id="symbol-tables">SYMBOL TABLES</h3>

<p>When performing Semantic Analysis on a portion of the program, we need to know which identifiers are
defined. Therefore we introduce a data structure that tracks the current bindings of identifiers,
which is Symbol Table.</p>

<p>For its recursive nature, we can use stack to represent nested scopes and a set to represent Symbol
Table in each scope</p>

<p>```
enter_scope()  - start a new nested scope
find_symbol(x) - find x (or null) in current scope
add_symbol(x)  - add symbol x to the table
check_scope(x) - true if x defined in current scope
exit_scope()   - exit current scope</p>

<p>scope: x, y
  scope: x, y, z
                       – enter_scope(), add_symbol(a)
    scope: x, y, z, a
                       – exit_scope()
```</p>

<p>There are cases, like Class names, that can be used before being defined. We cannot check class
names using Symbol Table or in one pass. We can gather all class names first and do the checking.
Therefore, Semantic Analysis requires multiple passes.</p>

<h2 id="types">3.2 Types</h2>

<p>What is a type? The notion varies from language to language. But a consensus is: a set of values and
a set of operations on those values. Classes are one instantiation of the modern notion of type,
like in OO.</p>

<p>A language’s type system specifies which operations are valid for which types. The goal of type
checking is to enture that operations are used with the correct types. There is no types in an
assembly language, therefore there are no types at the bit level in the machine code. So, type is a
virtual concept at the language level, and to type check is to enforce the intended interpretation of values.</p>

<p>There are a few kinds:</p>

<ul>
  <li>static typed langs: C, Java. A lot of code is written in statically typed lang has an “escape”
                    mechanism: like unsafe casts in C, Java (void pointer can be anything).</li>
  <li>dynamic typed langs: Lips, Ruby. A lot of dynamically typed lang rewrites their compilers with
                    static lang for optimization and better debugging.</li>
  <li>untyped langs: machine code</li>
</ul>

<h3 id="type-checking">3.2.1 TYPE CHECKING</h3>

<p>When type checking, the compiler basically infers types for every expression, to make sure it’s used
with the correct type.</p>

<p>We’ve seen two forma notations specifying parts of a compiler: Regular Expression and Context-Free
Grammars. The formalism for type check is <strong>logic rules of inference</strong>, which has the form that “if
Hypothesis is true, then Conclusion is true.” By tradition, inference rules are written as</p>

<p>```
⊢ Hypothesis .. ⊢ Hypothesis
—————————-
         ⊢ Conclusion</p>

<p>⊢ means “it’s provable that…”
```</p>

<p>If there is no Hypothesis required, we can consider it as an axiom</p>

<h2 id="section-1">```</h2>
<pre><code>     ⊢ Axiom
</code></pre>

<hr />
<p>⊢ 1: Int
```</p>

<p>As an example, the inference rule for Int plus operation is</p>

<p><code>
⊢ e1: Int ⊢ e2: Int
-------------------
  ⊢ e1+e2: Int
</code></p>

<p>A type system is sound if whenever <code>⊢ e: T</code>, then <code>e</code> evaluates to a value of type <code>T</code>. So, type
checking proves the fact of <code>e: T</code> on the strucuture of the AST, which then maintains the shape of
AST. There is one type rule used for each AST node and in a type rule of node <code>e</code>: Hypotheses are the proofs of
types of <code>e'</code>s subexpression and conclustion is the type of <code>e</code>. So, types are computed in a
bottom-up pass over the AST.</p>

<p><code>
   +                    ⊢ 1: Int ⊢ 2: Int
 /   \                  -----------------
1      2                    ⊢ 1+2: Int
</code></p>

<h3 id="type-environment">3.2.2 TYPE ENVIRONMENT</h3>

<p>Heuristic: for a literal <code>1 is an Int</code>, we know that <code>⊢ 1: Int</code>; but for a variable <code>x</code> how could we
know it type.</p>

<p>A type environment gives types for free variables/identifiers in the current scope, by free it means
the varaible is not defined. So, we can consider a type environment is a function from identifiers
to types.</p>

<p><code>
x                       // x is free
x + y                   // x, y is free
let y = 1 in { x + y }  // x is free, y is bound
</code></p>

<p>With the type environment, we can refine our Conclusion branch in inference rules: <code>O ⊢ e: T</code>, under
the assumption that free variables have the types given by <code>O</code>, it’s provable that the expression
<code>e</code> has type <code>T.</code></p>

<p><code>
O(x) = T
--------    [Var]
O ⊢ x: T
</code></p>

<p>To make it more scoped, we can introduce another notation <code>O[T/x]</code>, which means <code>x</code> is of type <code>T</code>
in <code>O</code></p>

<p><code>
   O[T/x] ⊢ e: T
--------------------      [Let]
O ⊢ let x: T in e: T
</code></p>

<p>A type environment is built inisde the Symbol Table and gets passed down the AST from root to
leaves. Types are computed up the AST from the leaves towars the root.</p>

<h3 id="subtype-methods-and-implementation">3.2.3 SUBTYPE, METHODS and IMPLEMENTATION</h3>

<p>SUBTYPE</p>

<p>Let’s look at the <code>[Assign]</code> inference rule</p>

<p><code>
O ⊢ e1: T1
O(x) = T0       [Assign]
T1 &lt;= T0
-------------
O ⊢ x = e1: T1
</code></p>

<p>What about <code>[if-then-else]</code>? It should be the smallest supertype larger than each branch. We
introduce <code>lub(X,Y)</code>, the least upper bound of X and Y. In an OO language, <code>lub</code> of two types is the
least common ancestor in the inheritance tree.</p>

<p><code>
O ⊢ e0: Bool
O ⊢ e1: T1
O ⊢ e2: T2
-----------------------------------------
O ⊢ if e0 then e1 else e2 fi: lub(T1, T2)
</code></p>

<p>METHODS</p>

<p>In the example language COOL, method and object identifiers in different name spaces, that a method
<code>foo</code> and an object <code>foo</code> can coexist in the same scope. This is reflected by a separate mapping <code>M</code>
for method signatures, which we call the method environment. In most cases, <code>M</code> is passed down the
AST and only gets used in <code>Dispatch</code> rules.</p>

<p>```
M(C, f) = (T1, T2, … Tn, Tn+1)</p>

<p>// The method signature of f in class C</p>

<p>f(x1: T1, x2: T2 … xn: Tn): Tn+1
```</p>

<p>Therefore a method is type checking by</p>

<p>```
O, M ⊢ e0: T0                          [Dispatch]
M(T0, f) = (T1’, T2’, … Tn’, Tn+1)</p>

<p>O, M ⊢ e1: T1
…
O, M ⊢ en: Tn</p>

<h2 id="ti--ti-for-1--i--n">Ti &lt;= Ti’ for 1 &lt;= i &lt;= n</h2>
<p>O, M ⊢ e0.f(e1, …, en): Tn+1
```</p>

<p>For a static dispatch, we require users to use <code>e0@T</code> explicitly</p>

<p>```
O, M ⊢ e0: T0                          [Static-dispatch]
M(T0, f) = (T1’, T2’, … Tn’, Tn+1)
T0 &lt;= T</p>

<p>O, M ⊢ e1: T1
…
O, M ⊢ en: Tn</p>

<h2 id="ti--ti-for-1--i--n-1">Ti &lt;= Ti’ for 1 &lt;= i &lt;= n</h2>
<p>O, M ⊢ e0@T.f(e1, …, en): Tn+1
```</p>

<p>SELF_TYPE</p>

<p>For a language supports self type, we need to know that which class the expression appears in. So
the full type environment for COOL is</p>

<ul>
  <li>A mapping <code>O</code> gives types to object identifiers</li>
  <li>A mapping <code>M</code> gives types to method identifiers</li>
  <li>The current class <code>C</code></li>
</ul>

<p>IMPLEMENTING</p>

<p>COOL type checking can be implemented in a single traversal of the AST, that type environment is
pass down and types are passed up.</p>

<p>Let’et take <code>[Let-init]</code> as an example. Inference rule is the defined as</p>

<p><code>
O,M,C ⊢ e0: T0
O[T/x], M, C ⊢ e1: T1
T0 &lt;= T
-------------------------------
O,M,C ⊢ let x: T = e0 in e1: T1
</code></p>

<p>The implementation would be</p>

<p><code>
TypeCheck(Environment, let x: T = e0 in e1) = {
    T0 = TypeCheck(Environment, e0);
    T1 = TypeCheck(Environment.add(x: T), e1);
    Check subtype(T0, T);
    return T1;
}
</code></p>

<h3 id="static-vs-dynamic">3.2.4 STATIC VS. DYNAMIC</h3>

<p>```</p>

<p>class A { .. }
class B inherits A { .. }
class Main {
  x: A = new A;           // static type of x is A, dynamic type of x is A
  x = new B;              // dynamic type of x is B now
}
```</p>

<p>In COOL, <code>dynamic_type(E) &lt;= static_type(E)</code></p>

<h3 id="selftype">3.2.5 SELF_TYPE</h3>

<p>```
class Count {
  i: int = 0;
  inc(): Count {
    i = i + 1;
    self;
  }
}</p>

<p>class Stock inherits Count {
  name: String;
}</p>

<p>class Main {
  Stock a = (new Stock).inc(); // Type error, as we are assigning Count to Stock
}
```</p>

<p>So, we should extend the type system with SELF_TYPE, which allows the return type of <code>inc</code> to change
whne <code>inc</code> is inherited.</p>

<p><code>
class Count {
  i: int = 0;
  inc(): SELF_TYPE {
    i = i + 1;
    self;
  }
}
</code></p>

<h1 id="runtime-organizations">4 Runtime Organizations</h1>

<p>Before we get into optimization and code generation, we need to understand what we are trying to
generate. A runtime organization controls the management of run-time resources. Particually, to
understand a compiler works, we should understand the correspondenc between static (compile-time)
and dynamic (run-time) strucutres: what is done by the compiler and what is deferred to the
generated program actually runs.</p>

<p>Execution of a program is initially by OS. When a program is invoked: the OS allocates space for the
program; the code is loaded into part of the space; the OS jumps to the entry point (“main”).</p>

<p>In terms of space (memroy), traditionally it’s like this. A compiler is responsible for generating
code and orchestrating code to use the data space.</p>

<p><code>
+-------------+
|     code    | ---+
+-------------+    |
|             |    |
|     data    | &lt;--+
|             |
+-------------+
</code></p>

<h2 id="activations">4.1 Activations</h2>

<p>There are two goals for code generation: correctness and speed. To talk about that, we need to talk
about activations.</p>

<p>An invocation of procedure P is an activation of P. The lifetime of an activation of P is all the
steps of execute P, including all the steps in procedures P calls. We can also say that the lifetime
of a varaible x is the portion of execution in which x is defined. To be noted that, lifetime is a
dynamic (run-time) concept, whereas scope is a static (compile-time) concept.</p>

<p>Given, when P calls Q, then Q returns before P returns, activation lifetimes can be depicted as a
activation tree. The activation tree depends on run-time behaviour and may be different for every
program input.</p>

<p>```
class Main {
  g(): Int { 1 };
  f(): Int { 2 };
  main() { g(); f(); }
}</p>

<p>main
  /  \
g     f</p>

<p>class Main {
  g(): Int { 1 };
  f(x: Int): Int { if x == 0 then g() else f(x-1) };</p>

<p>main(): Int {
    f(3)
  }
}</p>

<p>main
  |
  f(3)
  |
  f(2)
  |
  f(1)
  |
  f(0)
  |
  g
```</p>

<p><strong>Since activations are properly nested, we can use a stack to track currently active procedures.</strong></p>

<p><code>
+-------------+
|     code    |
+-------------+
|     stack   | ------------+
|       |     |             |
|       v     |            main
|             |             |
+-------------+             f(3)
                            |
                            ..
                            |
                            g
</code></p>

<h2 id="activation-records">4.2 Activation Records</h2>

<p>So what information we should keep for activations? The information needed to manage one procedure
activation is called an Activation Record or Frame.</p>

<p>If procedure F calls G, then G’s activation records contains a mix of info about F and G. Becuase
G’s AR should contain information to 1. complete execution of G 2. resume execution of F.</p>

<p>This is one of many possible AR designs (which works for C)</p>

<p><code>
+----------------+
| result         |
+----------------+
| argument       |
+----------------+
| control link   |   // who calls the current activation
+----------------+
| return address |   // where to resume execution after the current activation
+----------------+
</code></p>

<p>Let’s review this example again:</p>

<p>```
class Main {
  g(): Int { 1 };
  f(x: Int): Int { if x == 0 then g() else f(x-1) (**) };</p>

<p>main(): Int {
    f(3); (*)
  }
}</p>

<p>+—————-+ &lt;-+              main
| Main AR        |   |                |
+—————-+   |                |
                     |                |
+—————-+ &lt;-|—+           f(3)
| result         |   |   |            |
+—————-+   |   |            |
| argument: 3    |   |   |            |
+—————-+   |   |            |
| control link   | –+   |            |
+—————-+       |            |
| return (*)     |       |            |
+—————-+       |            |
                         |            |
+—————-+       |           f(2)
| result         |       |            |
+—————-+       |            |
| argument: 2    |       |            |
+—————-+       |
| control link   | ——+
+—————-+
| return (**)    |
+—————-+
```</p>

<p>The compiler must determine, at compile-time, the layout of AR and generate code
that correctly accesses location in the activation records. Thus, <strong>the AR layout and the code
generator must be designed together.</strong></p>

<h2 id="global--heaps">4.3 Global &amp;&amp; Heaps</h2>

<p>Globals cannot be stored in AR as all references to a global variable should point to
the same object. So, globals are assigned at a fixed address once, as statically allocated.</p>

<p>For values that outlive the procedure that creates it cannot be kept in the AR neither, like in
<code>method foo() { new Bar }</code>, that <code>Bar</code> value must survive deallocation of <code>foo</code>’s AR. So, we need to
use heap to store dynamically allocated data.</p>

<p><code>
+-------------+ Low address
|    code     |
+-------------+
|             |
| static data |
|             |
+-------------+
|   stack     |
|     |       |
|     v       |
|.............|
|             |
|             |
|             |
|             |
|             |
|.............|
|     ^       |
|     |       |
|    heap     |
+-------------+ High address
</code></p>

<h2 id="alignment">4.4 Alignment</h2>

<blockquote>
  <p>This is a very low level but important detail of machine architecture.</p>
</blockquote>

<p>Most modern machines are 32 or 64 bit: 8 bits in a byte, and 4 or 8 bytes in a <strong>word</strong>. Machines are
either byte or word addressable.</p>

<p>Data is <strong>word aligend</strong> if it begins at a word boundary. Most machines have some alignment
restrictions or performance penalties for poor alignment.</p>

<h1 id="code-generation">5 Code Generation</h1>

<h2 id="stack-machines">5.1 Stack Machines</h2>

<blockquote>
  <p>the simplest model for code generation</p>
</blockquote>

<p>A stack machines use a stack as the only storage. An instruction <code>r = F(a1,...an)</code> is executed as</p>

<ul>
  <li>Pops n operands from the stack</li>
  <li>Computes the operation F</li>
  <li>Pushes result back to the stack</li>
</ul>

<p>```
5 + 7</p>

<p>push(5) - push 5 on the stack
push(7) - push 7 on the stack
add     - pop, add and push 12 on the stack
```</p>

<h3 id="stack-machine-vs-register-machine">5.1.1 Stack machine vs Register machine</h3>

<p>What’s the benefit of using a stack machine?</p>

<p>Location of the operands/result is not explicitly stated, as which are always on the top of the
stack. We consider <code>add</code> as a valid operation, instead of <code>add r1, r2</code> in a register machine. This
leads to more compact programs (space). Java bytecode uses stack evaluation.</p>

<p>However, a register machine is mostly preferred and generally faster (time), because we can place
the data at exactly where we want it to be, which has generally less intermediate operations and
manipulation like pushing and popping off the stack.</p>

<h3 id="n-register-stack-machine">5.1.2 n-register stack machine</h3>

<p>It’s an intermediate form between pure stack machine and register machine. Conceptually, keep the
top n locations of the pure stack machine’s stack in registers. A 1-register stack machines is
called the accumulator.</p>

<p>In a pure stack machine, an <code>add</code> does 3 memory operations, with two reads and one write to the
stack. In a 1-register stack machine, <code>add</code> does one read as <code>acc &lt;- acc + top_of_stack</code>.</p>

<p>In a general form, for expression <code>op(e1,..en)</code> (each <code>e</code> is an subexpression)</p>

<ul>
  <li>for each <code>ei</code>
compute <code>ei</code>
store the result in <code>acc</code>
push result on the stack</li>
  <li>Pop n-1 values from the stack, compute <code>op</code></li>
  <li>Store result in <code>acc</code></li>
</ul>

<p>The invariance a stack machine maintains: After evaluating an expression e, the accumulator holds
the value of e and the stack is unchanged. This is a very important property: <strong>Expression
evaluation preserves the stack</strong>.</p>

<h2 id="intro-3">5.2 Intro</h2>

<p>We’ll focus on generating code for a stack machine with accumulator. We simulate stack machines
instrinstructions using MIPS instructions and registers. MIPS architecture is prototypical RISC.
Most operations use registers sfor operands &amp; results, use load&amp;store instructions to use values in
memory. There 32 general purpose registers (32 bits each), we’ll use <code>$a0 $sp $t1</code>.</p>

<p>```
Stack Machine                       MIPS</p>

<p>accumulator                         $a0 register
stack                               memory
  the next location on the stack    $sp
  the top of stack                  $sp + 4 (stack grows towards lower addresses)
```</p>

<p>MIPS instruction list</p>

<p>```
# register - memory
lw reg1 offset(reg2)    load a 32-bit word from reg2+offset into reg1
sw reg1 offset(reg2)    store a 32-bit word in reg1 at address reg2+offset</p>

<h1 id="register---immediate-value">register - immediate value</h1>
<p>li reg imm              eg. li reg 4</p>

<h1 id="register---add-operation">register - add operation</h1>
<p>add reg1 reg2 reg3
addiu reg1 reg2 imm
```</p>

<h2 id="code-gen">5.3 Code Gen</h2>

<p>Let’s define a simple language with integers and integer operations</p>

<p>```
P -&gt; D; P | D</p>

<p>D -&gt; def id(ARGS) = E;
ARGS -&gt; id, ARGS | id</p>

<p>E -&gt; int | id | if E1 = E2 then E3 else E4
      | E1 + E2 | E1 - E2 | id(E1,…En)
```</p>

<p>For each expression <code>e</code> we generate MIPS code that
* Computes the value of <code>e</code> in <code>$a0</code>
* Preserves <code>$sp</code> and the contents of the stack</p>

<p>```
cgen(e1 + e2) =
  // compile time code prints out runtime code</p>

<p>cgen(e1)
  print “sw $a0 0($sp)”           // push value onto stack
  print “addiu $sp $sp-4”</p>

<p>cgen(e2)</p>

<p>print “lw $t1 4($sp)”           // load value from stack
  print “add $a0 $t1 $a0”         // add
  print “addiu $sp $sp 4”         // pop from stack
```</p>

<p>How about code gen for functions?</p>

<p>Code for function calls and function definitions depend on the layout of the AR. A very simple AR
suffices for COOL lang:</p>

<ul>
  <li>The result is always in the accumulator (no need to store in the AR)</li>
  <li>The AR holds actual parameters. For <code>f(x1,..xn)</code> push <code>xn,..x1</code> on the stack</li>
  <li>The stack discipline guarantees that on function exit <code>$sp</code> is the same as it was on function
entry (no need for a control link)</li>
  <li>We need the return address</li>
  <li>A pointer to the current activation is useful. This pointer lives in register <code>$fp</code> (frame pointer)</li>
</ul>

<p>For AR, the caller’s frame pointer, the actual parameters and the return address suffices.</p>

<p>A call to <code>f(x, y)</code> the AR is</p>

<p><code>
+-------------+
| old fp      |
+-------------+
| y           |
+-------------+
| x           |
+-------------+
| return addr |
+-------------+
</code></p>

<p>The calling sequene is the instructions (of both caller and callee) to set up a function invocation.</p>

<p>The caller
  * saves its value of the frame pointer
  * then it saves the actual parameters in reverse order
  * finally the caller saves the return address in register $ra
  * The AR so far is 4*n + 4 bytes long
The callee
  * pops the return address, the actual arguments and restores the caller’s frame pointer</p>

<p>Summary:</p>

<ul>
  <li>The AR must be designed together with the code generator</li>
  <li>Code generation can be done by recursive traversal of the AST</li>
  <li>Production compilers do different things:
    <ul>
      <li>Emphasis on keeping values in registers</li>
      <li>Intermediate results (temporaries) are laid out in the AR, not pushed and poped from the stack</li>
    </ul>
  </li>
</ul>

<p>How about temporaries?</p>

<p>A rule of thumb is to keep temporaries in AR. So, code generation must know how many temporaries are
in use at each point, which can be done by looking at the AST.</p>

<p><code>
+-------------+
| old fp      |
+-------------+
| xn          |
+-------------+
| ..          |
+-------------+
| x1          |
+-------------+
| return addr |
+-------------+
| temp 1      |
+-------------+
| temp 2      |
+-------------+
</code></p>

<h2 id="code-gen---object-layout">5.4 Code Gen - Object Layout</h2>

<p>OO Slogan: If B is a subclass of A, then an object of class B can be used wherever an object of
class A is expected. This means that code in class A has to work unmodified for object of class B,
at compile time.</p>

<p>Q. How are objects represented in memory?</p>

<p>Objects are laid out in contiguous memory. Each attributes stored at a fixed offset in the object.
WHen a method is invoed, <code>self</code> points to the  whole object.</p>

<p>The layout for Cool objects</p>

<p><code>
+--------------+ First 3 words are headers
| Class tag    |
+--------------+
| Object size  |
+--------------+
| Dispatch Ptr |
+--------------+ then attributes
| attr 1       |
+--------------+
| attr 2       |
+--------------+
| ...          |
+--------------+
</code></p>

<p>Based on the observation: <strong>Given a layout for class A, a layout for subclass B can be defined by
extending the layout of A with additional slots of the addition attributes of B</strong>. So consider layout
of <code>A3 &lt; A2 &lt; A1</code></p>

<p><code>
+--------------+
| Header       |
+--------------+
| A1 attrs     |
+--------------+
| A2 attrs     |
+--------------+
| A3 attrs     |
+--------------+
</code></p>

<p>The offset for an attribute is the same in a class and all of its subclasses.</p>

<p>Q. How is dynamic dispatch implemented?</p>

<p>Every class has a fixed set of methods, including inherited methods. A dispatch table is used to
index these mtehods. It’s an array of method entrypoints. A method <code>f</code> lives at a fixed offset in
the dispatch table for a class and all of its subclasses.</p>

<p><code>
A { f() }
B &lt; A { g() }
C &lt; A { f(); h() }
</code></p>

<p>The dispatch table would be: tables for B and C extend table for A to the right, and because methods
can be overriden, the method of f is not the same in every class, but is always at the same offset.</p>

<p><code>
+--------+----+---+
| offset | 0  | 4 |
+--------+----+---+
| A      | FA |   |
| B      | FA | g |
| C      | FC | h |
+--------+----+---+
</code></p>

<p>So back to the dispatch pointer, the dispatch pointer in an object of class X points to the dispatch
table for class X:</p>

<p><code>
+--------------+
| Class tag    |
+--------------+
| Object size  |     Dispatch Table
+--------------+     +-------------------+
| Dispatch Ptr | --&gt; |    |    |    |    |
+--------------+     +-------------------+
| Attrs        |
+--------------+
</code></p>

<p>Every method <code>f</code> of <code>X</code> is assigned an offset <code>Of</code> in the dispatch table at compile time. That’s why
we maintain the offset when extending dispatch table.</p>

<p>Theorectially we can save the table directly as we do for attributes. But attributes are
states that 100 objects can each have a different set of attributes values. Methods are static that
it makes sense to share the common table among objects.</p>

<h2 id="evaluation-semantics">5.5 Evaluation Semantics</h2>

<blockquote>
  <p>runtime semantics, compared to compile-time Semantic Analysis</p>
</blockquote>

<p>```
LA - SA - Semantic Analysis - Code Generation
                                     |
              +—               bytecode                —+
              |                      |                      |
              |              +—————+              |
        Execution            | Stack Machine |    Evaluation Semantics
              |              +—————+              |
              |                      |                      |
              +—             machine code              —+</p>

<p>```</p>

<p>In Code Generation, we need to define an evaluation rule, which is also called Semantics.</p>

<p>Let’s look back the definition of a programming language:
* The tokens is parsed by Regular Expressions in Lexical Analysis
* The grammar is represented by CFG in Syntactic Analysis
* The typing rule is represented by Inferenece Rule in Semantics Analysis
* The evaluation rules is represented by Semantics in Code Generation and Optimization</p>

<p>We have specified evaluation rules indirectly by
* the compilation of Cool program to a stack machine bytecode; and
* the evaluation rules of the stack machine, which translates bytecode to some assembly program</p>

<p>This is a complete description of evaluation rules, but it’s not good enough, as assembly-language
descriptions of language implementations have irrelevant details, that we don’t want it become the
only way to execute our program.</p>

<ul>
  <li>whether to use a stack machine or not</li>
  <li>which way the stack grows</li>
  <li>how integers are represented</li>
  <li>the particular instruction set of the architecture</li>
</ul>

<p>Therefore, we’d love to have a complete description, but not an overly restrictive specification.</p>

<p>There are many ways to specify semantics:</p>

<ul>
  <li>Operational semantics: it describes program evaluation via execution rules on an abstract machine,
which is most useful for specifying implementations.</li>
  <li>Denotation semantics: program’s meaning is mapping to a mathematical function</li>
  <li>Axiomatic semantics: program’s behaviour is described via logical formalae. It’s the foundation of
many program verification systems.</li>
</ul>

<h2 id="operational-semantics">5.6 Operational Semantics</h2>

<p>We should introduce a formal notation, which is Logical rules of inference as in type checking.</p>

<p>```
# in type checking, this means in a given context, expression e has type C
Context ⊢ e: C</p>

<h1 id="in-evaluation-this-means-in-a-given-context-expression-e-evaluates-to-value-v-1">in evaluation, this means in a given context, expression e evaluates to value v</h1>
<p>Context ⊢ e: v
```</p>

<p>Consider the evaluation of <code>y &lt;- x + 1</code>, we should track variables and their values with:</p>

<ul>
  <li>an environment: where a variable is in memory</li>
  <li>a store: what is in the memory</li>
</ul>

<p><code>
  environment          store
     |                   |
var ---&gt; memory address ---&gt; value
</code></p>

<p>A variable environment maps variables to locations, that keeps track of which variables are in scope
and tells us where those variables are</p>

<p><code>
E = [a: l1, b: l2]
</code></p>

<p>A store maps memory locations to values</p>

<p>```
S = [l1 -&gt; 5, l2 -&gt; 7]</p>

<p>S’ = S[12/l1] defines a new store S’ which has a substitution of l1 to 12
```</p>

<p>Cool values are objects, which define <code>X(a1 = l1, ..., an = ln)</code> as a Cool object where <code>X</code> is the
class name, <code>ai</code> are the attributes (including the inherited ones) and <code>li</code> the location where the
value of <code>ai</code> is stored.</p>

<p>There are a few special cases (classes withouth attributes)
* <code>Int(5)</code>
* <code>Bool(true)</code>
* <code>String(4, "abcd")</code>
* <code>void</code> of type Object and usually use <code>NULL</code> as the concrete implementation</p>

<p>The evaluation judgement is</p>

<p>```
so, E, S ⊢ e: v, S’</p>

<p>Given
  so as the current value of self
  E as the current variable environment
  S as the current store
If the evaluation of e terminates then
  the value of e is v
  the new store is S’
```</p>

<p>Therefore, the result of an evaluation is a value and a new store, where new store models the
side-effects.</p>

<p>Examples</p>

<p><code>
  E(id) = lid
  S(lid) = v
-------------------
so, E, S ⊢ id: v, S
</code></p>

<h2 id="intermediate-language">5.7 Intermediate language</h2>

<p>A language between source and target. With more details than souce and less than target.</p>

<p>Intermidate language can be considered as high-level assmebly. It uses register names, but has an
unlimited number. It uses control structures as assembly language. It uses opcodes but some are
higher level, like <code>push</code> translates to several assembly instructions.</p>

<p>Usually, we prefer to apply optimizations over IL, instead of AST or assembly language.</p>

<h1 id="optimization">6 Optimization</h1>

<h2 id="intro-4">6.1 Intro</h2>

<p>Most complexity in modern compilers is in the optimizer. Optimization seeks to improve a program’s
resoure utilization: execution time, code size and network messages sent, etc. In practice, not all
fancy optimizations known are implemented, given the difficulty to implement, the cost in
compilation and low payoff. So, the goal of optimization should be to get maximum benefit with
minimum cost.</p>

<p>Q. What should we perform optimizations?</p>

<p>On AST (after Semantic Analysis)
* pro: machine independent
* con: too high level</p>

<p>On Assembly language (after code gen)
* pro: expose optimization opportunities
* con: machine independent
* con: must reimplement optimizations when re-targeting</p>

<p>On Intermediate language
* pro: machine independent
* pro: expose optimization opportunities</p>

<p>Q. What are the units of optimization?</p>

<p>A basic block is a maximal sequence of instructions with no labels (except at the first instruction)
and no jumps (except in the last instruction), which makes it a single-entry, single-exit,
straight-line code segment.</p>

<p>For example, within this basic block, given 3 executes only after 2, we can change 3 to be <code>w := 3*x</code></p>

<p><code>
1. L:
2.  t := 2*x
3.  w := t + x
4.  if w &gt; 0 goto L
</code></p>

<p>A control-flow graph is a directed graph with basic block as nodes. Between the nodes, there can be
an edge from block A to block B if the execution can pass from the last instruction in A to the
first instruction in B. Usually, the body of a function can be represented as a control-flow graph.</p>

<p>Q. What are granularities of optimizations? Like in C</p>

<ol>
  <li>Local optimization: apply to a basic block in isolation</li>
  <li>Global optimization (it’s not really global, but to function): apply to a control-flow graph in
isolation</li>
  <li>Inter-procedural optimization: apply across function boundaries.</li>
</ol>

<p>Most compilers do 1, many do 2 and few do 3.</p>

<h2 id="local-optimization">6.2 Local Optimization</h2>

<p>Each local optimization does little by itself, given the scope of basic block. But optimizations
typically interfact, comiplers repeat optimizations until no improvement is possible.</p>

<p>ALGEBRAIC OPTIMIZATION</p>

<p><code>
x = x * 0   =&gt; x = 0
y = y ** 2  =&gt; y = y*y
x = x * 8   =&gt; x = x &lt;&lt; 3
</code></p>

<p>CONSTANT FOLDING, that operations on constants can be computed at compile time. This can be
dangerous when compiler and gen code are running on different archs, eg. how float is represented.</p>

<p><code>
x = 2 + 2 =&gt; x = 4
</code></p>

<p>ELIMINIATE UNREACHABLE BASIC BLOCKS</p>

<p><code>
#define DEBUG 0
if DEBUG {
}
</code></p>

<p>Some optimizations are simplified if each register occurs only once on the left-hand side of an
assignment. We can rewrite intermediate code in <strong>SINGLE ASSIGNMENT FORM</strong>.</p>

<p><code>
x := z + y   =&gt;   b := z + y
a := x            a := b
x := 2 * x        x := 2 * b
</code></p>

<p>COMMON SUB-EXPRESSION ELIMINATION</p>

<p>If basic block is in single assignment form, and a definition <code>x:=</code> is the first use of <code>x</code> in a
block, then when two assignments have the same rhs, they compute the same value.</p>

<p><code>
x := y + z                    =&gt;  x := y + z
... // this won't change x
w := y + z                        w := x
</code></p>

<p>COPY PROPAGATION, which assumes single assignment form</p>

<p><code>
b := z + y  =&gt;  b := z + y
a := b          a := b
x := 2 * a      x := 2 * b
</code></p>

<p>DEAD CODE ELIMINATION, which assumes single assignement form</p>

<p>if <code>w := rhs</code> appears in a basic block, <code>w</code> does not appear anywhere else in the program. then <code>w</code>
is dead in the sense of not contributing to the program’s result that can be eliminated.</p>

<p><code>
x := z + y  =&gt;  b := z + y  =&gt;  b := z + y
a := x          a := b
x := 2 * b      x := 2 * b      x := 2 * b
</code></p>

<p>PEEPHOLE OPTIMIZATION</p>

<p>It’s a variation of local optimization, which directly applies on assemly code. The peephole is a
short sequenve of (usually contiguous) instructions. The optimizer replaces the sequenve with
another equivalent one (but faster).</p>

<p>```
// move from b to a
move $a $b, move $b $a -&gt; move $a $b</p>

<p>addiu $a $a i, addiu $a $a j -&gt; addiu $a $a i+j
```</p>

<p>This implies that many simple optimizations can still be applied on assembly language. “Program
optimization” is grossly misnamed, that code produced by “optimizers” is not optimal in any
reasonable sense. “Program improvement” is a more appropriate term.</p>

<h2 id="global-optimization">6.3 Global Optimization</h2>

<h3 id="dataflow-analysis">6.3.1 Dataflow analysis</h3>

<p>Before we get understand some global optimization technique, like global constant propagation, we
need to know the dataflow analysis.</p>

<p>Can we propagate constant <code>X := 3</code> to the end result?</p>

<p><code>
      X := 3
      B &gt; 0
    /          \
Y := Z + W    Y := 0
X := 4
    \          /
      A := 2 * X
</code></p>

<p>To replace a use of <code>x</code> by a constant <code>k</code> we must know: on every path to the use of <code>x</code>, the last
assignment to <code>x</code> is <code>x := k</code>.</p>

<p>The correctness condition is not trivial to check. It requres global dataflow analysis, which is an
anylysis of the entire control-flow graph.</p>

<p>Global optimization tasks share several traits:</p>

<ul>
  <li>the optimization depends on knowing a property <code>X</code> at a particular point in program execution</li>
  <li>proving <code>X</code> at any point requires knowledge of the entire program</li>
  <li>it’s Ok to be conservative, that we may say <code>X</code> is definitely true, or Don’t know if <code>X</code> is true.</li>
</ul>

<p>There are many global dataflow analysis, but they all follow the methodology: <strong>The analysis of a
complicated program can be expressed as a combination of simple rules relating the change in
information between adjacent statements.</strong></p>

<h3 id="global-constant-propogation">6.3.2 Global constant propogation</h3>

<p>To replace a use of <code>x</code> by a constant <code>k</code> we must know: on every path to the use of <code>x</code>, the last
assignment to <code>x</code> is <code>x := k</code>.</p>

<p>Let’s consider the case of computing for a single variable <code>X</code> at all program ponts. To make the
problem precise, we associate one of the following values with <code>X</code> at every program point</p>

<ul>
  <li>⊥ that <code>X</code> is bottom, this statement never executes</li>
  <li>C that <code>X</code> is constant <code>C</code></li>
  <li>T that <code>X</code> is top, means <code>X</code> is not a constant</li>
</ul>

<p>How to compute the properties <code>X = ?</code> at each program point?</p>

<p>It means to tag the property <code>X</code> before each statement in the program.We should define a transfer
function that transfers information one statement to another. There are 8 general rules in total,
here is one example</p>

<p><code>
if C(pi, X, out) = T, for any preceder of s, then C(s, X, in) = T
</code></p>

<p>The algorithm is depicted as</p>

<ul>
  <li>For every entry <code>s</code> to the program, set <code>C(s, X, in) = T</code>, set <code>C(s, X, in) = C(s, X, out) = ⊥</code> everywhere else</li>
  <li>Repeat until all points satisfy rule 1-8
Pick <code>s</code> not satisfiying rule 1-8 and update using the appropriate rule</li>
</ul>

<p>ORDERING</p>

<p>We can simplify the presentation of the analysis by ordering the values that <code>⊥ &lt; C &lt; T</code>
To make it clear, that all constants are in between and incomparable.</p>

<p><code>
          T
    /  /  |  \   \
   .. -1  0   1   ..
   \   \  |   /   /
          ⊥
</code></p>

<p>We can define the lub, least-upper bound</p>

<p><code>
lub(⊥, 1) = 1
lub(T, ⊥) = T
lub(1, 2) = T
</code></p>

<p>The use of lub explains why the algorithm terminates: Values start as <code>⊥</code> and only increas and <code>⊥</code>
can change to a constant, and to <code>T</code>, Thus <code>C(s, x, in/out)</code> can change at most twice.Therefore the
constant propagation algorithm is linear in program size:</p>

<p><code>
Number of steps = Number of C(..) values compuated * 2
                = Number of program statements * 2 (in and out) * 2
</code></p>

<h3 id="liveness-analysis">6.3.3 Liveness Analysis</h3>

<p>“live” here means value may be used in the future.</p>

<p><code>
X := 3 // x is dead
X := 4 // x is live
y := X
</code></p>

<p>A variable <code>x</code> is live at statement <code>s</code> if</p>

<ul>
  <li>there exists a statement <code>s'</code> that uses <code>x</code></li>
  <li>there is a path from <code>s</code> to <code>s'</code></li>
  <li>that path has no intervening assignment to <code>x</code></li>
</ul>

<p>How do we gather the liveness information?</p>

<p>Just as we do in the constant propagation, we can express liveness in terms of information
transferred between adjacent statements. It’s simpler as it only needs a boolean property.</p>

<p>The algorithm is depicted as</p>

<ul>
  <li>Let all <code>L(..) = false</code> initially</li>
  <li>Repeat untile all statements <code>s</code> satisfy rules 1-4
pick <code>s</code> where one of 1-4 does not hold and update use the appropriate rule</li>
</ul>

<p>A value can change from <code>false</code> to <code>true</code>, but not the other way around, so the order is <code>false &lt;
true</code>. Each value can change only once, so termination is guaranteed. Once the analysis is
computed, it is simple to eliminate dead code.</p>

<p>We’ve seen two kinds of analysis:
* Constant propagation is a forward analysis that information is pushed from input to output.
* Liveness is backwards analysis that information is pushed from output back towards input.</p>

<h2 id="register-allocation">6.4 Register Allocation</h2>

<blockquote>
  <p>one of the most sophiscated things that compilers do to optimize performance</p>
</blockquote>

<p>Register Allocation is a “must have” in compilers: because intermediate code uses too many
temporaries and it makes a big difference in performance.</p>

<blockquote>
  <p>temporaries -&gt; RIG -&gt; Graph Coloring</p>
</blockquote>

<p>Background: Intermediate code uses unlimited temporaries, which can simplify code generation and
optimization, but complicate the final translation to assembly. So, typical intermediate code uses
too many temporaries.</p>

<p>Problem: rewrite the intermediate code to use no more temporarie than there are machine
registers.</p>

<p>Solution: Register allocaiton is as old as compilers. There was a breakthrough in 1980 that people
found a algorithem that’s relatively simple, global and works well in practice, which is based on GRAPH
COLORING. The basic principle is: If t1 and t2 are live at the same time, they cannot share a
register.</p>

<p>Algorithm:</p>

<p>Construct an undirected graph, that a node for each temporary, an edge between t1 and t2
if they are live simultaneously at some point in the program, which is called REGISTER INTERFERENCE
GRAPH (RIG). Two temporaries can be allocated to the same register if there is no edge connecting
them. After RIG construction, the Register Allocation algorithm is architecture independent.</p>

<h3 id="graph-coloring">6.4.2 Graph Coloring</h3>

<p>A coloring of a graph is an assignment of colors to nodes, such that nodes connected by an edge have
different colors. A graph is K-COLORABLE if it has a coloring with k colors.</p>

<p>For Register Aollocation, we need to assign colors (registers) to graph nodes (temporaries), and let
k be the number of machine registers. If the RIG is k-colorable then there is a register assignment
that uses no more than k registers.</p>

<p>Graph coloring is hard:</p>

<ul>
  <li>Graph coloring is NP-hard, which means no efficient algorithems are known. So the solution is to use
heuristics, basically an approximation technique doesn’t solve the problem completely.</li>
  <li>A coloring might not exist for a given number of registers. The solution is to spill some spare
registers to memory.</li>
</ul>

<p>Observation (divide and conquor):
* Pick a node t with fewer than k neighbours in RIG
* Eliminate t and its edges from RIG
* If resulting graph is k-colorable, then so is the original grpah
* If not, spill registers to memory</p>

<p>Algorithm:</p>

<ol>
  <li>Pick a node t fewer than k neighbours; put t on a stack and remove it from the RIG; repeat until
the graph is empty
If there is no way to pick the node t with fewer than k neighbours, spill it to memory</li>
  <li>Assigne colors to nodes on the stack, starting with the last node added. At each step, pick a
color different from those assigned to already colored neighbours.</li>
</ol>

<h2 id="managing-cache">6.5 Managing Cache</h2>

<p>```
+———–+—————+———–+
| Registers | 1 cycle       | 256-8000B |
| Cache     | 3 cycles      | 256K-1M   |
| Memory    | 20-100 cycles | 32M-4G    |
| Disk      | 0.5-5M cycles | 4G-1T     |
+———–+—————+———–+</p>

<p>*cycle is the clock frequency
```</p>

<p>The cost of cache miss (for register) is very high, so typically it requires 2-layered cache to bridge
fast processor with large main memory.</p>

<p>Compilers are very good at managing registers, but not that good at managing caches. Compilers can,
and a few do, perform some cache optimizations.</p>

<p>A simple example is to perform a loop interchange.</p>

<p>```
// from
for (j=1; j&lt;10; j++)
  for (i=1; i&lt;1000000; i++)
    a[i] = b[i]</p>

<p>// to
for (i=1; i&lt;1000000; i++)
  for (j=1; j&lt;10; j++)
    a[i] = b[i]
```</p>

<h2 id="automatic-memory-management-gc">6.6 Automatic Memory Management (GC)</h2>

<h3 id="intro-5">6.6.1 Intro</h3>

<ul>
  <li>Advantage: it prevents serious storage bugs</li>
  <li>Disadvantge:
    <ul>
      <li>it reduces programmer control, like the layout of data in memory, or when is memory
deallocated;</li>
      <li>inefficient in some cases</li>
      <li>pauses problematic in real-time applications</li>
      <li>memory leaks possible</li>
    </ul>
  </li>
</ul>

<p>Automatic Memory Management became mainstream with the popularity of Java.</p>

<ul>
  <li>When an object is created, unused space is automatically allocated</li>
  <li>After a while there is no unused space. Some space is occupied by objects that will never be used
again. This space can be freed to be reused later</li>
</ul>

<p>How do we know an object will “never be used again”?</p>

<p>Observation: a program can use only the objects that it can find. An object <code>x</code> is REACHABLE if and
only if:</p>

<ul>
  <li>a register contains a pointer to <code>x</code>, or</li>
  <li>another reachable object <code>y</code> contains a point to <code>x</code></li>
</ul>

<p>You can find all reachable objects by starting from registers and following all the pointers. An
unreachable object can never be used, such objects are GARBAGE. One thing to be noted, reachability
is an approximation, which means you might be be able to find all reachability.</p>

<p>Every GC schema has the following steps:</p>

<ol>
  <li>Allocate space as needed for new objects</li>
  <li>When space runs out:
 a comput what objets might be used again (genearlly by tracing objects reachable from a set of
 “root” registers
 b free the space used by objects not found in a)</li>
</ol>

<h3 id="mark-and-sweep">6.6.2 Mark and Sweep</h3>

<ul>
  <li>Advantage: objects are not moved during GC, works well for languages with pointers like C and C++</li>
  <li>Disadvantge: fragment memory</li>
</ul>

<p>If there is no memory for GC happening, we can use some tricks, like reverse the pointer in the mark
phase and reuse garbage object in place as free list in the sweep phase.</p>

<p>CONSERVATIVE COLLECTION</p>

<p>This technique only works with Mark and sweep, as objects cannot be moved.</p>

<p>GC relies on being able to find all reachable objects which needs to find all pionters in an object.
In C or C++ it is impossible to identify the contents of objects in memory, thus we cannot tell
where all the pointers are.</p>

<p>But it’s ok to be conservative: if a memory word looks like a pointer, it’s considered to be a
pointer, like it must be algined, it must point to a valid address in the data segement.</p>

<h3 id="stop-and-copy">6.6.3 Stop and Copy</h3>

<p>Stop and copy is generally believed to be the fastest GC technique</p>

<ul>
  <li>Advantage: Allocation is very cheap (just increment the heap pointer). Collection is relatively
cheap, especially if there is a lot of garbage, as it only touches reachable objects</li>
  <li>Disadvantge: some languages do not allow copy, like C and C++.</li>
</ul>

<p>Memroy is organized into two areas: old space, used for allocation; new space, used as a reserve for GC</p>

<ol>
  <li>GC starts when the old space is full</li>
  <li>Copies all reachable objects from old space to new space, with garbage left behind</li>
  <li>After the copy, the roles of the old and new spaces are reversed and program resumes</li>
</ol>

<p>The problem is after we copy a reachable object into new space, we have to fix all pointers pointing
to it. One solution is to store in the old copy a forwarding pointer to the new copy.</p>

<h3 id="reference-counting">6.6.4 Reference Counting</h3>

<ul>
  <li>Advantage: easy to implement; collects garbage incrementally without large pauses in the execution</li>
  <li>Disadvantge: cannot collect circular structures; manipulating reference counts at each assignment is
very slow</li>
</ul>

<p>Rather than wait for memory to be exhausted, try to collect an object when there are no more
pointers to it. This requires to store in each object the number of pointers to that object, which
is the reference count. Each assignment operation manipulates the reference count.</p>

<p>```
Assume x, y point to objects o, p</p>

<p>Every assignment x = y becomes</p>

<p>rc(p) = rc(p) + 1
rc(o) = rc(o) + 1
if rc(o) == 0 then free o
x = y
```</p>

<p>To solve the circular case, one way is to make programmer be aware of it when creating circular
data; another way is to combine with other GC technique, like running a Mark and sweep every once a
while.</p>

<h3 id="advanced-gc-algorithm">6.6.5 Advanced GC Algorithm</h3>

<ul>
  <li>concurrent: allow the program to run while the collection is happening (collector is running in
background)</li>
  <li>parallel: several collectors working at once</li>
  <li>generational: do not scan long-lived objects at every collection</li>
  <li>real time: bound the length of pauses</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Essential Difference Between OOP and FP]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2018/07/01/an-essential-difference-between-oop-and-fp/"/>
    <updated>2018-07-01T23:03:54-04:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2018/07/01/an-essential-difference-between-oop-and-fp</id>
    <content type="html"><![CDATA[<p>A few months ago, I told my friend that I was learning Haskell recently. After giving him
a quick introduction, he asked me an intuitive but hard question that I was not able to give
the answer, “What is the difference between OOP and FP?”.</p>

<p>I thought hard, I asked around and I took it on my way learning Haskell, but apparently there
are no easy answers, considering they usually appear at the opposite position on the spectrum
of programming paradgim.</p>

<p>Anyway, today when I was reviewing my notes for recapping type system in Haskell, the question
got popped up again. But this time, I think I’ve got an answer:</p>

<p>Other than the obvious language characteristics, <strong>the essential difference between OOP and FP
is how they structure values and operations</strong>. OOP groups values and operations together in the
objects, while FP separates values and operations apart strictly.</p>

<p>Seriously, is that an answer? I konw, it’s not perfect, but definitely helpful to myself. I’ll make
a detailed explanation.</p>

<h2 id="what-is-a-program">What is a program?</h2>

<p>We use programming languages to write programs, which can be considered as a series of calculations
executed in many stacks of control flows. It’s not hard to figure out that, in essence, a program is
about <strong>operating values</strong>.</p>

<p>As the definition for <em>turing complete</em> says, language is about <em>data-manipulation</em>.</p>

<blockquote>
  <p>In computability theory, a system of data-manipulation rules (such as a computer’s instruction set,
a programming language, or a cellular automaton) is said to be Turing complete or computationally
universal if it can be used to simulate any Turing machine.</p>
</blockquote>

<h2 id="what-is-a-value">What is a value?</h2>

<p><strong><em>The first question is what are basic values?</em></strong></p>

<p>No matter what language you are using, there must be</p>

<p><code>
1
'c'
True # somehow boolean value can also be considered as a integer
</code></p>

<p><strong><em>But what if I want to represent more compliated values?</em></strong></p>

<p>We use data structures:</p>

<ul>
  <li>List, <code>[1,2,3]</code>, a set of independent values</li>
  <li>Tuple, <code>(1,2,3)</code>, several values compound over each other as one value</li>
  <li>Tree, values structured with a purpose to be manipulated easily</li>
  <li>Dictionary? It seems like a combination for value and algorithm</li>
</ul>

<p><strong><em>Apparently, that’s not enough for us to carve the real world. What’s the ultimate way to represent data no matter how complicated it is?</em></strong></p>

<p>It depends on what mechanism a language supports. Here jumps in the discussion for difference between
OOP and FP language, which I’ll take Ruby and Haskell as examples.</p>

<p>In Ruby, we use class to model the real world problem and use objects to hold values, which we also call
it state in OO.</p>

<p>```rb
class Person
  attr_accessor :name, :age
end</p>

<p>person = Person.new(“Di”, 18)
```</p>

<p>As we can see, class defines what pattern of data (<code>name</code> and <code>age</code>) we want to hold.
After initializing, an object will wrap the plain data <code>"Di"</code> and <code>18</code> together as a
whole new value.</p>

<p>In Haskell, we use type to model the real world values and everytime we create a customized type,
there will also come with a data constructor, which holds the values together.</p>

<p>```haskell
data Person = Person String Integer</p>

<p>person = Person “Di” 18
```</p>

<p>As a summary, no matter how complicated the value is, we can always represent it by
applying this kind of mechanisms the language provides over and over again.</p>

<h2 id="how-to-operate-value">How to operate value?</h2>

<p>By rules, laws, or formulas. In another saying, methods or functions.</p>

<p>In Ruby, we define methods in class definition to empower the object to apply onto its states.</p>

<p>```rb
class Person
  attr_accessor :name, :age</p>

<p>def gets_older; age += 1; end
end</p>

<p>person = Person.new(“Bart”, 10)
```</p>

<p>When we call <code>person.gets_older</code>, we’ll alter the <code>age</code> value by incrementing it by one. If we
have a peek into our memory, there will be data blobs like in below. Each object exists as
a bundle of values and operations.</p>

<p><code>haskell
 +-----------------+   +-----------------+   +-----------------+
 |_class_: Person  |   |_class_: Person  |   |_class_: Person  |
 |                 |   |                 |   |                 |
 |name: "Bart"     |   |name: "Lisa"     |   |name: "Maggie"   |
 |age:  10         |   |age:  8          |   |age:  1          |
 |                 |   |                 |   |                 |
 |#gets_older      |   |#gets_older      |   |#gets_older      |
 +-----------------+   +-----------------+   +-----------------+
</code></p>

<p>However, in Haskell, we make use of functions to operate values, with limits put on their types.</p>

<p>```haskell
data Person = Person String Integer
person = Person “Bart” 10</p>

<p>getsOlder :: Person -&gt; Person
getsOlder (Person name age) = Person name (age+1)
```</p>

<p>When we call <code>getsOlder person</code>, we’ll create a new copy of person data with <code>age</code> incremented
by one, conforming to immutability. As a comparison, if we look into our memory, we’ll see what’s
in below.</p>

<p>```haskell
 +—————–+   +—————–+   +—————–+
 |<em>type</em>: Person   |   |<em>type</em>: Person   |   |<em>type</em>: Person   |
 |                 |   |                 |   |                 |
 |name: “Bart”     |   |name: “Lisa”     |   |name: “Maggie”   |
 |age:  10         |   |age: 8           |   |age: 1           |
 +—————–+   +—————–+   +—————–+</p>

<p>+—————————————————————+</p>

<p>+———————+
 |<em>type</em>: Function     |
 |                     |
 |name: getsOlder      |
 |sig: Person -&gt; Person|
 +———————+
```</p>

<h2 id="wrap-up">Wrap up</h2>

<p>As a conclusion, I think OOP and FP take a different approach to operating values. OOP packs up
values and permitted operations together via objects, wheras FP separates values and operations
apart strictly.</p>

<p><em>PS. How does polymorphism fit in the discussion?</em></p>

<p>We can keep following the imaginary memory snapshot above. In OOP, it’ll work as long as
objects repond to the same method, whereas in Haskell, we can loose the function to allow
more general types passed in.</p>

<p>In Ruby, it’s about duck typing.</p>

<p>```rb
class Person
  def gets_older; age += 1; end
end</p>

<p>class Duck
  def gets_older; age += 1; end
end</p>

<p>def as_time_goes_by(a_living)
  a_living.gets_older # no matter it’s a person or a duck
end
```</p>

<p>In Haskell, we use type to represent a set of values and typeclass to enforce rules on what kind
of operations can be applied onto the type (the set of values).</p>

<p>```haskell
data Person = Person String Integer deriving (Show)
data Duck = Duck Integer deriving (Show)</p>

<p>class Living a where
  getsOlder :: a -&gt; a</p>

<p>instance Living Person where
  getsOlder (Person name age) = Person name (age+1)</p>

<p>instance Living Duck where
  getsOlder (Duck age) = Duck (age+1)</p>

<p>asTimeGoesBy :: Living a =&gt; a -&gt; a
asTimeGoesBy living = getsOlder living
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Peep at Types in Haskell]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2018/04/21/a-peep-at-types-in-haskell/"/>
    <updated>2018-04-21T10:15:19-04:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2018/04/21/a-peep-at-types-in-haskell</id>
    <content type="html"><![CDATA[<p>Functional programming is a shining and trending topic on the other side of the spectrum of programming, which has been
proven to be a better choice in some specific area of problem solving. I could still remember the mind blowing of various
ideas and thoughts in FP when I first learnt Elm two years ago. It just shows me yet another possibility of thinking
after being immersed in the world I took for granted, all kinds of the imperative languages, C, Java, Ruby, Javascript.
It’s a whole new world.</p>

<p>Recently, I picked up the FP learning by taking part in a book reading club in the company I work, for
<a href="http://haskellbook.com/">“Haskell Programming from first principles”</a>. My girlfriend was teasing me that I’ve never
showed up in a Haskell class at the university, why the heck you talk about it all the time now. I should admit that I might
still feel bored if I could jump back in time. It’s just I don’t have the mind power or experience to think about the
problem in a big picture at that time. Anyway, I’m glad it’s never too late. It’s been two months for the fun journey.
I want to share some thoughts on it.</p>

<p>I just finished reading about algebraic data types in Haskell. Basically, it’s done talking about type systems in the
book. By getting to know types in Haskell, I questioned about why type exists all the time. I cannot help myself
thinking about how would I solve the same problem in OO world, like what is the difference about how data is
structured between OO and FP. Gradually, keeping punching my head, I feel like I start to see something through. I start
to realize that, <strong>programming is all about data and operations</strong>. Different paradigms, OO and FP, are two different
approaches. Under the big picture, languages choose to implement different characteristics, like
inheritance, encapsulation, polymorphism, prototype, immutability, static and dynamic types, etc, to reach a same goal,
to represent and manipulate data in an effective way.</p>

<p>Since it’s all about data, what is data? I consider it as a general name for all possible values, like primitive ones, <code>1, 'a',
True</code>, which are usually bulit into the language, like a list <code>[]</code> and a map <code>{}</code>, working as a group of values following some basic
simple rules. To better fit the language into modeling various realistic problems, OO introduces class, which packs up the states
(data) and methods (operation) together (In some sense, class is a set of data and rules applied onto the data). In the meantime, FP
provides a different path, with type as a representation of a set of data and function (operation) as a way to transform data. For
example, in OO <code>obj</code> has a state <code>value</code>, and <code>obj.foo</code> might migrate the state. In FP, we just do <code>foo(value)</code>
directly. (The limitation or visibility of the operation comes from encapsulation in OO, but type matching in FP)</p>

<p>So what is type? Type, in short, is a set of values (data). For example, <code>Int8</code> is a built-in type in Haskell as a set of 256
numbers, starting from -128 to 127, <code>Char</code> is a set of all possible charaters, <code>String</code> is a list of <code>Char</code>. How
to represent a binary tree:</p>

<p><code>
	  2
  /   \
1      3
</code></p>

<p>What exactly do we need to know about the tree? Three nodes with three values, and the connections among them.</p>

<p>In OO, we are going to create a structure regarding the node, with a state recording the value and two links for the
connection.</p>

<p>```ruby
class Node
  attr_accessor :left, right</p>

<p>def initialize(value)
    @value = value
    left = nil
    right = nil
  end
end</p>

<p>root        = Node.new(2)
root.left   = Node.new(1)
root.right  = Node.new(3)
```</p>

<p>To make it a tree, instead of a plie of nodes, we can create a structure like</p>

<p>```ruby
class BinaryTree
  attr_reader :root_node</p>

<p>def initialize(root_value)
    @root_node = Node.new(root_value)
  end</p>

<p>def insert(node_value)
    …
  end
end
```</p>

<p>So a final representation for the binary tree in Ruby, it’ll be</p>

<p><code>ruby
bt = BinaryTree.new(2).insert(1).insert(3)
</code></p>

<p>What about in Haskell? At first, we create a type to represent the tree like below, which basically means a binary tree
is either a leaf, or a node with a value and two nodes, which are both binary trees.</p>

<p><code>haskell
data BinaryTree a = Leaf | Node (BinaryTree a) a (BinaryTree a)
</code></p>

<p>To represent the tree,</p>

<p><code>haskell
bt = Node (Node Leaf 2 Leaf) 1 (Node Leaf 3 Leaf)
</code></p>

<p>It’s done! To be honest, I was totally blown away by the second I understand how things work here. It’s neat, elegant
and way over my thinking.</p>

<p>Cool, so how it works? Between two cases in two languages. Here is my understanding: in the perspective of a
compiler (or interpreter),</p>

<ul>
  <li>In Ruby, it sees a reference to a chunk of memory, which contains the necessary info about getting to know that, there
is an object, which has a state and maybe a reference to its parent, which contains operations allowed to do.</li>
  <li>In Haskell, it sees a sequence of tokens, literally all the data about it. According to the types definition, it
could parse out the tokens in the right pattern, thus understand it.</li>
</ul>

<p><img src="https://raw.githubusercontent.com/ifyouseewendy/ifyouseewendy.github.io/source/image-repo/binary-tree-oo-and-haskell.png" alt="binary-tree-oo-and-haskell" /></p>

<p>Bascially, this is a note of thinking while I was learning it. I’m still digesting and trying to find out a path to fit the FP ideas into my system of knowledge.</p>

<p>At last, I want to share some materials which is truly helpful to me</p>

<ul>
  <li><a href="http://www.defmacro.org/2006/06/19/fp.html">Functional Programming For The Rest of Us - Slava Akhmechet</a> Such a pleasant article as a beginner read, the history, the features, everything you need to know</li>
  <li><a href="https://youtu.be/yVuEPwNuCHw">Types, and why you should care - Ron Minsky</a> An intro video about the pros and cons
about types</li>
  <li><a href="https://youtu.be/V1po0BT7kac">Type Systems Will Make You a Better JavaScript Developer - Jared Forsyth</a> A video talking about why Facebook Flow works effectively for JS. I use it at work, but sometimes I stumble upon it and end up complaining about its stupidity. Next time, I’ll watch the video again.</li>
  <li><a href="https://flow.org/en/docs/lang/">Type Systems - Facebook Flow</a> I find it as a good supplyment after getting understand
the types in Haskell, maybe because it is a standalone type system?</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Name Driven Development]]></title>
    <link href="http://blog.ifyouseewendy.com/blog/2016/05/26/name-driven-development/"/>
    <updated>2016-05-26T18:53:08+08:00</updated>
    <id>http://blog.ifyouseewendy.com/blog/2016/05/26/name-driven-development</id>
    <content type="html"><![CDATA[<blockquote>
  <p>“Any fool can write code that a computer can understand. Good programmers write code that humans can understand.” - Martin Fowler</p>
</blockquote>

<blockquote>
  <p>“There are only two hard things in Computer Science: cache invalidation and naming things.” — Phil Karlton</p>
</blockquote>

<p>“Name Driven Development”, this is a ghost topic you can’t find on wiki. I just use it to remind me how much importance a good name can give. Maybe it’s just another bad name😂.</p>

<p>In a nutshell, why naming matters a lot is that it’s so closely related to refactoring. Here are some basic ideas I conclude</p>

<ul>
  <li>Good name reveals intention, shows legibility, and keeps clarity.</li>
  <li>Keep refactoring, until the name reveals the intention in an easy way.</li>
  <li>Don’t bother about naming too much when developing. Let the test and implementation help reveal it’s purpose. Then make a good name.</li>
</ul>

<p>To tackle this non-existing topic, I’ve googled around, reading and thinking. Here are some notes I made (to be updated).</p>

<hr />

<blockquote>
  <p><a href="https://ilinkuo.wordpress.com/2013/05/07/whats-in-a-name/#more-137">What’s in a Name? - ilinkuo</a></p>
</blockquote>

<p>Your Names Tell a Story about Your Design</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/name-drive-development/your_names_tell_a_story_about_your_design.png" alt="your_names_tell_a_story_about_your_design" /></p>

<blockquote>
  <p><a href="http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882">Clean Code: Chapter 2, “Meaningful Names” - Uncle Bob</a></p>

  <p>The “definitive” guide</p>
</blockquote>

<p><em>Good</em></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/name-drive-development/meaningful_names_basic.png" alt="meaningful_names_basic" /></p>

<p><em>Better</em></p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/name-drive-development/meaningful_names_advanced.png" alt="meaningful_names_advanced" /></p>

<blockquote>
  <p><a href="http://arlobelshee.com/good-naming-is-a-process-not-a-single-step/">Good naming is a process, not a single step - Arlo Belshee</a></p>

  <p>This serial posts provide a methodology, which explains the naming process in a clear and specific way. The first four steps aim at how to better name considering implementation, then move to thinking of intent, and domain abstraction.</p>
</blockquote>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/name-drive-development/good_naming_is_a_process.png" alt="good_naming_is_a_process" /></p>

<h3 id="summary">Summary</h3>

<ul>
  <li>Missing</li>
  <li>Nonsense</li>
  <li>Honest</li>
  <li>Honest and Complete</li>
  <li>Does the Right Thing</li>
  <li>Intent</li>
  <li>Domain Abstraction</li>
</ul>

<h3 id="why">Why</h3>

<p>The answer to that question lies at the heart of understanding, preventing, and paying off technical debt.</p>

<ul>
  <li>Indebted code is any code that is hard to scan.</li>
  <li>Technical debt is anything that increases the difficulty of reading code.</li>
</ul>

<p><em>Shouldn’t the definition of technical debt be something about the cost and risk of changing code?</em></p>

<p>It turns out that the largest single thing developers <strong>spend time doing is reading code</strong>. More than design, more than writing code, more than scanning, even more than meetings (well, probably).</p>

<p><strong>Bugs come from incomplete understanding</strong>. Incomplete understanding arises when the system is harder to understand than we can store in our heads at once.</p>

<p>So if our definition of technical debt is code that is difficult, expensive, or risky to change, then the root cause of that is code that is hard to scan. And how do we make code easy to scan? Use good names to encapsulate details.</p>

<h3 id="how">How</h3>

<p>If we want to make code more scannable, we need to increase the percentage of relevant information that it screams at you. Which also means hiding the irrelevant information.</p>

<p>The process of reducing debt is simple:</p>

<ul>
  <li>Look at something.</li>
  <li>Have an insight.</li>
  <li>Write it down.
    <ul>
      <li>Ccomment. But <strong>comments</strong> aren’t actually part of the code. They duplicate the code, which causes all the usual duplication problems.</li>
      <li>If your insight is structural then it belongs in a <strong>name</strong>. If it is a runtime insight then use an <strong>assertion</strong>.</li>
      <li>Assertions need to be easy to find. So don’t litter them around your core code. Express your insight as an example and write it down in a test. And name the test about the insight (not about what code it happens to execute).</li>
      <li>So, insights belong in names.</li>
    </ul>
  </li>
  <li>Check it in.
    <ul>
      <li>Express your intent by naming your commit using a message.</li>
    </ul>
  </li>
</ul>

<p>The insight loop is all there is</p>

<ul>
  <li>Refactoring legacy code is running this loop and writing stuff down in names.</li>
  <li>Understanding legacy code is running this loop and writing stuff down as examples in tests.</li>
  <li>TDD is running this loop three times:
    <ul>
      <li>First a loop where we look at the customer interview and we write it down as one example in a test.</li>
      <li>Second a loop where we look at the test and we write it down in names in the code.</li>
      <li>Third a loop of refactoring the (new) legacy code.</li>
    </ul>
  </li>
  <li>Design is a loop where the place you look is “how hard was it to write this test” and you write down insights by changing names (usually fixing the Does the Right Thing step).</li>
</ul>

<h3 id="steps">Steps</h3>

<p>Each transition is about refactoring.</p>

<p><img src="https://github.com/ifyouseewendy/ifyouseewendy.github.io/raw/source/image-repo/name-drive-development/good_naming_is_a_process_table.png" alt="good_naming_is_a_process_table" /></p>

<blockquote>
  <p><a href="https://stackoverflow.com/posts/422093/revisions">krosenvold</a> answer on <a href="https://stackoverflow.com/questions/421965/anyone-else-find-naming-classes-and-methods-one-of-the-most-difficult-part-in-pr/423140#423140">Stack Overflow - Anyone else find naming classes and methods one of the most difficult part in programming?</a></p>
</blockquote>

<p><code>
function programming_job(){
    while (i make classes){
         Give each class a name quickly; always fairly long and descriptive.
         Implement and test each class to see what they really are.
         while (not satisfied){
            Re-visit each class and make small adjustments
         }
    }
}
</code></p>

<blockquote>
  <p><a href="http://programmers.stackexchange.com/users/31260/gnat">gnat</a> answered on <a href="http://programmers.stackexchange.com/questions/129961/are-there-good-techniques-or-tests-for-naming-types">Stack Exchange - Are there good techniques or tests for naming types?</a></p>
</blockquote>

<p>For naming, there are six techniques that were proven to work for me:</p>

<ul>
  <li>spend a lot of time on inventing names</li>
  <li>use code reviews</li>
  <li>don’t hesitate to rename</li>
  <li>spend a lot of time on inventing names</li>
  <li>use code reviews</li>
  <li>don’t hesitate to rename</li>
</ul>

<blockquote>
  <p><a href="http://www.gameproducer.net/2008/11/11/the-7-worst-verbs-programmers-use-in-function-calls/">The 7 Worst Verbs Programmers Use In Function Calls - Juuso Hietalahti</a></p>
</blockquote>

<ul>
  <li>dispatch</li>
  <li>do</li>
  <li>resolve</li>
  <li>handle</li>
  <li>manage</li>
  <li>perform</li>
  <li>populate</li>
</ul>

<blockquote>
  <p><a href="http://objology.blogspot.com/2011/09/one-of-best-bits-of-programming-advice.html">One of the Best Bits of Programming Advice I ever Got</a></p>
</blockquote>

<p>Don’t make objects that end with ‘er’.</p>

<ul>
  <li>Managers - Every time I see one of these, I cringe. People will usually tell me what it does, long before they can tell me what it is. Is it a registry? Fine call it a registry. Is it a history or a log? Call it that. Is it a factory? Call it that.</li>
  <li>Controllers - Only good controller object I’ve made in the last 20 years was an interface to a BallastVoltageController that represented a real world object. The fact that every single MVC implementation in the world has had a different role for Controller ought to tell us something about how well that idea fit.</li>
  <li>Organizer (and many like them) - Focus is on what it does. This is a great example of how easy it is to turn many of these ‘ers’ into nouns. Call it an Organization. Now we’re focusing on what it is.</li>
  <li>Analyzer/Renderer/etc - Definitely examples of “worker” objects. What if they had been Analysis/Rendering/etc.</li>
  <li>Builder/Loader/Reader/Writer/etc - Remove the focus from the objects being manipulated, and tend assume to much responsibility themselves.</li>
</ul>

<blockquote>
  <p><a href="http://www.carlopescio.com/2011/04/your-coding-conventions-are-hurting-you.html">Your coding conventions are hurting you - Carlo Pescio</a></p>

  <p>Great article explaining four harmful conventions with obvious examples. There is a following post, <a href="http://www.carlopescio.com/2012/03/life-without-controller-case-1.html">Life without a controller</a></p>
</blockquote>

<p>From a distance, everything is object oriented, extra-cool, modern-flexible-etc, but as you get closer, you realize it’s just a thin veneer over procedural thinking (and don’t even get me started about being “modern”).</p>

<p>Fake OO names and harmful conventions</p>

<ul>
  <li>the -er suffix</li>
  <li>the -able suffix</li>
  <li>the -Object suffix</li>
  <li>the I- prefix</li>
</ul>

<p><strong>Manager, Helper, Handler…</strong></p>

<p>Good ol’ Peter Coad used to say: Challenge any class name that ends in “-er” (e.g. Manager or Controller). If it has no parts, change the name of the class to what each object is managing. If it has parts, put as much work in the parts that the parts know enough to do themselves (that was the “<strong>er-er Principle</strong>”).</p>

<ul>
  <li>Manager. When you need a Manager, it’s often a sign that the Managed are just plain old data structures, and that the Manager is the smart procedure doing the real work.</li>
  <li>Handler, again, is an obvious resurrection of procedural thinking. What is an handler if not a damn procedure?</li>
</ul>

<p><strong>Something-able</strong></p>

<p>It’s like calling a nail “Hammerable”, because you known, that’s what you do with a nail, you hammer it. It encourages procedural thinking, and leads to ineffective abstractions.</p>

<p><strong>Something-Object</strong></p>

<p>When you don’t know how to name something, pick some dominant trait and add Object to the end. Again, the problem is that the “dominant trait” is moving us away from the concept of an object. Object is dropped in just to avoid more careful thinking about the underlying concept.</p>

<p><strong>ISomething</strong></p>

<p>The problem is that it’s too easy to fall into the trap, and just take a concrete class name, put an I in front of it, and lo and behold!, you got an interface name. Sort of calling a concept IDollar instead of Currency.</p>

<p>Eg.</p>

<ul>
  <li>IList to RandomAccessContainer</li>
  <li>IEnumerable to Sequence.
    <ul>
      <li>A List is an IEnumerable (what??)</li>
      <li>A List is a Sequence (well, all right!)</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>How to Name Things: the solution to the hardest problem in programming - Peter Hilton <a href="https://skillsmatter.com/skillscasts/5747-how-to-name-things-the-solution-to-the-hardest-problem-in-programming">video</a>, <a href="http://www.slideshare.net/pirhilton/how-to-name-things-the-hardest-problem-in-programming">slide</a></p>

  <p>Taking advice from writers, funny quotes, like Stephen King on refactoring, Hemingway on modelling with personas, .etc.</p>
</blockquote>

<p>Remember: “rename” is the simplest but most effective refactoring. Use it.</p>

<p><strong>Gater domain-specific vocabulary</strong>. Scan the domain model entities Wikipedia pages for names of related concepts. Read novels set in your customer’s domain to learn their jargon. Find out what they really mean.</p>

<p>Comments: the basics</p>

<ul>
  <li>Don’t say what the code does (because the code already says that)</li>
  <li>Don’t explain awkward logic (improve the code to make it clear)</li>
  <li>Don’t add too many comments (it’s messy and they’ll get out of date)</li>
  <li>Explain why the code exists
    <ul>
      <li>When should I use this code?</li>
      <li>When shouldn’t I use it?</li>
      <li>What are the alternatives to this code?</li>
    </ul>
  </li>
</ul>

<p>How to write good comments</p>

<ul>
  <li>Try to write good code first</li>
  <li>Try to write a one-sentence comment</li>
  <li>Refactor the code until the comment is easy to write</li>
  <li>Now write a good comment</li>
  <li>Don’t forget the rules of good writing. (eg. remove unnecessary comments)</li>
</ul>

<p>P.S. Peter also has several posts talking about commenting, check <a href="http://hilton.org.uk/blog/how-to-comment-code">How to comment code</a></p>

]]></content>
  </entry>
  
</feed>
